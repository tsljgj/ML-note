{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Example Notes","text":"<p>Abbr: The HTML specification is maintained by the W3C.</p> <p>Annotation: I am happy, damn bro(1).</p> <ol> <li> <p>U.S BOY (1) </p> <ol> <li>Damn bro another annotation!</li> </ol> </li> </ol> <p>Lorem ipsum dolor sit amet, (1) consectetur adipiscing elit.</p> <ol> <li> I'm an annotation!</li> </ol> <p>Hover me</p> <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> <code>Lorem ipsum dolor sit amet</code> <p>Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis.</p> <code>Cras arcu libero</code> <p>Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante.</p> <p>Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor.</p> Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource <pre><code>graph LR;\nid1((2 )) --&gt; id2((7 ));\nid1((2 )) --&gt; id3((5 ));\nid2((7 )) --&gt; id4((2 ));\nid2((7 )) --&gt; id5((10));\nid2((7 )) --&gt; id6((6 ));\nid3((5 )) --&gt; id7((9 ));\nid7((9 )) --&gt; id8((4 ));\nid6((6 )) --&gt; id9((5 ));\nid6((6 )) --&gt; id10((11));</code></pre> <p>Lorem ipsum<sup>1</sup> dolor sit amet, consectetur adipiscing elit.<sup>2</sup></p> test.cpp<pre><code>#include&lt;bits/stdc++.h&gt;\n</code></pre> <p>Subscribe to our newsletter</p> <p>Lorem ipsum dolor sit amet, consectetur</p> <p>Lorem ipsum</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum</p> <p>Lorem ipsum dolor sit amet, consectetur</p> <p>Phasellus posuere in sem ut cursus (1)</p> <p>Lorem ipsum dolor sit amet, (2) consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <ol> <li> I'm an annotation!</li> <li> I'm an annotation as well!</li> </ol> <p>No title admonition.</p> collapsible good my friend <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> Exercise Luogu 1101 <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> Tab 1Tab 2 <p>Lorem ipsum dolor sit amet, (1) consectetur adipiscing elit.</p> <ol> <li> I'm an annotation!</li> </ol> <p>Phasellus posuere in sem ut cursus (1)</p> <ol> <li> I'm an annotation as well!</li> </ol> <p>This is an example</p> Unordered ListOrdered List <pre><code>* Sed sagittis eleifend rutrum\n* Donec vitae suscipit est\n* Nulla tempor lobortis orci\n</code></pre> <pre><code>1. Sed sagittis eleifend rutrum\n2. Donec vitae suscipit est\n3. Nulla tempor lobortis orci\n</code></pre> Image caption <ol> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u00a0\u21a9</p> </li> <li> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.\u00a0\u21a9</p> </li> </ol>"},{"location":"dl/cnn/","title":"Convolutional Neural Networks (CNN)","text":""},{"location":"dl/cnn/#convolutions","title":"Convolutions","text":"<p>Definition (Convolutions)</p> <p>In mathematics, the convolution between two functions \\(f,g: \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is defined as </p> \\[\\begin{align*} (f * g)(x) = \\int f(t)g(x-t)dt \\end{align*}\\] <p>We measure the overlap between \\(f\\) and \\(g\\) when one function is \"flipped\" and shifted by \\(x\\).</p> <p>Example (Dice - Not a Good Example)</p> <p>Assume there are two 6-face dices. We want to know the probability of the sum of two dices equals to 4.  Define \\(f(x) = \\text{probability of getting x on dice 1}\\), \\(g(x) = \\text{probability of getting x on dice 2}\\). The probability of getting a sum of 4 is:</p> \\[\\begin{align*} (f * g)(4) = \\sum^3_{m=1}f(4-m)g(m) = f(1)g(3) + f(2)g(2) + f(3)g(1) \\end{align*}\\] <p>We can view this as first flipped the function \\(g\\), and then shift \\(1,2,3,4\\) position and calculate the overlapping area.</p> <p> </p> <p>Important Note (Convolutions are Impact Sum)</p>"},{"location":"dl/cnn/#architecture-overview","title":"Architecture Overview","text":"<p>Important Note (Layers in ConvNets)</p> <p>We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer. We will stack these layers to form a full ConvNet architecture.</p> <p>Example (ConvNet Architecture for CIFAR-10 Classification)</p> <ul> <li>INPUT Layer [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</li> <li>CONV Layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.</li> <li>RELU Layer will apply an elementwise activation function, such as the \\(\\max(0,x)\\) thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).</li> <li>POOL Layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].</li> <li>FC Layer (i.e. fully-connected) will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10.</li> </ul> <p> Activations of an ConvNet Architecture </p>"},{"location":"dl/cnn/#convolutional-layer","title":"Convolutional Layer","text":"<p>Definition (Filter - Receptive Fields)</p> <p>The Filter, or the Receptive Field, in the context of CNN, is a \\(F \\times F \\times 3\\) square with which we use to multiply local regions in the image. </p> <p>Important Note (Intuition about Filters)</p> <p>Each filter is looking for a specific feature in the picture.</p> <p>Definition (Stride)</p> <p>Stride is the number of the pixel jumped when the filters slide. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.</p> <p>Important Note (Why Stride 1)</p> <p>Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.</p> <p>Definition (Zero-Padding)</p> <p>The zero-padding is a boarder around the input volume that only has element 0. Sometimes it will be convenient to pad the input volume with zeros around the border. The size of this zero-padding is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes</p> <p>Important Note (Why Padding?)</p> <p>Why use padding? In addition to keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be \u201cwashed away\u201d too quickly.</p> <p>Important Note (Computing Output volume)</p> <p>The Conv Layer:</p> <ul> <li>Accepts a volume of size \\( W_1 \\times H_1 \\times D_1 \\)</li> <li>Requires four hyperparameters:     \u00a0\u00a0\u00a0\u00a01. Number of filters \\( K \\),     \u00a0\u00a0\u00a0\u00a02. their spatial extent \\( F \\),     \u00a0\u00a0\u00a0\u00a03. the stride \\( S \\),     \u00a0\u00a0\u00a0\u00a04. the amount of zero padding \\( P \\).</li> <li>Produces a volume of size \\( W_2 \\times H_2 \\times D_2 \\) where:  \\( W_2 = \\left(\\frac{W_1 - F + 2P}{S}\\right) + 1 \\) \\( H_2 = \\left(\\frac{H_1 - F + 2P}{S}\\right) + 1 \\) \u00a0 (i.e. width and height are computed equally by symmetry)  \\( D_2 = K \\)</li> <li>With parameter sharing, it introduces \\( F \\cdot F \\cdot D_1 \\) weights per filter, for a total of \\( (F \\cdot F \\cdot D_1) \\cdot K \\) weights and \\( K \\) biases.</li> <li>In the output volume, the \\( d \\)-th depth slice (of size \\( W_2 \\times H_2 \\)) is the result of performing a valid convolution of the \\( d \\)-th filter over the input volume with a stride of \\( S \\), and then offset by \\( d \\)-th bias.</li> </ul> <p>A common setting of the hyperparameters is \\( F = 3 \\), \\( S = 1 \\), \\( P = 1 \\).</p> <p>Important Note (Convolution Demo)</p> <p>Below is a running demo of a CONV layer. The input volume is of size \\( W_1 = 5 \\), \\( H_1 = 5 \\), \\( D_1 = 3 \\), and the CONV layer parameters are \\( K = 2 \\), \\( F = 3 \\), \\( S = 2 \\), \\( P = 1 \\). Therefore, the output volume size has spatial size \\( (5 - 3 + 2)/2 + 1 = 3 \\). The visualization below iterates over the output activations (green), and shows that each element is computed by elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias.</p> <p> </p> <p>Important Note (Implementation as Matrix Multiplication)</p> <p>A common implementation pattern of the CONV layer is to formulate the forward pass of a convolutional layer as one big matrix multiply as follows:</p> <p>The local regions (blocks that have the same shape as the filter) in the input image are stretched out into columns in an operation commonly called im2col. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take blocks of shape [11x11x3] in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives \\(((227-11)/4+1)^2\\) = 3025 blocks, leading to an output matrix \\(X_{col}\\) of im2col of size [363 x 3025].  Remember that we are to multiply each column of \\(X_{col}\\) with the weights of the CONV Layer. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix \\(W_{row}\\) of size [96 x 363].  The result of a convolution is now equivalent to performing one large matrix multiply <code>np.dot(W_row, X_col)</code>. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location.  The result must finally be reshaped back to its proper output dimension [55x55x96].  The downside is that it can use a lot of memory, since some values in the input volume are replicated multiple times in \\(X_{col}\\). The benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (e.g. BLAS API). </p> <p>Note (1x1 Convolution)</p> <p>As an aside, several papers use 1x1 convolutions, as first investigated by Network in Network.</p> <p>Important Note (Implementing Convolution Layer)</p> <pre><code>def conv_forward_naive(x, w, b, conv_param):\n    \"\"\"A naive implementation of the forward pass for a convolutional layer.\n\n    The input consists of N data points, each with C channels, height H and\n    width W. We convolve each input with F different filters, where each filter\n    spans all C channels and has height HH and width WW.\n\n    Input:\n    - x: Input data of shape (N, C, H, W)\n    - w: Filter weights of shape (F, C, HH, WW)\n    - b: Biases, of shape (F,)\n    - conv_param: A dictionary with the following keys:\n    - 'stride': The number of pixels between adjacent receptive fields in the\n        horizontal and vertical directions.\n    - 'pad': The number of pixels that will be used to zero-pad the input.\n\n    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n    along the height and width axes of the input. Be careful not to modfiy the original\n    input x directly.\n\n    Returns a tuple of:\n    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n    H' = 1 + (H + 2 * pad - HH) / stride\n    W' = 1 + (W + 2 * pad - WW) / stride\n    - cache: (x, w, b, conv_param)\n    \"\"\"\n    out = None\n\n    N, C, H, W = x.shape\n    F, _, HH, WW = w.shape\n    stride, pad = conv_param['stride'], conv_param['pad']\n    H_out = 1 + (H + 2 * pad - HH) // stride\n    W_out = 1 + (W + 2 * pad - WW) // stride\n    out = np.zeros((N, F, H_out, W_out))\n\n    for image_index in range(N):\n        image = x[image_index]\n\n        # Create a new matrix with the padded dimensions\n        padded_image = np.zeros((C, H + 2 * pad, W + 2 * pad))\n\n        # Insert the original image into the padded matrix\n        padded_image[:, pad:pad+H, pad:pad+W] = image\n\n        _, padded_H, padded_W = padded_image.shape\n\n        for filter_index in range(F):\n            _filter = w[filter_index]\n            for i in range(0, padded_H-HH+1, stride):\n                for j in range(0, padded_W-WW+1, stride):\n                    # Extract the region of the padded image corresponding to the filter's location\n                    region = padded_image[:, i:i+HH, j:j+WW]\n\n                    # Perform element-wise multiplication and sum the result\n                    out[image_index][filter_index][i//stride][j//stride] = np.sum(region * _filter) + b[filter_index]             \n\n    cache = (x, w, b, conv_param)\n\n    print(out)\n\n    return out, cache\n\ndef conv_backward_naive(dout, cache):\n    \"\"\"A naive implementation of the backward pass for a convolutional layer.\n\n    Inputs:\n    - dout: Upstream derivatives of shape (N, F, H_out, W_out).\n    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n\n    Returns a tuple of:\n    - dx: Gradient with respect to x, of shape (N, C, H, W)\n    - dw: Gradient with respect to w, of shape (F, C, HH, WW)\n    - db: Gradient with respect to b, of shape (F,)\n    \"\"\"\n    x, w, b, conv_param = cache\n    N, C, H, W = x.shape\n    F, _, HH, WW = w.shape\n    stride, pad = conv_param['stride'], conv_param['pad']\n    H_out = 1 + (H + 2 * pad - HH) // stride\n    W_out = 1 + (W + 2 * pad - WW) // stride\n\n    # Initialize gradients\n    dx = np.zeros_like(x)\n    dw = np.zeros_like(w)\n    db = np.zeros_like(b)\n\n    # Pad the input x and dx\n    padded_x = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n    padded_dx = np.zeros_like(padded_x)\n\n    # Compute db\n    db = np.sum(dout, axis=(0, 2, 3))\n\n    # Compute dw and dx\n    for image_index in range(N):\n        image = padded_x[image_index]\n        dimage = padded_dx[image_index]\n\n        for filter_index in range(F):\n            _filter = w[filter_index]\n            dout_filter = dout[image_index, filter_index]\n\n            for i in range(H_out):\n                for j in range(W_out):\n                    # Calculate the current region\n                    i_start = i * stride\n                    j_start = j * stride\n                    i_end = i_start + HH\n                    j_end = j_start + WW\n\n                    region = image[:, i_start:i_end, j_start:j_end]\n\n                    # Update the gradient for w (dw)\n                    dw[filter_index] += region * dout_filter[i, j]\n\n                    # Update the gradient for x (dx)\n                    dimage[:, i_start:i_end, j_start:j_end] += _filter * dout_filter[i, j]\n\n        # Remove padding from the gradient for x\n        dx[image_index] = dimage[:, pad:-pad, pad:-pad]\n\n    return dx, dw, db\n</code></pre>"},{"location":"dl/cnn/#relu-layer","title":"Relu Layer","text":"<p>Important Note (Implementing Relu Layer)</p> <pre><code>def relu_forward(x):\n    \"\"\"Computes the forward pass for a layer of rectified linear units (ReLUs).\n\n    Input:\n    - x: Inputs, of any shape\n\n    Returns a tuple of:\n    - out: Output, of the same shape as x\n    - cache: x\n    \"\"\"\n    out = None\n\n    out = np.maximum(0, x)\n\n    cache = x\n    return out, cache\n\n\ndef relu_backward(dout, cache):\n    \"\"\"Computes the backward pass for a layer of rectified linear units (ReLUs).\n\n    Input:\n    - dout: Upstream derivatives, of any shape\n    - cache: Input x, of same shape as dout\n\n    Returns:\n    - dx: Gradient with respect to x\n    \"\"\"\n    dx, x = None, cache\n\n    dx = dout * (x &gt; 0)\n\n    return dx\n</code></pre>"},{"location":"dl/cnn/#pooling-layer","title":"Pooling Layer","text":"<p>Definition (Pooling Layer)</p> <p>It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. The goal is: To progressively reduce the spatial size of the representation, thus to reduce the amount of parameters and computation, and hence to control overfitting. We use MAX operation to achieve these goals.  The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer:  </p> <ul> <li>Accepts a volume of size \\( W_1 \\times H_1 \\times D_1 \\).</li> <li>Requires two hyperparameters:  \u00a0\u00a0\u00a0\u00a01. their spatial extent \\( F \\),  \u00a0\u00a0\u00a0\u00a02. the stride \\( S \\). </li> <li>Produces a volume of size \\( W_2 \\times H_2 \\times D_2 \\) where:  \\( W_2 = \\left(\\frac{W_1 - F}{S}\\right) + 1 \\) \\( H_2 = \\left(\\frac{H_1 - F}{S}\\right) + 1 \\) \\( D_2 = D_1 \\)</li> <li>Introduces zero parameters since it computes a fixed function of the input.</li> <li>For Pooling layers, it is not common to pad the input using zero-padding.</li> </ul> <p>In most cases, \\( F = 3 \\), \\( S = 2 \\) (also called overlapping pooling), or more commonly \\( F = 2 \\), \\( S = 2 \\). Pooling sizes with larger receptive fields are too destructive.</p> <p> Pooling Layer </p> <p>Important Note (Implementing Pooling Layer)</p> <pre><code>def max_pool_forward_naive(x, pool_param):\n    \"\"\"A naive implementation of the forward pass for a max-pooling layer.\n\n    Inputs:\n    - x: Input data, of shape (N, C, H, W)\n    - pool_param: dictionary with the following keys:\n    - 'pool_height': The height of each pooling region\n    - 'pool_width': The width of each pooling region\n    - 'stride': The distance between adjacent pooling regions\n\n    No padding is necessary here, eg you can assume:\n    - (H - pool_height) % stride == 0\n    - (W - pool_width) % stride == 0\n\n    Returns a tuple of:\n    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n    H' = 1 + (H - pool_height) // stride\n    W' = 1 + (W - pool_width) // stride\n    - cache: (x, pool_param)\n    \"\"\"\n    N, C, H, W = x.shape\n    pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n    H_out = 1 + (H - pool_height) // stride\n    W_out = 1 + (W - pool_width) // stride\n\n    out = np.zeros((N, C, H_out, W_out))\n\n    for image_index in range(N):\n        for c in range(C):\n            for i in range(H_out):\n                for j in range(W_out):\n                    i_start = i * stride\n                    j_start = j * stride\n                    i_end = i_start + pool_height\n                    j_end = j_start + pool_width\n\n                    # Extract the region to pool\n                    region = x[image_index, c, i_start:i_end, j_start:j_end]\n\n                    # Perform max pooling\n                    out[image_index, c, i, j] = np.max(region)\n\n    cache = (x, pool_param)\n    return out, cache\n\ndef max_pool_backward_naive(dout, cache):\n    \"\"\"A naive implementation of the backward pass for a max-pooling layer.\n\n    Inputs:\n    - dout: Upstream derivatives of shape (N, C, H_out, W_out)\n    - cache: A tuple of (x, pool_param) as in the forward pass.\n\n    Returns:\n    - dx: Gradient with respect to x, of shape (N, C, H, W)\n    \"\"\"\n    x, pool_param = cache\n    N, C, H, W = x.shape\n    pool_height, pool_width, stride = pool_param['pool_height'], pool_param['pool_width'], pool_param['stride']\n    H_out = 1 + (H - pool_height) // stride\n    W_out = 1 + (W - pool_width) // stride\n\n    dx = np.zeros_like(x)\n\n    for image_index in range(N):\n        for c in range(C):\n            for i in range(H_out):\n                for j in range(W_out):\n                    i_start = i * stride\n                    j_start = j * stride\n                    i_end = i_start + pool_height\n                    j_end = j_start + pool_width\n\n                    # Extract the region of the input that was pooled\n                    region = x[image_index, c, i_start:i_end, j_start:j_end]\n\n                    # Find the mask of the maximum value in the region\n                    mask = (region == np.max(region))\n\n                    # Distribute the gradient from dout to the corresponding max location in dx\n                    dx[image_index, c, i_start:i_end, j_start:j_end] += mask * dout[image_index, c, i, j]\n\n    return dx\n</code></pre>"},{"location":"dl/cnn/#fully-connected-fc-layer","title":"Fully Connected (FC) Layer","text":"<p>Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.</p> <p>Important Note (Converting FC Layers to CONV Layers)</p> <p>Note that the neurons in both FC layers and CONV layers compute dot products, so their functional form is identical. Therefore, it turns out that it\u2019s possible to convert between FC and CONV layers. Of these two conversions, the ability to convert an FC layer to a CONV layer is particularly useful in practice. For example, an FC layer with \\( K = 4096 \\) that is looking at some input volume of size \\( 7 \\times 7 \\times 512 \\) can be equivalently expressed as a CONV layer with \\( F = 7, P = 0, S = 1, K = 4096 \\). In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be \\( 1 \\times 1 \\times 4096 \\) since only a single depth column \u201cfits\u201d across the input volume, giving identical result as the initial FC layer.</p> <p>Example (FC-CONV Conversion in AlexNet)</p> <p>Consider a ConvNet architecture that takes a 224x224x3 image, and then uses a series of CONV layers and POOL layers to reduce the image to an activations volume of size 7x7x512 (in an AlexNet architecture that we\u2019ll see later, this is done by use of 5 pooling layers that downsample the input spatially by a factor of two each time, making the final spatial size 224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC layers of size 4096 and finally the last FC layers with 1000 neurons that compute the class scores. We can convert each of these three FC layers to CONV layers as described above: </p> <ul> <li>Replace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size \\( F = 7 \\), giving output volume [1x1x4096].</li> <li>Replace the second FC layer with a CONV layer that uses filter size \\( F = 1 \\), giving output volume [1x1x4096].</li> <li>Replace the last FC layer similarly, with \\( F = 1 \\), giving final output [1x1x1000].</li> </ul> <p>Each of these conversions could in practice involve manipulating (e.g. reshaping) the weight matrix \\( W \\) in each FC layer into CONV layer filters. It turns out that this conversion allows us to \u201cslide\u201d the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass.</p> <p>For example, if 224x224 image gives a volume of size [7x7x512] - i.e. a reduction by 32, then forwarding an image of size 384x384 through the converted architecture would give the equivalent volume in size [12x12x512], since 384/32 = 12. Following through with the next 3 CONV layers that we just converted from FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 + 1 = 6. Note that instead of a single vector of class scores of size [1x1x1000], we\u2019re now getting an entire 6x6 array of class scores across the 384x384 image.</p> <p>Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.</p> <p>Naturally, forwarding the converted ConvNet a single time is much more efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick is often used in practice to get better performance, where for example, it is common to resize an image to make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the class scores.</p> <p>Important Note (Implementing Affine Layer)</p> <pre><code>def affine_forward(x, w, b):\n    \"\"\"Computes the forward pass for an affine (fully connected) layer.\n\n    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n    then transform it to an output vector of dimension M.\n\n    Inputs:\n    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n    - w: A numpy array of weights, of shape (D, M)\n    - b: A numpy array of biases, of shape (M,)\n\n    Returns a tuple of:\n    - out: output, of shape (N, M)\n    - cache: (x, w, b)\n    \"\"\"\n    out = None\n\n    out = x.reshape(len(x), -1) @ w + b\n\n    cache = (x, w, b)\n    return out, cache\n\ndef affine_backward(dout, cache):\n    \"\"\"Computes the backward pass for an affine (fully connected) layer.\n\n    Inputs:\n    - dout: Upstream derivative, of shape (N, M)\n    - cache: Tuple of:\n    - x: Input data, of shape (N, d_1, ... d_k)\n    - w: Weights, of shape (D, M)\n    - b: Biases, of shape (M,)\n\n    Returns a tuple of:\n    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n    - dw: Gradient with respect to w, of shape (D, M)\n    - db: Gradient with respect to b, of shape (M,)\n    \"\"\"\n    x, w, b = cache\n    dx, dw, db = None, None, None\n\n    dx = (dout @ w.T).reshape(x.shape)\n    dw = x.reshape(len(x), -1).T @ dout\n    db = dout.sum(axis=0)\n\n    return dx, dw, db\n</code></pre>"},{"location":"dl/cnn/#softmax-loss","title":"Softmax Loss","text":"<p>Important Note (Implementing Softmax Loss)</p> <pre><code>def softmax_loss(x, y):\n    \"\"\"Computes the loss and gradient for softmax classification.\n\n    Inputs:\n    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n    class for the ith input.\n    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n    0 &lt;= y[i] &lt; C\n\n    Returns a tuple of:\n    - loss: Scalar giving the loss\n    - dx: Gradient of the loss with respect to x\n    \"\"\"\n    loss, dx = None, None\n\n    N = len(y) # number of samples\n\n    P = np.exp(x - x.max(axis=1, keepdims=True)) # numerically stable exponents\n    P /= P.sum(axis=1, keepdims=True)            # row-wise probabilities (softmax)\n\n    loss = -np.log(P[range(N), y]).sum() / N     # sum cross entropies as loss\n\n    P[range(N), y] -= 1\n    dx = P / N\n\n    return loss, dx\n</code></pre>"},{"location":"dl/cnn/#regularization","title":"Regularization","text":"<p>Important Note (Implementing Batchnorm)</p> <pre><code>def batchnorm_forward(x, gamma, beta, bn_param):\n    \"\"\"Forward pass for batch normalization.\n\n    During training the sample mean and (uncorrected) sample variance are\n    computed from minibatch statistics and used to normalize the incoming data.\n    During training we also keep an exponentially decaying running mean of the\n    mean and variance of each feature, and these averages are used to normalize\n    data at test-time.\n\n    At each timestep we update the running averages for mean and variance using\n    an exponential decay based on the momentum parameter:\n\n    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n    running_var = momentum * running_var + (1 - momentum) * sample_var\n\n    Note that the batch normalization paper suggests a different test-time\n    behavior: they compute sample mean and variance for each feature using a\n    large number of training images rather than using a running average. For\n    this implementation we have chosen to use running averages instead since\n    they do not require an additional estimation step; the torch7\n    implementation of batch normalization also uses running averages.\n\n    Input:\n    - x: Data of shape (N, D)\n    - gamma: Scale parameter of shape (D,)\n    - beta: Shift paremeter of shape (D,)\n    - bn_param: Dictionary with the following keys:\n    - mode: 'train' or 'test'; required\n    - eps: Constant for numeric stability\n    - momentum: Constant for running mean / variance.\n    - running_mean: Array of shape (D,) giving running mean of features\n    - running_var Array of shape (D,) giving running variance of features\n\n    Returns a tuple of:\n    - out: of shape (N, D)\n    - cache: A tuple of values needed in the backward pass\n    \"\"\"\n    mode = bn_param[\"mode\"]\n    eps = bn_param.get(\"eps\", 1e-5)\n    momentum = bn_param.get(\"momentum\", 0.9)\n\n    N, D = x.shape\n    running_mean = bn_param.get(\"running_mean\", np.zeros(D, dtype=x.dtype))\n    running_var = bn_param.get(\"running_var\", np.zeros(D, dtype=x.dtype))\n\n    out, cache = None, None\n    if mode == \"train\":\n        mu = x.mean(axis=0)\n        var = x.var(axis=0)\n        std = np.sqrt(var + eps)\n        x_new = (x - mu) / std\n        out = gamma * x_new + beta\n\n        shape = bn_param.get('shape', (N, D))            # reshape used in backprop\n        axis = bn_param.get('axis', 0)                # axis to sum used in backprop\n        cache = x, mu, var, std, gamma, x_new, shape, axis   # save for backprop\n\n        if axis == 0:                                                    # if not batchnorm\n            running_mean = momentum * running_mean + (1 - momentum) * mu # update overall mean\n            running_var = momentum * running_var + (1 - momentum) * var  # update overall variance\n\n    elif mode == \"test\":\n        x_new = (x - running_mean) / np.sqrt(running_var + eps)\n        out = gamma * x_new + beta\n\n    else:\n        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n\n    # Store the updated running means back into bn_param\n    bn_param[\"running_mean\"] = running_mean\n    bn_param[\"running_var\"] = running_var\n\n    return out, cache\n\ndef batchnorm_backward(dout, cache):\n    \"\"\"Backward pass for batch normalization.\n\n    For this implementation, you should write out a computation graph for\n    batch normalization on paper and propagate gradients backward through\n    intermediate nodes.\n\n    Inputs:\n    - dout: Upstream derivatives, of shape (N, D)\n    - cache: Variable of intermediates from batchnorm_forward.\n\n    Returns a tuple of:\n    - dx: Gradient with respect to inputs x, of shape (N, D)\n    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n    \"\"\"\n    dx, dgamma, dbeta = None, None, None\n\n    x, mu, var, std, gamma, x_hat, shape, axis = cache          # expand cache\n\n    dbeta = dout.reshape(shape, order='F').sum(axis)            # derivative w.r.t. beta\n    dgamma = (dout * x_hat).reshape(shape, order='F').sum(axis) # derivative w.r.t. gamma\n\n    dx_hat = dout * gamma                                       # derivative w.t.r. x_hat\n    dstd = -np.sum(dx_hat * (x-mu), axis=0) / (std**2)          # derivative w.t.r. std\n    dvar = 0.5 * dstd / std                                     # derivative w.t.r. var\n    dx1 = dx_hat / std + 2 * (x-mu) * dvar / len(dout)          # partial derivative w.t.r. dx\n    dmu = -np.sum(dx1, axis=0)                                  # derivative w.t.r. mu\n    dx2 = dmu / len(dout)                                       # partial derivative w.t.r. dx\n    dx = dx1 + dx2                                              # full derivative w.t.r. x\n\n    return dx, dgamma, dbeta\n\ndef batchnorm_backward_alt(dout, cache):\n    \"\"\"Alternative backward pass for batch normalization.\n\n    For this implementation you should work out the derivatives for the batch\n    normalizaton backward pass on paper and simplify as much as possible. You\n    should be able to derive a simple expression for the backward pass.\n    See the jupyter notebook for more hints.\n\n    Note: This implementation should expect to receive the same cache variable\n    as batchnorm_backward, but might not use all of the values in the cache.\n\n    Inputs / outputs: Same as batchnorm_backward\n    \"\"\"\n    dx, dgamma, dbeta = None, None, None\n\n    _, _, _, std, gamma, x_hat, shape, axis = cache # expand cache\n    S = lambda x: x.sum(axis=0)                     # helper function\n\n    dbeta = dout.reshape(shape, order='F').sum(axis)            # derivative w.r.t. beta\n    dgamma = (dout * x_hat).reshape(shape, order='F').sum(axis) # derivative w.r.t. gamma\n\n    dx = dout * gamma / (len(dout) * std)          # temporarily initialize scale value\n    dx = len(dout)*dx  - S(dx*x_hat)*x_hat - S(dx) # derivative w.r.t. unnormalized x\n\n    return dx, dgamma, dbeta\n</code></pre> <p>Important Note (Implementing Layernorm)</p> <pre><code>def layernorm_forward(x, gamma, beta, ln_param):\n    \"\"\"Forward pass for layer normalization.\n\n    During both training and test-time, the incoming data is normalized per data-point,\n    before being scaled by gamma and beta parameters identical to that of batch normalization.\n\n    Note that in contrast to batch normalization, the behavior during train and test-time for\n    layer normalization are identical, and we do not need to keep track of running averages\n    of any sort.\n\n    Input:\n    - x: Data of shape (N, D)\n    - gamma: Scale parameter of shape (D,)\n    - beta: Shift paremeter of shape (D,)\n    - ln_param: Dictionary with the following keys:\n        - eps: Constant for numeric stability\n\n    Returns a tuple of:\n    - out: of shape (N, D)\n    - cache: A tuple of values needed in the backward pass\n    \"\"\"\n    out, cache = None, None\n    eps = ln_param.get(\"eps\", 1e-5)\n\n    bn_param = {\"mode\": \"train\", \"axis\": 1, **ln_param} # same as batchnorm in train mode + over which axis to sum for grad\n    [gamma, beta] = np.atleast_2d(gamma, beta)          # assure 2D to perform transpose\n\n    out, cache = batchnorm_forward(x.T, gamma.T, beta.T, bn_param) # same as batchnorm\n    out = out.T                                                    # transpose back\n\n    return out, cache\n\ndef layernorm_backward(dout, cache):\n    \"\"\"Backward pass for layer normalization.\n\n    For this implementation, you can heavily rely on the work you've done already\n    for batch normalization.\n\n    Inputs:\n    - dout: Upstream derivatives, of shape (N, D)\n    - cache: Variable of intermediates from layernorm_forward.\n\n    Returns a tuple of:\n    - dx: Gradient with respect to inputs x, of shape (N, D)\n    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n    \"\"\"\n    dx, dgamma, dbeta = None, None, None\n    dx, dgamma, dbeta = batchnorm_backward_alt(dout.T, cache) # same as batchnorm backprop\n    dx = dx.T\n\n    return dx, dgamma, dbeta\n</code></pre>"},{"location":"dl/gaussian/","title":"Gaussian Processes","text":"<ul> <li> Gaussian Processes</li> </ul>"},{"location":"dl/generative/","title":"Generative Learning Algorithms","text":""},{"location":"dl/generative/#discriminative-vs-generative","title":"Discriminative v.s. Generative","text":"<p>Definition (Discriminative Learning Algorithms)</p> <p>Algorithms that try to learn \\(p(y\\mid x)\\) directly (such as logistic regression), or algorithms that try to learn mappings directly from the space of inputs \\(X\\) to the labels \\(\\{0, 1\\}\\), (such as the perceptron algorithm) are called Discriminative Learning Algorithms.</p> <p>Note (Intuition of Discriminative Learning Algorithms)</p> <p>Consider a classification problem in which we want to learn to distinguish between elephants (\\(y = 1\\)) and dogs (\\(y = 0\\)), based on some features of an animal. Given a training set, an algorithm like logistic regression or the perceptron algorithm (basically) tries to find a straight line\u2014that is, a decision boundary\u2014that separates the elephants and dogs. Then, to classify a new animal as either an elephant or a dog, it checks on which side of the decision boundary it falls, and makes its prediction accordingly.</p> <p>Definition (Generative Learning Algorithms)</p> <p>Instead of learning \\(p(y\\mid x)\\), Generative Learning Algorithms tries to model \\(p(x\\mid y)\\) (and \\(p(y)\\)). For instance, if \\(y\\) indicates whether an example is a dog (\\(0\\)) or an elephant (\\(1\\)), then \\(p(x\\mid y = 0)\\) models the distribution of dogs\u2019 features, and \\(p(x\\mid y = 1)\\) models the distribution of elephants\u2019 features.</p> <p>Note (Intuition of Generative Learning Algorithms)</p> <p>First, looking at elephants, we can build a model of what elephants look like. Then, looking at dogs, we can build a separate model of what dogs look like. Finally, to classify a new animal, we can match the new animal against the elephant model, and match it against the dog model, to see whether the new animal looks more like the elephants or more like the dogs we had seen in the training set. </p> <p>Definition (Posterior Probability<sup>1</sup>)</p> <p>In variational Bayesian methods, the posterior probability is the probability of the parameters \\( \\theta \\) given the evidence \\( X \\), and is denoted \\( p(\\theta \\mid X) \\). It contrasts with the likelihood function, which is the probability of the evidence given the parameters: \\( p(X \\mid \\theta) \\).</p> <p>Note (Posterior Probability v.s. Likelihood Function<sup>1</sup>)</p> <p>Given a prior belief that a probability distribution function is \\( p(\\theta) \\) and that the observations \\( x \\) have a likelihood \\( p(x \\mid \\theta) \\), then the posterior probability is defined as</p> \\[ p(\\theta \\mid x) = \\frac{p(x \\mid \\theta)}{p(x)} p(\\theta), \\] <p>where \\(p(\\theta)\\) is called Prior Probability, and $\\( p(x) \\) is the normalizing constant and is calculated as</p> \\[ p(x) = \\int p(x \\mid \\theta) p(\\theta) d\\theta \\] <p>for continuous \\( \\theta \\), or by summing \\( p(x \\mid \\theta) p(\\theta) \\) over all possible values of \\( \\theta \\) for discrete \\( \\theta \\).</p> <p>The posterior probability is therefore proportional to the product Likelihood \\(\\cdot\\) Prior probability.</p> Example (Example of Posterior Probability<sup>1</sup>) <p>Suppose there is a school with 60% boys and 40% girls as students. The girls wear trousers or skirts in equal numbers; all boys wear trousers. An observer sees a (random) student from a distance; all the observer can see is that this student is wearing trousers. What is the probability this student is a girl? The correct answer can be computed using Bayes' theorem.</p> <p>The event \\( G \\) is that the student observed is a girl, and the event \\( T \\) is that the student observed is wearing trousers. To compute the posterior probability \\( P(G \\mid T) \\), we first need to know:</p> <ul> <li>\\( P(G) \\), or the probability that the student is a girl regardless of any other information. Since the observer sees a random student, meaning that all students have the same probability of being observed, and the percentage of girls among the students is 40%, this probability equals 0.4.</li> <li>\\( P(B) \\), or the probability that the student is not a girl (i.e. a boy) regardless of any other information (\\( B \\) is the complementary event to \\( G \\)). This is 60%, or 0.6.</li> <li>\\( P(T \\mid G) \\), or the probability of the student wearing trousers given that the student is a girl. As they are as likely to wear skirts as trousers, this is 0.5.</li> <li>\\( P(T \\mid B) \\), or the probability of the student wearing trousers given that the student is a boy. This is given as 1.</li> <li>\\( P(T) \\), or the probability of a (randomly selected) student wearing trousers regardless of any other information. Since \\( P(T) = P(T \\mid G)P(G) + P(T \\mid B)P(B) \\) (via the law of total probability), this is \\( P(T) = 0.5 \\times 0.4 + 1 \\times 0.6 = 0.8 \\).</li> </ul> <p>Given all this information, the posterior probability of the observer having spotted a girl given that the observed student is wearing trousers can be computed by substituting these values in the formula:</p> \\[ P(G \\mid T) = \\frac{P(T \\mid G)P(G)}{P(T)} = \\frac{0.5 \\times 0.4}{0.8} = 0.25 \\] <p>Important Note (GLA Logistic)</p> <p>When using GLAs to do a classification, we first use MLE to learn the joint probability \\(p(x,y)\\), which is to learn \\(p(y=0)\\) and \\(p(y=1)\\), the Class Priors, and \\(p(x\\mid y=0)\\) and \\(p(x\\mid y=1)\\). Then, we can use Bayes rule to derive the posterior distribution:</p> \\[\\begin{align*} p(y\\mid x) &amp;= \\frac{p(x\\mid y)p(y)}{p(x)} \\\\            &amp;= \\frac{p(x\\mid y)p(y)}{p(x\\mid y=1)p(y=1) + p(x\\mid y=0)p(y=0)} \\end{align*}\\] <p>We can calculate both situation where \\(y=1\\) or \\(y=0\\) and compare which has a larger possibility. This is how we \"compare\" which models best fit \\(x\\).</p> <p>Note that we don't actually need to calculate \\(p(x)\\), as</p> \\[ \\arg \\max_{y} p(y \\mid x) = \\arg \\max_{y} \\frac{p(x \\mid y) p(y)}{p(x)} = \\arg \\max_{y} p(x \\mid y) p(y). \\] <p>There are two different types of generative learning models: Gaussian Discriminant Analysis Model and Naive Bayes. We\u2019ll introduce them in the following.</p>"},{"location":"dl/generative/#gaussian-discriminant-analysis-gda","title":"Gaussian Discriminant Analysis (GDA)","text":"<p>Definition (Multivariate Normal Distribution)</p> <p>The multivariate normal distribution in \\( d \\)-dimensions, also called the multivariate Gaussian distribution, is parameterized by a mean vector \\( \\mu \\in \\mathbb{R}^d \\) and a covariance matrix \\( \\Sigma \\in \\mathbb{R}^{d \\times d} \\), where \\( \\Sigma \\geq 0 \\) is symmetric and positive semi-definite. Also written \\( \\mathcal{N}(\\mu, \\Sigma) \\), its density is given by:</p> \\[ p(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp\\left( -\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1}(x - \\mu) \\right). \\] <p>In the equation above, \\( |\\Sigma| \\) denotes the determinant of the matrix \\( \\Sigma \\).</p> <p>For a random variable \\( X \\) distributed \\( \\mathcal{N}(\\mu, \\Sigma) \\), the mean is (unsurprisingly) given by \\( \\mu \\):</p> \\[ \\mathbb{E}[X] = \\int x \\, p(x; \\mu, \\Sigma) \\, dx = \\mu. \\] <p>The covariance of a vector-valued random variable \\( Z \\) is defined as \\( \\mathrm{Cov}(Z) = \\mathbb{E}[(Z - \\mathbb{E}[Z])(Z - \\mathbb{E}[Z])^T] \\). This generalizes the notion of the variance of a scalar.</p> <p>The covariance can also be defined as \\( \\mathrm{Cov}(Z) = \\mathbb{E}[ZZ^T] - (\\mathbb{E}[Z])(\\mathbb{E}[Z])^T \\). (You should be able to prove to yourself that these two definitions are equivalent.) If \\( X \\sim \\mathcal{N}(\\mu, \\Sigma) \\), then</p> \\[ \\mathrm{Cov}(X) = \\Sigma. \\] <p>Definition (Gaussian Discriminant Analysis Model)</p> <p>When we have a classification problem in which the input features \\( x \\) are continuous-valued random variables, we can then use the Gaussian Discriminant Analysis (GDA) model, which models \\( p(x \\mid y) \\) using a multivariate normal distribution. The model is:</p> \\[ \\begin{align*} y &amp;\\sim \\text{Bernoulli}(\\phi) \\\\ x \\mid y = 0 &amp;\\sim \\mathcal{N}(\\mu_0, \\Sigma) \\\\ x \\mid y = 1 &amp;\\sim \\mathcal{N}(\\mu_1, \\Sigma) \\end{align*} \\] <p>Writing out the distributions, this is:</p> \\[ \\begin{align*} p(y) &amp;= \\phi^y (1 - \\phi)^{1 - y} \\\\ p(x \\mid y = 0) &amp;= \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_0)^T \\Sigma^{-1} (x - \\mu_0) \\right) \\\\ p(x \\mid y = 1) &amp;= \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}} \\exp \\left( -\\frac{1}{2} (x - \\mu_1)^T \\Sigma^{-1} (x - \\mu_1) \\right) \\end{align*} \\] <p>Here, the parameters of our model are \\( \\phi \\), \\( \\Sigma \\), \\( \\mu_0 \\) and \\( \\mu_1 \\). (Note that while there are two different mean vectors \\( \\mu_0 \\) and \\( \\mu_1 \\), this model is usually applied using only one covariance matrix \\( \\Sigma \\).) The log-likelihood of the data is given by</p> \\[ \\begin{align*} \\ell (\\phi, \\mu_0, \\mu_1, \\Sigma) &amp;= \\log \\prod_{i=1}^n p(x^{(i)}, y^{(i)} ; \\phi, \\mu_0, \\mu_1, \\Sigma) \\\\ &amp;= \\log \\prod_{i=1}^n p(x^{(i)} \\mid y^{(i)} ; \\mu_0, \\mu_1, \\Sigma) p(y^{(i)} ; \\phi). \\end{align*} \\] <p> Probability Distributions of The Data </p>"},{"location":"dl/generative/#naive-bayes-nb","title":"Naive Bayes (NB)","text":"<p>For our motivating example, consider building an email spam filter. Here, we wish to classify messages according to whether they are unsolicited commercial (spam) email, or non-spam email. Classifying emails is one example of a broader set of problems called text classification.</p> <p>Let's say we have a training set (a set of emails labeled as spam or non-spam). We'll begin our construction of our spam filter by specifying the features \\( x_j \\) used to represent an email.</p> <p>We will represent an email via a feature vector whose length is equal to the number of distinct words in the training set. Specifically, if an email contains the \\( j \\)-th word of the dictionary, then we will set \\( x_j = 1 \\); otherwise, we let \\( x_j = 0 \\). For instance, the vector</p> \\[ x = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0 \\\\ \\end{bmatrix} \\] <p>is used to represent an email that contains the words \"a\" and \"buy.\" The set of words encoded into the feature vector is called the vocabulary, so the dimension of \\( x \\) is equal to the size of the vocabulary.</p> <p>Having chosen our feature vector, we now want to build a generative model. So, we have to model \\( p(x \\mid y) \\). But if we have, say, a vocabulary of 50000 words, then \\( x \\in \\{0, 1\\}^{50000} \\) (i.e. a 50000-dimensional vector of 0's and 1's), and if we were to model \\( x \\) explicitly with a multinomial distribution over the \\( 2^{50000} \\) possible outcomes, then we'd end up with a \\( (2^{50000} - 1) \\)-dimensional parameter vector. This is clearly too many parameters.</p> <p>Definition (Naive Bayes Assumption)</p> <p>To model \\( p(x \\mid y) \\), we make a very strong assumption: \\( x_i \\)'s are conditionally independent given \\( y \\). This assumption is called the Naive Bayes Assumption, and the resulting algorithm is called the Naive Bayes classifier. Generally. </p> \\[ P[X^{(1)} \\ldots X^{(n)} \\mid Y] = \\prod_{i=1}^{n} P[X^{(i)} \\mid Y] \\] Example (Example of NB Assumption) <p>If \\( y = 1 \\) means spam email; \"buy\" is word 2087 and \"price\" is word 39831; then we are assuming that if I tell you \\( y = 1 \\) (that a particular piece of email is spam), then knowledge of \\( x_{2087} \\) (knowledge of whether \"buy\" appears in the message) will have no effect on your beliefs about the value of \\( x_{39831} \\) (whether \"price\" appears). More formally, this can be written \\( p(x_{2087} \\mid y) = p(x_{2087} \\mid y, x_{39831}) \\). (Note that this is not the same as saying that \\( x_{2087} \\) and \\( x_{39831} \\) are independent, which would have been written \\( p(x_{2087}) = p(x_{2087} \\mid x_{39831}) \\); rather, we are only assuming that \\( x_{2087} \\) and \\( x_{39831} \\) are conditionally independent given \\( y \\).)</p> <p>We now have:</p> \\[ \\begin{align*} p(x_1, \\ldots, x_{50000} \\mid y) &amp;= p(x_1 \\mid y) p(x_2 \\mid y, x_1) p(x_3 \\mid y, x_1, x_2) \\cdots p(x_{50000} \\mid y, x_1, \\ldots, x_{49999}) \\\\ &amp;= p(x_1 \\mid y) p(x_2 \\mid y) p(x_3 \\mid y) \\cdots p(x_{50000} \\mid y) \\\\ &amp;= \\prod_{j=1}^{50000} p(x_j \\mid y) \\end{align*} \\] <p>The first equality simply follows from the usual properties of probabilities, and the second equality used the NB assumption. We note that even though the Naive Bayes assumption is an extremely strong assumption, the resulting algorithm works well on many problems.</p> <p>Model (Naive Bayes Classifier)</p> <p>Our model is parameterized by \\( \\phi_{j \\mid y=1} = p(x_j = 1 \\mid y = 1) \\), \\( \\phi_{j \\mid y=0} = p(x_j = 1 \\mid y = 0) \\), and \\( \\phi_y = p(y = 1) \\). As usual, given a training set \\( \\{ (x^{(i)}, y^{(i)}) ; i = 1, \\ldots, n \\} \\), we can write down the joint likelihood of the data:</p> \\[ \\mathcal{L}(\\phi_y, \\phi_{j \\mid y=0}, \\phi_{j \\mid y=1}) = \\prod_{i=1}^n p(x^{(i)}, y^{(i)}). \\] <p>Maximizing this with respect to \\( \\phi_y \\), \\( \\phi_{j \\mid y=0} \\), and \\( \\phi_{j \\mid y=1} \\) gives the maximum likelihood estimates:</p> \\[ \\begin{align*} \\phi_{j \\mid y=1} &amp;= \\frac{\\sum_{i=1}^n 1 \\{ x_j^{(i)} = 1 \\land y^{(i)} = 1 \\}}{\\sum_{i=1}^n 1 \\{ y^{(i)} = 1 \\}} \\\\ \\phi_{j \\mid y=0} &amp;= \\frac{\\sum_{i=1}^n 1 \\{ x_j^{(i)} = 1 \\land y^{(i)} = 0 \\}}{\\sum_{i=1}^n 1 \\{ y^{(i)} = 0 \\}} \\\\ \\phi_y &amp;= \\frac{\\sum_{i=1}^n 1 \\{ y^{(i)} = 1 \\}}{n} \\end{align*} \\] <p>The parameters have a very natural interpretation. For instance, \\( \\phi_{j \\mid y=1} \\) is just the fraction of the spam ( \\( y = 1 \\) ) emails in which word \\( j \\) does appear.</p> <p>Having fit all these parameters, to make a prediction on a new example with features \\( x \\), we then simply calculate</p> \\[ \\begin{align*} p(y = 1 \\mid x) &amp;= \\frac{p(x \\mid y = 1) p(y = 1)}{p(x)} \\\\ &amp;= \\frac{\\left( \\prod_{j=1}^d p(x_j \\mid y = 1) \\right) p(y = 1)}{\\left( \\prod_{j=1}^d p(x_j \\mid y = 1) \\right) p(y = 1) + \\left( \\prod_{j=1}^d p(x_j \\mid y = 0) \\right) p(y = 0)}, \\end{align*} \\] <p>and pick whichever class has the higher posterior probability.</p> <p>Remark (NB for Multiple Classes)</p> <p>We note that while we have developed the Naive Bayes algorithm mainly for the case of problems where the features \\( x_j \\) are binary-valued, the generalization to where \\( x_j \\) can take values in \\( \\{1, 2, \\ldots, k_j \\} \\) is straightforward. Here, we would simply model \\( p(x_j \\mid y) \\) as multinomial rather than as Bernoulli. Indeed, even if some original input attribute (say, the living area of a house, as in our earlier example) were continuous-valued, it is quite common to discretize it\u2014that is, turn it into a small set of discrete values\u2014and apply Naive Bayes. For instance, if we use some feature \\( x_j \\) to represent living area, we might discretize the continuous values as follows:</p> \\[ \\begin{array}{c|c|c|c|c|c} \\text{Living area (sq. feet)} &amp; &lt; 400 &amp; 400-800 &amp; 800-1200 &amp; 1200-1600 &amp; &gt; 1600 \\\\ x_i &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\\\ \\end{array} \\] <p>Thus, for a house with living area 890 square feet, we would set the value of the corresponding feature \\( x_j \\) to 3. We can then apply the Naive Bayes algorithm, and model \\( p(x_j \\mid y) \\) with a multinomial distribution, as described previously. When the original, continuous-valued attributes are not well-modeled by a multivariate normal distribution, discretizing the features and using Naive Bayes (instead of GDA) will often result in a better classifier.</p> <p>Important Note (Comparing GDA and NB)</p> <p>GDA: </p> <ul> <li>continuous \\(x \\in \\mathbb{R}^d\\) </li> <li>\\(p(x\\mid y) \\sim \\mathcal{N}(\\mu_y, \\Sigma)\\) </li> <li>\\(p(y\\mid x) = \\frac{1}{1+\\text{exp}(-\\theta^Tx)}\\) </li> </ul> <p>NB: </p> <ul> <li>discrete \\(x\\) </li> <li>conditional Independence Assumption: \\(p(x_j \\mid y, x_k) = p(x_j\\mid y)\\) </li> <li>Bernoulli Event Model: \\(p(x_j \\mid y) \\sim \\text{Bernoulli}[x_j \\text{is jth word in vocabulary}]\\) </li> <li>multinomial Event Model: \\(p(x_j \\mid y) \\sim \\text{Multinomial}[x_j \\text{is jth word in message}]\\)</li> </ul>"},{"location":"dl/generative/#laplace-smoothing","title":"Laplace Smoothing","text":"<p>The Naive Bayes algorithm as we have described it will work fairly well for many problems, but there is a simple change that makes it work much better, especially for text classification.</p> <ul> <li> Laplace Smoothing </li> </ul> <ol> <li> <p>Wikipedia: Posterior Probability \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"dl/glms/","title":"Generalized Linear Models","text":""},{"location":"dl/glms/#the-exponential-family","title":"The Exponential Family","text":"<p>We have already discussed regression problem and classification problem. Note that we assume, for example, in linear regression that the noise follow a Gaussian distribution, or in logistic regression that \\(y\\) is a Bernoulli distribution. However, not every data set is close to Gaussian distribution. It may be a better choice to choose some other distribution, such as Poisson distribution. After we change the distribution, we still need to get to its derivative or likelihood view. In this chapter, we are looking for a generalized way of viewing linear models. With this tech, we can shift to different distribution easily.</p> <p>Definition (The Exponential Family)</p> <p>A class of distributions is in the exponential family if it can be written in the form </p> \\[\\begin{align*} p(y;\\eta) = b(y)\\text{exp}(\\eta^TT(y)-a(\\eta)) \\end{align*}\\] <p>Here, \\(\\eta\\) is called the natural parameter (also called the canonical parameter) of the distribution; \\(T(y)\\) is the sufficient  statistic, which we normally set to be its identity function, i.e., \\(T(y) = y\\); and \\(a(\\eta)\\) is the log partition function. \\(e^{a(\\eta)}\\) essentially plays the role of a normalization constant, that make sure the distribution \\(p(y;\\eta)\\) sums/integrates over \\(y\\) to \\(1\\). </p> <p>A fixed choice of T, a and b defines a family (or set) of distributions that is parameterized by \u03b7; as we vary \u03b7, we then get different distributions within this family.</p> <p>Example (Bernoulli Distribution)</p> <p>We write the Bernoulli distribution as:</p> \\[ \\begin{align*}     p(y; \\phi) &amp;= \\phi^y (1 - \\phi)^{1-y} \\\\             &amp;= \\exp(y \\log \\phi + (1 - y) \\log (1 - \\phi)) \\\\             &amp;= \\exp \\left( \\left( \\log \\left( \\frac{\\phi}{1 - \\phi} \\right) \\right) y + \\log (1 - \\phi) \\right). \\end{align*} \\] <p>Thus, the natural parameter is given by \\(\\eta = \\log(\\phi / (1 - \\phi))\\). Interestingly, if we invert this definition for \\(\\eta\\) by solving for \\(\\phi\\) in terms of \\(\\eta\\), we obtain \\(\\phi = 1 / (1 + e^{-\\eta})\\). This is the familiar sigmoid function! This will come up again when we derive logistic regression as a GLM. To complete the formulation of the Bernoulli distribution as an exponential family distribution, we also have</p> \\[ \\begin{align*}     T(y) &amp;= y \\\\     a(\\eta) &amp;= -\\log(1 - \\phi) \\\\             &amp;= \\log(1 + e^{\\eta}) \\\\     b(y) &amp;= 1 \\end{align*} \\] <p>This shows that the Bernoulli distribution belongs to the exponential family.</p> <p>Example (Gaussian Distribution)</p> <p>Let's now move on to consider the Gaussian distribution. Recall that, when deriving linear regression, the value of \\(\\sigma^2\\) had no effect on our final choice of \\(\\theta\\) and \\(h_{\\theta}(x)\\). Thus, we can choose an arbitrary value for \\(\\sigma^2\\) without changing anything. To simplify the derivation below, let's set \\(\\sigma^2 = 1\\). We then have:</p> \\[ \\begin{align*}     p(y; \\mu) &amp;= \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{1}{2}(y - \\mu)^2 \\right) \\\\             &amp;= \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{1}{2} y^2 \\right) \\cdot \\exp \\left( \\mu y - \\frac{1}{2} \\mu^2 \\right) \\end{align*} \\] <p>Thus, we see that the Gaussian is in the exponential family, with</p> \\[ \\begin{align*}     \\eta &amp;= \\mu \\\\     T(y) &amp;= y \\\\     a(\\eta) &amp;= \\mu^2 / 2 \\\\             &amp;= \\eta^2 / 2 \\\\     b(y) &amp;= \\left( \\frac{1}{\\sqrt{2\\pi}} \\right) \\exp \\left( -y^2 / 2 \\right) \\end{align*} \\] <p>There're many other distributions that are members of the exponential family: The multinomial (which we'll see later), the Poisson (for modelling count-data; also see the problem set); the gamma and the exponential (for modelling continuous, non-negative random variables, such as time-intervals); the beta and the Dirichlet (for distributions over probabilities); and many more. </p>"},{"location":"dl/glms/#constructing-glms","title":"Constructing GLMs","text":"<ul> <li> Constructing GLMs</li> <li> Least Square Revisit</li> <li> Logistic Regression Revisit</li> </ul>"},{"location":"dl/kernel/","title":"Kernel Methods","text":"<p>Definition (Feature Map)</p> <p>In a non-linear model, we call the original input \\(x\\) the input attributes. The feature variables now become the output of a function \\(\\phi : {\\mathbb{R}}^d \\rightarrow {\\mathbb{R}}^p\\), where \\(d\\) is the #dimension of \\(x\\), and \\(p\\) is the #dimension of the feature. We call \\(\\phi\\) a Feature Map.</p> Example (Example of Feature Map) <p>We considered the problem of predicting the price of a house (denoted by \\( y \\)) from the living area of the house (denoted by \\( x \\)). The price \\( y \\) can be more accurately represented as a non-linear function of \\( x \\).</p> <p>Consider fitting cubic functions \\( y = \\theta_3 x^3 + \\theta_2 x^2 + \\theta_1 x + \\theta_0 \\). We can view the cubic function as a linear function over a different set of feature variables. Concretely, let the function \\( \\phi : \\mathbb{R} \\to \\mathbb{R}^4 \\) be defined as</p> \\[ \\phi(x) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ x^3 \\\\ \\end{bmatrix} \\in \\mathbb{R}^4. \\] <p>Let \\( \\theta \\in \\mathbb{R}^4 \\) be the vector containing \\( \\theta_0, \\theta_1, \\theta_2, \\theta_3 \\) as entries. Then we can rewrite the cubic function in \\( x \\) as:</p> \\[ \\theta_3 x^3 + \\theta_2 x^2 + \\theta_1 x + \\theta_0 = \\theta^T \\phi(x) \\] <p>Thus, a cubic function of the variable \\( x \\) can be viewed as a linear function over the variables \\( \\phi(x) \\).</p> <p>Important Note (LMS (Least Mean Squares) with Features)</p> <p>We will derive the gradient descent algorithm for fitting the model \\( \\theta^T \\phi(x) \\). First recall that for ordinary least square problem where we were to fit \\( \\theta^T x \\), the batch gradient descent update is:</p> \\[\\begin{align*} \\theta &amp;:= \\theta + \\alpha \\sum_{i=1}^{n} \\left( y^{(i)} - h_{\\theta}(x^{(i)}) \\right) x^{(i)} \\\\        &amp;:= \\theta + \\alpha \\sum_{i=1}^{n} \\left( y^{(i)} - \\theta^T x^{(i)} \\right) x^{(i)}. \\end{align*}\\] <p>Let \\( \\phi : \\mathbb{R}^d \\to \\mathbb{R}^p \\) be a feature map that maps attribute \\( x \\) (in \\( \\mathbb{R}^d \\)) to the features \\( \\phi(x) \\) in \\( \\mathbb{R}^p \\). (In the previous example, we have \\( d = 1 \\) and \\( p = 4 \\).) Now our goal is to fit the function \\( \\theta^T \\phi(x) \\), with \\( \\theta \\) being a vector in \\( \\mathbb{R}^p \\) instead of \\( \\mathbb{R}^d \\). We can replace all the occurrences of \\( x^{(i)} \\) in the algorithm above by \\( \\phi(x^{(i)}) \\) to obtain the new update:</p> \\[ \\theta := \\theta + \\alpha \\sum_{i=1}^{n} \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}). \\] <p>Similarly, the corresponding stochastic gradient descent update rule is</p> \\[ \\theta := \\theta + \\alpha \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}). \\] <p>The gradient descent update, or stochastic gradient update above becomes computationally expensive when the features \\(\\phi(x)\\) is high-dimensional. It may appear at first that such runtime per update and memory usage are inevitable, because the vector \\(\\phi\\) itself is of the same dimension as \\(\\phi(x)\\), and we may need to update every entry of \\(\\phi\\) and store it. However, we will introduce the kernel trick with which we will not need to store \\(\\phi\\) explicitly, and the runtime can be significantly improved.</p> <p>For simplicity, we assume the initialize the value \\( \\theta = 0 \\), and we focus on the iterative update. </p> <p>Statement</p> <p>\\( \\theta \\) can be represented as a linear combination of the vectors \\( \\phi(x^{(1)}), \\ldots, \\phi(x^{(n)}) \\). </p> Proof <p>We proceed by induction. The base case is: \\( \\theta = 0 = \\sum_{i=1}^{n} 0 \\cdot \\phi(x^{(i)}) \\). Assume at some point, \\( \\theta \\) can be represented as</p> \\[ \\theta = \\sum_{i=1}^{n} \\beta_i \\phi(x^{(i)}) \\] <p>for some \\( \\beta_1, \\ldots, \\beta_n \\in \\mathbb{R} \\). Then we claim that in the next round, \\( \\theta \\) is still a linear combination of \\( \\phi(x^{(1)}), \\ldots, \\phi(x^{(n)}) \\) because </p> \\[\\begin{align*} \\theta :&amp;= \\theta + \\alpha \\sum_{i=1}^{n} \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}) \\\\        &amp;= \\sum_{i=1}^{n} \\beta_i \\phi(x^{(i)}) + \\alpha \\sum_{i=1}^{n} \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\phi(x^{(i)}) \\\\        &amp;= \\sum_{i=1}^{n} \\left( \\beta_i + \\alpha \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\right) \\phi(x^{(i)}) \\end{align*}\\] <p>Important Note (Transforming GD into Updating \\( \\beta \\))</p> <p>Our general strategy is to implicitly represent the \\( p \\)-dimensional vector \\( \\theta \\) by a set of coefficients \\( \\beta_1, \\ldots, \\beta_n \\). By previous calculation, we derive the update rule of the coefficients \\( \\beta_1, \\ldots, \\beta_n \\):</p> \\[ \\beta_i := \\beta_i + \\alpha \\left( y^{(i)} - \\theta^T \\phi(x^{(i)}) \\right) \\] <p>Here we still have the old \\( \\theta \\) on the RHS of the equation. Replacing \\( \\theta = \\sum_{j=1}^{n} \\beta_j \\phi(x^{(j)}) \\) gives</p> \\[ \\forall i \\in \\{1, \\ldots, n\\}, \\beta_i := \\beta_i + \\alpha \\left( y^{(i)} - \\sum_{j=1}^{n} \\beta_j \\phi(x^{(j)})^T \\phi(x^{(i)}) \\right) \\] <p>We often rewrite \\( \\phi(x^{(j)})^T \\phi(x^{(i)}) \\) as \\( \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\rangle \\) to emphasize that it's the inner product of the two feature vectors. Viewing \\( \\beta_i \\)'s as the new representation of \\( \\theta \\), we have successfully translated the batch gradient descent algorithm into an algorithm that updates the value of \\( \\beta \\) iteratively. It may appear that at every iteration, we still need to compute the values of \\( \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\rangle \\) for all pairs of \\( i, j \\), each of which may take roughly \\( O(p) \\) operation. However, two important properties come to rescue:</p> <ol> <li> <p>We can pre-compute the pairwise inner products \\( \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\rangle \\) for all pairs of \\( i, j \\) before the loop starts.</p> </li> <li> <p>For many feature map \\( \\phi \\), computing \\( \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\rangle \\) can be efficient.</p> </li> </ol> <p>Important Note (Computing \\( \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\rangle \\))</p> <p>In previous example, we can compute \\( \\langle \\phi(x^{(j)}), \\phi(x^{(i)}) \\rangle \\) without computing \\( \\phi(x^{(i)}) \\) explicitly:</p> \\[\\begin{align*} \\langle \\phi(x), \\phi(z) \\rangle &amp;= 1 + \\sum_{i=1}^{d} x_i z_i + \\sum_{i,j \\in \\{1, \\ldots, d\\}} x_i x_j z_i z_j + \\sum_{i,j,k \\in \\{1, \\ldots, d\\}} x_i x_j x_k z_i z_j z_k \\\\ &amp;= 1 + \\sum_{i=1}^{d} x_i z_i + \\left( \\sum_{i=1}^{d} x_i z_i \\right)^2 + \\left( \\sum_{i=1}^{d} x_i z_i \\right)^3 \\\\ &amp;= 1 + \\langle x, z \\rangle + \\langle x, z \\rangle^2 + \\langle x, z \\rangle^3 \\end{align*}\\] <p>Therefore, to compute \\( \\langle \\phi(x), \\phi(z) \\rangle \\), we can first compute \\( \\langle x, z \\rangle \\) with \\( O(d) \\) time and then take another constant number of operations to compute \\( 1 + \\langle x, z \\rangle + \\langle x, z \\rangle^2 + \\langle x, z \\rangle^3 \\).</p> <p>Definition (Kernel)</p> <p>The inner products between the features \\( \\langle \\phi(x), \\phi(z) \\rangle \\) are essential throughout our calculation. Thus, we define the Kernel corresponding to the feature map \\( \\phi \\) as a function that maps \\( \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R} \\) satisfying: </p> \\[ K(x, z) \\triangleq \\langle \\phi(x), \\phi(z) \\rangle \\] <p>Note that \\( \\mathcal{X} \\) is the space of the input \\(x\\). In our example, \\( \\mathcal{X} = \\mathbb{R}^d \\).</p> <p>Model (Final Algorithm with Kernel)</p> <p>We write down the final algorithm as follows:</p> <ol> <li> <p>Compute all the values \\( K(x^{(i)}, x^{(j)}) \\triangleq \\langle \\phi(x^{(i)}), \\phi(x^{(j)}) \\rangle \\) for all \\( i, j \\in \\{1, \\ldots, n\\} \\). Set \\( \\beta := 0 \\).</p> </li> <li> <p>Loop:</p> </li> </ol> \\[ \\forall i \\in \\{1, \\ldots, n\\}, \\beta_i := \\beta_i + \\alpha \\left( y^{(i)} - \\sum_{j=1}^{n} \\beta_j K(x^{(i)}, x^{(j)}) \\right) \\] <p>Or in vector notation, letting \\( K \\) be the \\( n \\times n \\) matrix with \\( K_{ij} = K(x^{(i)}, x^{(j)}) \\), we have</p> \\[ \\beta := \\beta + \\alpha (\\vec{y} - K \\beta) \\] <p>With the algorithm above, we can update the representation \\( \\beta \\) of the vector \\( \\theta \\) efficiently with \\( O(n) \\) time per update. </p> <p>Important Note (Computing the Prediction with Kernel)</p> <p>The knowledge of the representation \\( \\beta \\) suffices to compute the prediction \\( \\theta^T \\phi(x) \\):</p> \\[ \\theta^T \\phi(x) = \\sum_{i=1}^{n} \\beta_i \\phi(x^{(i)})^T \\phi(x) = \\sum_{i=1}^{n} \\beta_i K(x^{(i)}, x) \\] <p>Fundamentally, all we need to know about the feature map \\( \\phi(\\cdot) \\) is encapsulated in the corresponding kernel function \\( K(\\cdot, \\cdot) \\).</p> <p>Note (Limitation of Kernel)</p> <p>Note that to make a prediction, we need to store all training dataset, which can be an obstacle for Kernel method.</p> <ul> <li> Property of Kernel</li> <li> How to define a valid Kernel</li> </ul>"},{"location":"dl/kernel/#support-vector-machine","title":"Support Vector Machine","text":"<ul> <li> Support Vector Machine</li> </ul>"},{"location":"dl/knn/","title":"k - Nearest Neighbor Classifier (kNN)","text":""},{"location":"dl/knn/#nearest-neighbor-classifier-nn","title":"Nearest Neighbor Classifier (NN)","text":"<p>Nearest Neighbor Classifier is very rarely used in practice, but it will allow us to get an idea about the basic approach to an image classification problem.</p> <p>Important Note (CIFAR-10 Dataset)</p> <p>One popular toy image classification dataset is the CIFAR-10 dataset. This dataset consists of 60,000 tiny images that are 32 pixels high and wide. Each image is labeled with one of 10 classes (for example \u201cairplane, automobile, bird, etc\u201d). These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images.</p> <p> CIFAR-10 Dataset </p> <p>We load CIFAR-10 Dataset using cs231n library:</p> <pre><code>import random\nimport numpy as np\nfrom cs231n.data_utils import load_CIFAR10\nimport matplotlib.pyplot as plt\n\n# This is a bit of magic to make matplotlib figures appear inline in the notebook\n# rather than in a new window.\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n# Some more magic so that the notebook will reload external python modules;\n# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2\n\n# Load the raw CIFAR-10 data.\ncifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n\n# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\ntry:\ndel X_train, y_train\ndel X_test, y_test\nprint('Clear previously loaded data.')\nexcept:\npass\n\nX_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n\n# As a sanity check, we print out the size of the training and test data.\nprint('Training data shape: ', X_train.shape)\nprint('Training labels shape: ', y_train.shape)\nprint('Test data shape: ', X_test.shape)\nprint('Test labels shape: ', y_test.shape)\n</code></pre> <p>Important Note (Visualizing CIFAR-10)</p> <pre><code># Visualize some examples from the dataset.\n# We show a few examples of training images from each class.\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nnum_classes = len(classes)\nsamples_per_class = 7\nfor y, cls in enumerate(classes):\n    idxs = np.flatnonzero(y_train == y)\n    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n    for i, idx in enumerate(idxs):\n        plt_idx = i * num_classes + y + 1\n        plt.subplot(samples_per_class, num_classes, plt_idx)\n        plt.imshow(X_train[idx].astype('uint8'))\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls)\nplt.show()\n</code></pre> <p>Before introducing algorithms to solve an image classification problem, we need to first define the difference between two images. One of the simplest possibilities is to compare the images pixel by pixel and add up all the differences. </p> <p>Definition (L1 Distance)</p> <p>Given two images and representing them as vectors \\(I_1, I_2\\), a reasonable choice for comparing them might be the L1 distance:</p> \\[d_1(I_1, I_2) = \\sum_p |I_1^p - I_2^p|\\] <p>Here is the procedure visualized:</p> <p> </p> <p>There are many other ways of computing distances between vectors. Another common choice could be to instead use the L2 distance, which has the geometric interpretation of computing the Euclidean distance between two vectors. </p> <p>Definition (L2 Distance)</p> <p>The L2 distance takes the form:</p> \\[d_2(I_1, I_2) = \\sqrt{\\sum_p (I_1^p - I_2^p)^2}\\] <p>Definition (Nearest Neighbor Classifier)</p> <p>Suppose now that we are given the CIFAR-10 training set of 50,000 images (5,000 images for every one of the labels), and we wish to label the remaining 10,000. The nearest neighbor classifier will take a test image, compare it to every single one of the training images, and predict the label of the closest training image. In the image \"CIFAR-10 Dataset\" you can see an example result of such a procedure for 10 example test images.</p> <p>Note (Performance of Nearest Neighbor Classifier)</p> <p>If you ran the Nearest Neighbor classifier on CIFAR-10 with L2 distance, you would obtain 35.4% accuracy (slightly lower than our L1 distance result - 38.6%).</p>"},{"location":"dl/knn/#k-nearest-neighbor-classifier-knn_1","title":"k - Nearest Neighbor Classifier (kNN)","text":"<p>You may have noticed that it is strange to only use the label of the nearest image when we wish to make a prediction. Indeed, it is almost always the case that one can do better by using what\u2019s called a k-Nearest Neighbor Classifier.</p> <p>Definition (k - Nearest Neighbor Classifier)</p> <p>instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image. In particular, when k = 1, we recover the Nearest Neighbor classifier. Intuitively, higher values of k have a smoothing effect that makes the classifier more resistant to outliers:</p> <p> k - Nearest Neighbor Classifier </p> <p>Important Note (Implementing kNN)</p> <p>In <code>train</code> method, we train the model, i.e. get the trained parameters. However, in kNN, we are just memorizing the training data:</p> <pre><code>def train(self, X, y):\n    \"\"\"\n    Inputs:\n    - X: A numpy array of shape (num_train, D) containing the training data\n      consisting of num_train samples each of dimension D.\n    - y: A numpy array of shape (N,) containing the training labels, where\n         y[i] is the label for X[i].\n    \"\"\"\n    self.X_train = X\n    self.y_train = y\n</code></pre> <p>Next is our <code>predict</code> method, which is to predict labels for test data. Note that as the first classifier to implement, we all use three different methods to calculate distances:</p> <pre><code>def predict(self, X, k=1, num_loops=0):\n    \"\"\"\n    Inputs:\n    - X: A numpy array of shape (num_test, D) containing test data consisting\n         of num_test samples each of dimension D.\n    - k: The number of nearest neighbors that vote for the predicted labels.\n    - num_loops: Determines which implementation to use to compute distances\n      between training points and testing points.\n\n    Returns:\n    - y: A numpy array of shape (num_test,) containing predicted labels for the\n      test data, where y[i] is the predicted label for the test point X[i].\n    \"\"\"\n    if num_loops == 0:\n        dists = self.compute_distances_no_loops(X)\n    elif num_loops == 1:\n        dists = self.compute_distances_one_loop(X)\n    elif num_loops == 2:\n        dists = self.compute_distances_two_loops(X)\n    else:\n        raise ValueError(\"Invalid value %d for num_loops\" % num_loops)\n\n    return self.predict_labels(dists, k=k)\n</code></pre> <p>We now use a naive way of computing <code>dists</code> with two loops (very slow):</p> <pre><code>def compute_distances_two_loops(self, X):\n    \"\"\"\n    Compute the distance between each test point in X and each training point\n    in self.X_train using a nested loop over both the training data and the\n    test data.\n\n    Inputs:\n    - X: A numpy array of shape (num_test, D) containing test data.\n\n    Returns:\n    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n    is the Euclidean distance between the ith test point and the jth training\n    point.\n    \"\"\"\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n    for i in range(num_test):\n        for j in range(num_train):                                                    \n            # Compute the l2 distance between the ith test point and the jth\n            # training point, and store the result in dists[i, j]. \n            dists[i, j] = np.sqrt(np.sum(np.square(X[i] - self.X_train[j])))\n\n    return dists\n</code></pre> <p>Now we try to use only one loop (not necessarily faster than two loops, however):</p> <pre><code>def compute_distances_one_loop(self, X):\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    for i in range(num_test):           \n        dists[i] = np.sqrt(np.sum(np.square(X[i] - self.X_train), axis = 1))\n\n    return dists\n</code></pre> <p>Finally, we use the fact that \\((x-y)^2 = x^2 + y^2 - 2xy\\) to implement a no-loop version (much faster than previous methods):</p> <pre><code>def compute_distances_no_loops(self, X):\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    dists = np.sqrt(\n        -2 * (X @ self.X_train.T) +\n        np.square(X).sum(axis=1, keepdims=True) +\n        np.square(self.X_train).sum(axis=1, keepdims=True).T\n    )\n\n    return dists\n</code></pre> <p>The time for each method is listed: \u00a0\u00a0\u00a0\u00a0 1. Two loop version took <code>38.411060</code> seconds \u00a0\u00a0\u00a0\u00a0 2. One loop version took <code>55.596785</code> seconds \u00a0\u00a0\u00a0\u00a0 3. No loop version took <code>0.620380</code> seconds </p> <p>After computing the distance, we try to predict the class of the image:</p> <pre><code>def predict_labels(self, dists, k=1):\n    \"\"\"\n    Given a matrix of distances between test points and training points,\n    predict a label for each test point.\n\n    Inputs:\n    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n    gives the distance betwen the ith test point and the jth training point.\n\n    Returns:\n    - y: A numpy array of shape (num_test,) containing predicted labels for the\n    test data, where y[i] is the predicted label for the test point X[i].\n    \"\"\"\n    num_test = dists.shape[0]\n    y_pred = np.zeros(num_test)\n    for i in range(num_test):\n        # A list of length k storing the labels of the k nearest neighbors to\n        # the ith test point.\n        closest_y = []\n\n        # Use the distance matrix to find the k nearest neighbors of the ith    #\n        # testing point, and use self.y_train to find the labels of these       #\n        # neighbors. Store these labels in closest_y.                           \n        closest_y = self.y_train[np.argsort(dists[i])[:k]]\n\n        # Now that you have found the labels of the k nearest neighbors, you    #\n        # need to find the most common label in the list closest_y of labels.   #\n        # Store this label in y_pred[i]. Break ties by choosing the smaller     #\n        # label.                                                                #\n        y_pred[i] = np.argmax(np.bincount(closest_y))\n\n    return y_pred\n</code></pre> Code (Complete kNN Implementation) <pre><code>from builtins import range\nfrom builtins import object\nimport numpy as np\nfrom past.builtins import xrange\n\nclass KNearestNeighbor(object):\n    \"\"\" a kNN classifier with L2 distance \"\"\"\n\ndef __init__(self):\n    pass\n\ndef train(self, X, y):\n    \"\"\"\n    Train the classifier. For k-nearest neighbors this is just\n    memorizing the training data.\n\n    Inputs:\n    - X: A numpy array of shape (num_train, D) containing the training data\n    consisting of num_train samples each of dimension D.\n    - y: A numpy array of shape (N,) containing the training labels, where\n        y[i] is the label for X[i].\n    \"\"\"\n    self.X_train = X\n    self.y_train = y\n\ndef predict(self, X, k=1, num_loops=0):\n    \"\"\"\n    Predict labels for test data using this classifier.\n\n    Inputs:\n    - X: A numpy array of shape (num_test, D) containing test data consisting\n        of num_test samples each of dimension D.\n    - k: The number of nearest neighbors that vote for the predicted labels.\n    - num_loops: Determines which implementation to use to compute distances\n    between training points and testing points.\n\n    Returns:\n    - y: A numpy array of shape (num_test,) containing predicted labels for the\n    test data, where y[i] is the predicted label for the test point X[i].\n    \"\"\"\n    if num_loops == 0:\n        dists = self.compute_distances_no_loops(X)\n    elif num_loops == 1:\n        dists = self.compute_distances_one_loop(X)\n    elif num_loops == 2:\n        dists = self.compute_distances_two_loops(X)\n    else:\n        raise ValueError(\"Invalid value %d for num_loops\" % num_loops)\n\n    return self.predict_labels(dists, k=k)\n\ndef compute_distances_two_loops(self, X):\n    \"\"\"\n    Compute the distance between each test point in X and each training point\n    in self.X_train using a nested loop over both the training data and the\n    test data.\n\n    Inputs:\n    - X: A numpy array of shape (num_test, D) containing test data.\n\n    Returns:\n    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n    is the Euclidean distance between the ith test point and the jth training\n    point.\n    \"\"\"\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n    for i in range(num_test):\n        for j in range(num_train):\n            # Compute the l2 distance between the ith test point and the jth    #\n            # training point, and store the result in dists[i, j]. You should   #\n            # not use a loop over dimension, nor use np.linalg.norm().          #\n\n            dists[i, j] = np.sqrt(np.sum(np.square(X[i] - self.X_train[j])))\n\n    return dists\n\ndef compute_distances_one_loop(self, X):\n    \"\"\"\n    Compute the distance between each test point in X and each training point\n    in self.X_train using a single loop over the test data.\n\n    Input / Output: Same as compute_distances_two_loops\n    \"\"\"\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n    for i in range(num_test):\n        # Compute the l2 distance between the ith test point and all training #\n        # points, and store the result in dists[i, :].                        #\n\n        dists[i] = np.sqrt(np.sum(np.square(X[i] - self.X_train), axis = 1))\n\n    return dists\n\ndef compute_distances_no_loops(self, X):\n    \"\"\"\n    Compute the distance between each test point in X and each training point\n    in self.X_train using no explicit loops.\n\n    Input / Output: Same as compute_distances_two_loops\n    \"\"\"\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    # Compute the l2 distance between all test points and all training      #\n    # points without using any explicit loops, and store the result in      #\n    # dists.                                                                #\n\n    dists = np.sqrt(\n    -2 * (X @ self.X_train.T) +\n    np.square(X).sum(axis=1, keepdims=True) +\n    np.square(self.X_train).sum(axis=1, keepdims=True).T\n    )\n\n    return dists\n\ndef predict_labels(self, dists, k=1):\n    \"\"\"\n    Given a matrix of distances between test points and training points,\n    predict a label for each test point.\n\n    Inputs:\n    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n    gives the distance betwen the ith test point and the jth training point.\n\n    Returns:\n    - y: A numpy array of shape (num_test,) containing predicted labels for the\n    test data, where y[i] is the predicted label for the test point X[i].\n    \"\"\"\n    num_test = dists.shape[0]\n    y_pred = np.zeros(num_test)\n    for i in range(num_test):\n        # A list of length k storing the labels of the k nearest neighbors to\n        # the ith test point.\n        closest_y = []\n\n        # Use the distance matrix to find the k nearest neighbors of the ith    #\n        # testing point, and use self.y_train to find the labels of these       #\n        # neighbors. Store these labels in closest_y.                           #\n        # Hint: Look up the function numpy.argsort.                             #\n\n        closest_y = self.y_train[np.argsort(dists[i])[:k]]\n        y_pred[i] = np.argmax(np.bincount(closest_y))\n\n    return y_pred\n</code></pre> <p>Definition (Validation Set)</p> <p>There is a correct way of tuning the hyperparameters and it does not touch the test set at all. The idea is to split our training set in two: a slightly smaller training set, and what we call a validation set. Using CIFAR-10 as an example, we could for example use 49,000 of the training images for training, and leave 1,000 aside for validation. This validation set is essentially used as a fake test set to tune the hyper-parameters. </p> <p>Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance.</p> <p>Definition (Cross-Validation)</p> <p>In cases where the size of your training data (and therefore also the validation data) might be small, people sometimes use a more sophisticated technique for hyperparameter tuning called cross-validation. Working with our previous example, the idea is that instead of arbitrarily picking the first 1000 datapoints to be the validation set and rest training set, you can get a better and less noisy estimate of how well a certain value of k works by iterating over different validation sets and averaging the performance across these. For example, in 5-fold cross-validation, we would split the training data into 5 equal folds, use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds.</p> <p>Important Note (Implementing Cross-Validation to Find The Best <code>k</code> Value)</p> <pre><code>num_folds = 5\nk_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n\nX_train_folds = []\ny_train_folds = []\n\nX_train_folds = np.array_split(X_train, num_folds)\ny_train_folds = np.array_split(y_train, num_folds)\n\n# A dictionary holding the accuracies for different values of k that we find\n# when running cross-validation. After running cross-validation,\n# k_to_accuracies[k] should be a list of length num_folds giving the different\n# accuracy values that we found when using that value of k.\nk_to_accuracies = {}\n\n# Perform k-fold cross validation to find the best value of k. For each        #\n# possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #\n# where in each case you use all but one of the folds as training data and the #\n# last fold as a validation set. Store the accuracies for all fold and all     #\n# values of k in the k_to_accuracies dictionary.                               #\nfor k in k_choices:\n    k_to_accuracies[k] = []\n    for i in range(num_folds):\n        X_train_temp = np.concatenate(np.compress(np.arange(num_folds) != i, X_train_folds, axis=0))\n        y_train_temp = np.concatenate(np.compress(np.arange(num_folds) != i, y_train_folds, axis=0))\n\n        classifier.train(X_train_temp, y_train_temp)\n\n        y_pred_temp = classifier.predict(X_train_folds[i], k=k)\n\n        num_correct = np.sum(y_pred_temp == y_train_folds[i])\n        accuracy = float(num_correct) / len(y_train_folds[i])\n        k_to_accuracies[k].append(accuracy)\n\n# Print out the computed accuracies\nfor k in sorted(k_to_accuracies):\n    for accuracy in k_to_accuracies[k]:\n        print('k = %d, accuracy = %f' % (k, accuracy))\n</code></pre> <p>Important Note (In Practice)</p> <p>In practice, people prefer to avoid cross-validation in favor of having a single validation split, since cross-validation can be computationally expensive. The splits people tend to use is between 50%-90% of the training data for training and rest for validation. However, this depends on multiple factors: For example if the number of hyperparameters is large you may prefer to use bigger validation splits. If the number of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use cross-validation. Typical number of folds you can see in practice would be 3-fold, 5-fold or 10-fold cross-validation.</p> <p> Common Data Splits </p> <p>Note (Final Accuracy of kNN)</p> <p>After using cross-validation, we found that the best <code>k</code> value is <code>10</code>, which leads to the final accuracy of kNN: 0.282.</p> <p>Important Note (Pros and Cons of NN)</p> <p>The nearest neighbor classifier takes no time to train, since all that is required is to store and possibly index the training data. However, we pay that computational cost at test time, since classifying a test example requires a comparison to every single training example. </p> <p>Important Note (Problems of NN)</p> <p>Images are high-dimensional objects (i.e. they often contain many pixels), and distances over high-dimensional spaces can be very counter-intuitive. The image below illustrates the point that the pixel-based L2 similarities we developed above are very different from perceptual similarities:</p> <p> </p> <p>Here is one more visualization to convince you that using pixel differences to compare images is inadequate. We can use a visualization technique called t-SNE to take the CIFAR-10 images and embed them in two dimensions so that their (local) pairwise distances are best preserved. In this visualization, images that are shown nearby are considered to be very near according to the L2 pixelwise distance we developed above:</p> <p> </p> <p>In particular, note that images that are nearby each other are much more a function of the general color distribution of the images, or the type of background rather than their semantic identity.</p>"},{"location":"dl/linearClassifier/","title":"Linear Classifiers","text":""},{"location":"dl/linearClassifier/#interpreting-linear-classifiers","title":"Interpreting Linear Classifiers","text":"<p>Important Note (Naive Interpretation of Linear Classifiers)</p> <p>A linear classifier computes the score of a class as a weighted sum of all of its pixel values across all 3 of its color channels. The function has the capacity to like or dislike (the sign of each weight) certain colors at certain positions in the image. For instance, the \u201cship\u201d class might be more likely if there is a lot of blue on the sides of an image. The \u201cship\u201d classifier would then have a lot of positive weights across its blue channel weights, and negative weights in the red/green channels.</p> <p> Mapping Images to Class Scores </p> <p>Important Note (Analogy of Images as High-Dimensional Points.)</p> <p>Since the images are stretched into high-dimensional column vectors, we can interpret each image as a single point in this space (e.g. each image in CIFAR-10 is a point in 3072-dimensional space of 32x32x3 pixels). </p> <p>Since we defined the score of each class as a weighted sum of all image pixels, each class score is a linear function over this space. We cannot visualize 3072-dimensional spaces, but if we imagine squashing all those dimensions into only two dimensions, then we can try to visualize what the classifier might be doing:</p> <p> Mapping Images to Class Scores </p> <p>Using the example of the car classifier (in red), the red line shows all points in the space that get a score of zero for the car class. The red arrow shows the direction of increase, so all points to the right of the red line have positive (and linearly increasing) scores, and all points to the left have a negative (and linearly decreasing) scores.</p> <p>Important Note (Interpretation of Linear Classifiers as Template Matching)</p> <p>Another interpretation for the weights \\(\\mathbf{W}\\) is that each row of \\(\\mathbf{W}\\) corresponds to a template for one of the classes. The score of each class for an image is then obtained by comparing each template with the image using dot product one by one to find the one that \u201cfits\u201d best. With this terminology, the linear classifier is doing template matching, where the templates are learned. </p> <p>Another way to think of it is that we are still effectively doing Nearest Neighbor, but instead of having thousands of training images we are only using a single image per class (although we will learn it, and it does not necessarily have to be one of the images in the training set), and we use the (negative) inner product as the distance instead of the L1 or L2 distance.</p> <p> Learned Weights at The End of Learning for CIFAR-10 </p> <p>Note (Observation in Templates)</p> <p>Note that the horse template seems to contain a two-headed horse, which is due to both left and right facing horses in the dataset. The linear classifier merges these two modes of horses in the data into a single template. Similarly, the car classifier seems to have merged several modes into a single template which has to identify cars from all sides, and of all colors. In particular, this template ended up being red, which hints that there are more red cars in the CIFAR-10 dataset than of any other color. The linear classifier is too weak to properly account for different-colored cars, but neural networks will allow us to perform this task.</p> <p>Strategy (Bias Trick)</p> <p>Recall that we defined the score function as:</p> \\[ f(x_i, W, b) = W x_i + b \\] <p>A commonly used trick is to combine the two sets of parameters into a single matrix that holds both of them by extending the vector \\(x_i\\) with one additional dimension that always holds the constant 1 - a default bias dimension. With the extra dimension, the new score function will simplify to a single matrix multiply:</p> \\[ f(x_i, W) = W x_i \\] <p>With our CIFAR-10 example, \\(x_i\\) is now [3073 x 1] instead of [3072 x 1] - (with the extra dimension holding the constant 1), and \\(W\\) is now [10 x 3073] instead of [10 x 3072]. The extra column that \\(W\\) now corresponds to the bias \\(b\\). An illustration might help clarify:</p> <p> Illustration of The Bias Trick </p> <p>Strategy (Normalization of Input Features)</p> <p>In Machine Learning, it is a very common practice to always perform normalization of input features (in the case of images, every pixel is thought of as a feature). In particular, it is important to center your data by subtracting the mean from every feature. In the case of images, this corresponds to computing a mean image across the training images and subtracting it from every image to get images where the pixels range from approximately [-127 \u2026 127]. Further common preprocessing is to scale each input feature so that its values range from [-1, 1]. Of these, zero mean centering is arguably more important but we will talk about it later.</p>"},{"location":"dl/linearClassifier/#multiclass-support-vector-machine-svm-loss","title":"Multiclass Support Vector Machine (SVM) Loss","text":"<p>The SVM loss is set up so that the SVM \u201cwants\u201d the correct class for each image to have a score higher than the incorrect classes by some fixed margin \\(\\Delta\\). </p> <p>Definition (SVM Loss)</p> <p>Recall that for the i-th example we are given the pixels of image \\(x_i\\) and the label \\(y_i\\) that specifies the index of the correct class. The score function takes the pixels and computes the vector \\(f(x_i, W)\\) of class scores, which we will abbreviate to \\(\\mathbf{s}\\) (short for scores). For example, the score for the j-th class is the j-th element: \\(s_j = f(x_i, W)_j\\). The Multiclass SVM loss for the i-th example is then formalized as follows:</p> \\[ L_i = \\sum_{j \\ne y_i} \\max(0, s_j - s_{y_i} + \\Delta) \\] <p> The Multiclass Support Vector Machine </p> <p>Note that in this particular module we are working with linear score functions \\(( f(x_i; W) = W x_i )\\), so we can also rewrite the loss function in this equivalent form:</p> \\[ L_i = \\sum_{j \\ne y_i} \\max(0, \\mathbf{w}_j^T x_i - \\mathbf{w}_{y_i}^T x_i + \\Delta) \\] <p>where \\(\\mathbf{w}_j\\) is the j-th row of \\(W\\) reshaped as a column. However, this will not necessarily be the case once we start to consider more complex forms of the score function \\(f\\).</p> Example (Example of SVM Loss) <p>Suppose that we have three classes that receive the scores \\(\\mathbf{s} = [13, -7, 11]\\), and that the first class is the true class (i.e. \\(y_i = 0\\)). Also assume that \\(\\Delta\\) (a hyperparameter we will go into more detail about soon) is 10. The expression above sums over all incorrect classes \\((j \\ne y_i)\\), so we get two terms:</p> \\[ L_i = \\max(0, -7 - 13 + 10) + \\max(0, 11 - 13 + 10) \\] <p>You can see that the first term gives zero since \\([-7 - 13 + 10]\\) gives a negative number, which is then thresholded to zero with the \\(\\max(0, -)\\) function. We get zero loss for this pair because the correct class score (13) was greater than the incorrect class score (-7) by at least the margin 10. In fact the difference was 20, which is much greater than 10 but the SVM only cares that the difference is at least 10; Any additional difference above the margin is clamped at zero with the max operation. The second term computes \\([11 - 13 + 10]\\) which gives 8. That is, even though the correct class had a higher score than the incorrect class \\((13 &gt; 11)\\), it was not greater by the desired margin of 10. The difference was only 2, which is why the loss comes out to 8 (i.e. how much higher the difference would have to be to meet the margin). In summary, the SVM loss function wants the score of the correct class \\(y_i\\) to be larger than the incorrect class scores by at least \\(\\Delta\\) (delta). If this is not the case, we will accumulate loss.</p> <p>Definition (Hinge Loss)</p> <p>When the threshold is at zero, i.e. \\(\\max(0, -)\\), the function is often called the Hinge Loss. You\u2019ll sometimes hear about people instead using the squared hinge loss SVM (or L2-SVM), which uses the form \\(\\max(0, -)^2\\) that penalizes violated margins more strongly (quadratically instead of linearly). The unsquared version is more standard, but in some datasets the squared hinge loss can work better. This can be determined during cross-validation.</p>"},{"location":"dl/linearClassifier/#regularization","title":"Regularization","text":"<p>There is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters \\(W\\) that correctly classify every example (i.e. all scores are so that all the margins are met, and \\(L_i = 0\\) for all \\(i\\)). The issue is that this set of \\(W\\) is not necessarily unique: there might be many similar \\(W\\) that correctly classify the examples. For example, \\(2W\\). </p> <p>In other words, we wish to encode some preference for a certain set of weights \\(W\\) over others to remove this ambiguity. We can do so by extending the loss function with a regularization penalty \\(R(W)\\). </p> <p>The most common regularization penalty is the squared L2 norm. </p> <p>Definition (L2 Regularization)</p> <p>L2 Regularization discourages large weights through an elementwise quadratic penalty over all parameters:</p> \\[ R(W) = \\sum_k \\sum_l W_{k,l}^2 \\] <p>Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the data loss (which is the average loss \\(L_i\\) over all examples) and the regularization loss. That is, the full Multiclass SVM loss becomes:</p> \\[ L = \\frac{1}{N} \\sum_i L_i + \\lambda R(W) \\] <p>Or expanding this out in its full form:</p> \\[ L = \\frac{1}{N} \\sum_i \\sum_{j \\ne y_i} \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) + \\lambda \\sum_k \\sum_l W_{k,l}^2 \\] <p>Where \\(N\\) is the number of training examples. As you can see, we append the regularization penalty to the loss objective, weighted by a hyperparameter \\(\\lambda\\). There is no simple way of setting this hyperparameter and it is usually determined by cross-validation.</p> <p>In addition to the motivation we provided above there are many desirable properties to include the regularization penalty, many of which we will come back to in later sections. For example, it turns out that including the L2 penalty leads to the appealing max margin property in SVMs.</p> <p>Important Note (Why Regularization?)</p> <p>The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself.</p> Example (Why Not Regularization?) <p>Suppose that we have some input vector \\(\\mathbf{x} = [1, 1, 1, 1]\\) and two weight vectors \\(\\mathbf{w}_1 = [1, 0, 0, 0]\\), \\(\\mathbf{w}_2 = [0.25, 0.25, 0.25, 0.25]\\). Then \\(\\mathbf{w}_1^T \\mathbf{x} = \\mathbf{w}_2^T \\mathbf{x} = 1\\) so both weight vectors lead to the same dot product, but the L2 penalty of \\(\\mathbf{w}_1\\) is 1.0 while the L2 penalty of \\(\\mathbf{w}_2\\) is only 0.5. Therefore, according to the L2 penalty the weight vector \\(\\mathbf{w}_2\\) would be preferred since it achieves a lower regularization loss. Intuitively, this is because the weights in \\(\\mathbf{w}_2\\) are smaller and more diffuse. Since the L2 penalty prefers smaller and more diffuse weight vectors, the final classifier is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions very strongly.</p> <p>wait, if we prefer more difused weights, doesn't the model overfit more since we include more features and count for subtle difference?</p> <p>Note (Biases v.s. Penalty)</p> <p>Note that biases do not have the same effect as penalty, since, unlike the weights, they do not control the strength of influence of an input dimension. Therefore, it is common to only regularize the weights \\(W\\) but not the biases \\(b\\). However, in practice this often turns out to have a negligible effect. </p> <p>Note (0 Loss Impossible with Regularization)</p> <p>Note that due to the regularization penalty we can never achieve loss of exactly 0.0 on all examples, because this would only be possible in the pathological setting of \\(W = 0\\).</p>"},{"location":"dl/lnn/","title":"Linear Regression","text":""},{"location":"dl/lnn/#loss-function","title":"Loss Function","text":"<p>Important Note (Probabilistic Interpretation of Squared Loss)</p> <p>Why squared loss is a reasonable choice? Assume our target \\(y^{(i)}\\), feature vector \\(x^{(i)}\\), weights vector \\(\\theta\\), and bias \\(\\epsilon^{(i)}\\) are related via the equation:</p> \\[\\begin{align*} y^{(i)} = \\theta^{T}x^{(i)} + \\epsilon^{(i)} \\end{align*}\\] <p>Let us further assume that \\(\\epsilon^{(i)}\\) are distributed IID (independently and identically distributed) according to a Gaussian distribution with mean zero and variance \\(\\sigma^2\\), i.e. \u00a0 \\(\\epsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^2)\\). Thus, we have:</p> \\[\\begin{align*} p(\\epsilon^{(i)}) &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\frac{(\\epsilon^{(i)} - \\mu)^2}{2\\sigma^2} \\right) \\\\                   &amp;= \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( -\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2} \\right) \\\\ \\end{align*}\\] <p>Since \\(y^{(i)} = \\theta^{T}x^{(i)} + \\epsilon^{(i)}\\), this leads to </p> \\[\\begin{align*} p(y^{(i)} \\mid x^{(i)}; \\theta) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( -\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2} \\right) \\end{align*}\\] <p>Considering all samples \\(\\mathbf{X}\\), we use the likelyhood function \\(L(\\theta) = L(\\theta; X, \\vec{y}) = p(\\vec{y} \\mid X; \\theta)\\) to denote the probability of targets vector \\(\\vec{y}\\). Since \\(\\epsilon^{(i)}\\) are distributed independently, we have:</p> \\[\\begin{align*} L(\\theta) &amp;= \\prod_{i=1}^n p(y^{(i)} \\mid x^{(i)}; \\theta) \\\\           &amp;= \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( -\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2 \\sigma^2} \\right) \\end{align*}\\] <p>Now, we naturally want to increase the probability of \\(L(\\theta)\\), which is to choose a proper \\(\\theta\\) to maximize likelyhood. Instead of maximizing \\(L(\u03b8)\\), we can also maximize any strictly increasing function of \\(L(\u03b8)\\). In particular, the derivations will be a bit simpler if we instead maximize the log likelihood \\(\\ell(\\theta)\\):</p> \\[\\begin{align*} \\ell(\\theta) &amp;= \\log L(\\theta) \\\\              &amp;= \\log \\prod_{i=1}^n \\left( \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( -\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2} \\right) \\right) \\\\              &amp;= \\sum_{i=1}^n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp \\left( -\\frac{(y^{(i)} - \\theta^T x^{(i)})^2}{2\\sigma^2} \\right) \\right) \\\\              &amp;= n \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma}} \\right) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y^{(i)} - \\theta^T x^{(i)})^2 \\end{align*}\\] <p>Hence, maximizing \\(\\ell(\\theta)\\) is equivalent to minimizing </p> \\[\\begin{align*} \\frac{1}{2} \\sum_{i=1}^n (y^{(i)} - \\theta^T x^{(i)})^2 \\end{align*}\\]"},{"location":"dl/lnn/#maximum-likelihood-estimation-mle","title":"Maximum Likelihood Estimation (MLE)","text":"<p>Definition (Maximum Likelihood Estimation)</p> <p>Maximum Likelihood Estimation (MLE) is the process of estimating the parameters of a distribution that maximize the likelihood of the observed data belonging to that distribution. <sup>1</sup></p>"},{"location":"dl/lnn/#generalization","title":"Generalization","text":"<p>Definition (Underfitting, Overfitting and Regularization)</p> <p>Underfitting is the phenomenon that the model is unable to reduce the training error and to capture the pattern that we are trying to model. Overfitting is the phenomenon of fitting closer to the training data than to the underlying distribution, and techniques for combatting overfitting are often called regularization methods.</p> <p>Definition (Training Error, Generalization Error, and Validation Error)</p> <p>The training error \\(R_{\\text{emp}}\\) is a statistic calculated on the training dataset, and the generalization error \\(R\\) is an expectation taken with respect to the underlying distribution. Generalization error is what you would see if applying the model to an infinite stream of additional data examples drawn from the same underlying data distribution. Formally the training error is expressed as a sum:</p> \\[ R_{\\text{emp}}[\\mathbf{X}, \\mathbf{y}, f] = \\frac{1}{n} \\sum_{i=1}^{n} l(\\mathbf{x}^{(i)}, y^{(i)}, f(\\mathbf{x}^{(i)})), \\] <p>while the generalization error is expressed as an integral:</p> \\[ R[p, f] = \\mathbb{E}_{(\\mathbf{x}, y) \\sim P} [l(\\mathbf{x}, y, f(\\mathbf{x}))] = \\iint l(\\mathbf{x}, y, f(\\mathbf{x})) p(\\mathbf{x}, y) \\, d\\mathbf{x} \\, dy. \\] <p>We can never calculate the generalization error \\(R\\) exactly. In practice, we must estimate the generalization error by applying our model to an independent test set constituted of a random selection of examples \\(\\mathbf{X}'\\) and labels \\(\\mathbf{y}'\\) that were withheld from our training set. This consists of applying the same formula that was used for calculating the empirical training error but to a test set \\(\\mathbf{X}', \\mathbf{y}'\\).</p> <p>Error on the holdout data, i.e., validation set, is called the validation error.</p> <p>Methodology (Underfitting or Overfitting?)</p> <p>When our training error and validation error are both substantial but there is a little gap between them, then it may be an underfitting.  If the training error is significantly lower than the validation error, then it may be an overfitting.</p> <p>Strategy (Cross-Validation)</p> <p>\\(K\\)-fold cross-validation: Splitting the original dataset into non-overlapping subsets. Then model training and validation are executed \\(K\\) times, each time training on subsets and validating on a different subset (the one not used for training in that round). Finally, the training and validation errors are estimated by averaging over the results from the experiments.</p> <p>Note (Fixing Overfitting)</p> <p>A very blunt way to fix overfitting is to reduce the number of parameters (weights). The intuition is: a higher degree polynomial can depict a more complicated model than a lower degree polynomial. So if #weights is large, then the polynomial may fit in with the training dataset too well.</p>"},{"location":"dl/lnn/#weight-decay","title":"Weight Decay","text":"<p>Definition (Weight Decay)</p> <p>Rather than directly manipulating the number of parameters, weight decay, operates by restricting the values that the parameters can take. More commonly called \\(\\ell_2\\) regularization outside of deep learning circles when optimized by minibatch stochastic gradient descent, weight decay might be the most widely used technique for regularizing parametric machine learning models.</p> <ul> <li> Weight Decay Interpretation</li> <li> Lagrange Multiplier Interpretation</li> <li> Bayes Interpretation</li> </ul>"},{"location":"dl/lnn/#analytical-solution-of-linear-regression","title":"Analytical Solution of Linear Regression","text":"<p>Definition (Projection Matrix)</p> <p>The projection matrix of vector \\(\\mathbf{v}\\) is the outer product of \\(\\mathbf{v}\\) and itself \u00a0 \"divides by\" their inner product:</p> \\[ \\frac{\\mathbf{v}\\mathbf{v}^T}{\\mathbf{v}^T\\mathbf{v}} \\] <p>The projection matrix of matrix \\(\\mathbf{X}\\) is also the outer product of \\(\\mathbf{X}\\) and itself \u00a0 \"divides by\" their inner product. Here we multiply the inverse of \\(\\mathbf{X}^T\\mathbf{X}\\):</p> \\[ \\mathbf{X}{(\\mathbf{X}^T\\mathbf{X})}^{-1}\\mathbf{X}^T  \\] <p>Note</p> <p>We put \\({(\\mathbf{X}^T\\mathbf{X})}^{-1}\\) in the middle because only \\(\\mathbf{X}{(\\mathbf{X}^T\\mathbf{X})}^{-1}\\mathbf{X}^T\\): \\((m\\times n)\\times((n\\times m)\\times(m\\times n))\\times(n\\times m)\\) makes sense when we do calculation.</p> <p>Theorem (Computing Projection)</p> <p>The projection of vector \\(\\mathbf{b}\\) on the vector \\(\\mathbf{v}\\) is</p> \\[ \\text{projection matrix}(v) \\cdot \\mathbf{b} \\] Proof <p>Note that \\(\\mathbf{v}^T\\mathbf{v}=\\|\\mathbf{v}\\|\\). Thus we have:</p> \\[\\begin{align} \\text{projection matrix}(v) \\cdot \\mathbf{b} &amp;= \\frac{\\mathbf{v}\\mathbf{v}^T}{\\mathbf{v}^T\\mathbf{v}}\\cdot\\mathbf{b} \\\\ &amp;= \\frac{\\mathbf{v}}{\\mathbf{\\|\\mathbf{v}\\|}}\\cdot\\frac{\\mathbf{v}^T}{\\mathbf{\\|\\mathbf{v}\\|}}\\cdot\\mathbf{b} \\\\ &amp;= \\tilde{v}\\cdot(\\tilde{v})^T\\cdot\\mathbf{b} \\\\ &amp;= \\tilde{v}\\cdot((\\tilde{v})^T\\mathbf{b}) \\end{align}\\] <p>Note that \\((\\tilde{v})^T\\mathbf{b}\\) is the length of the projection of \\(\\mathbf{b}\\) on \\(\\mathbf{v}\\). Thus, \\(\\tilde{v}\\cdot((\\tilde{v})^T\\mathbf{b})\\) is exactly the projection of \\(\\mathbf{b}\\) on \\(\\mathbf{v}\\).</p> <ol> <li> <p>Understanding Maximum Likelihood Estimation by Aniruddha Karajg\u00a0\u21a9</p> </li> </ol>"},{"location":"dl/logistic/","title":"Logistic Regression","text":"<p>Recall that we have discussed the sigmoid function, which is also known as the logistic function. We will discuss how to build up a regression based on that for a classification problem.</p>"},{"location":"dl/logistic/#logistic-regression_1","title":"Logistic Regression","text":"<p>Model (Logistic Regression)</p> <p>Consider a classification problem where \\(y \\in \\{0,1\\}\\). Define </p> \\[\\begin{align*} g(z) = \\frac{1}{1+e^{-z}} \\end{align*}\\] <p>We change our hypotheses \\(h_{\\theta}(x)\\) to </p> \\[\\begin{align*} h_{\\theta}(x) = g(\\theta^Tx) = \\frac{1}{1 + e^{-\\theta^Tx}} \\end{align*}\\] <p>We pretend \\(h_{\\theta}(x)\\) to be the probability of \\(y = 1\\).  Recall that </p> \\[\\begin{align*} g'(z) &amp;= \\frac{d}{dz}\\frac{1}{1+e^{-z}} \\\\       &amp;= \\frac{1}{(1+e^{-z})^2}(e^{-z}) \\\\       &amp;= \\frac{1}{1+e^{-z}}\\cdot (1-\\frac{1}{1+e^{-z}}) \\\\       &amp;= g(z)(1-g(z)) \\end{align*}\\] <p>Similar to MLE in linear regression, let us assume </p> \\[\\begin{align*} P(y=1\\mid x;\\theta) &amp;= h_{\\theta}(x) \\\\ P(y=0\\mid x;\\theta) &amp;= 1 - h_{\\theta}(x) \\end{align*}\\] <p>which is equivalent to </p> \\[\\begin{align*} p(y \\mid x;\\theta) = (h_{\\theta}(x))^y(1-h_{\\theta}(x))^{1-y} \\end{align*}\\] <p>By IID assumption, the likelihood of the parameters is:</p> \\[\\begin{align*} L(\\theta) &amp;= p(\\hat{y}\\mid X;\\theta) \\\\           &amp;= \\prod_{i=1}^{n} p(y^{(i)} | x^{(i)}; \\theta) \\\\           &amp;= \\prod_{i=1}^{n} (h_{\\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\\theta}(x^{(i)}))^{1 - y^{(i)}} \\\\ \\end{align*}\\] <p>Transform this into log likelihood:</p> \\[\\begin{align*} \\ell(\\theta) &amp;= \\log L(\\theta) \\\\              &amp;= \\log \\left( \\prod_{i=1}^{n} (h_{\\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\\theta}(x^{(i)}))^{1 - y^{(i)}} \\right) \\\\              &amp;= \\sum_{i=1}^{n} \\log \\left( (h_{\\theta}(x^{(i)}))^{y^{(i)}} (1 - h_{\\theta}(x^{(i)}))^{1 - y^{(i)}} \\right) \\\\              &amp;= \\sum_{i=1}^{n} \\left( y^{(i)} \\log h_{\\theta}(x^{(i)}) + (1 - y^{(i)}) \\log (1 - h_{\\theta}(x^{(i)})) \\right) \\\\              &amp;= \\sum_{i=1}^{n} \\left( y^{(i)} \\log h_{\\theta}(x^{(i)}) + (1 - y^{(i)}) \\log (1 - h_{\\theta}(x^{(i)})) \\right) \\end{align*}\\] <p>To maximize the likelihood, we use gradient ascent. The equation to update \\(\\theta\\) is:</p> \\[\\begin{align*} \\theta := \\theta + \\alpha \\nabla_{\\theta} \\ell(\\theta) \\end{align*}\\] <p>Now, let us calculate the partial derivative of \\(\\ell(\\theta)\\) with repect to \\(\\theta_j\\). Remember that \\(g'(z) = g(z)(1-g(z))\\): </p> \\[\\begin{align*} \\frac{\\partial}{\\partial \\theta_j} \\ell(\\theta)  &amp;= \\left( \\frac{y}{g(\\theta^T x)} - \\frac{(1 - y)}{1 - g(\\theta^T x)} \\right) \\frac{\\partial}{\\partial \\theta_j} g(\\theta^T x) \\\\ &amp;= \\left( \\frac{y}{g(\\theta^T x)} - \\frac{(1 - y)}{1 - g(\\theta^T x)} \\right) g(\\theta^T x) (1 - g(\\theta^T x)) \\frac{\\partial}{\\partial \\theta_j} \\theta^T x \\\\ &amp;= \\left( \\frac{y}{g(\\theta^T x)} - \\frac{(1 - y)}{1 - g(\\theta^T x)} \\right) g(\\theta^T x) (1 - g(\\theta^T x)) x_j \\\\ &amp;= \\left( y (1 - g(\\theta^T x)) - (1 - y) g(\\theta^T x) \\right) x_j \\\\ &amp;= (y - h_{\\theta}(x)) x_j \\end{align*}\\] <p>Using the above partial derivative, we update \\(\\theta_j\\) using the stochastic gradient ascent rule:</p> \\[ \\theta_j := \\theta_j + \\alpha \\left( y^{(i)} - h_{\\theta}(x^{(i)}) \\right) x_j^{(i)} \\] <p>Important Note (Comparing Logistic and Linear Regression)</p> Method Distribution Problem Type \\(y\\) value Linear \\(y \\mid x;\\theta \\sim \\mathcal{N}(\\theta^{T}x, \\sigma^{2})\\) Regression \\(y\\in \\mathbb{R}\\) Logistic \\(y \\mid x;\\theta \\sim \\text{Bernoulli}\\left(\\frac{1}{1 + e^{-\\theta^{T}x}}\\right)\\) Classification \\(y\\in \\{0,1\\}\\)"},{"location":"dl/logistic/#newtons-method","title":"Newton's Method","text":"<p>Other than gradient descent (GD), Newton's Method can also seek for the proper \\(\\theta\\). Newton's method typically converges faster than GD. However, it can be more expensive than one iteration of gradient descent, since it requires finding and inverting a d-by-d Hessian. But so long as d is not too large, it is usually much faster overall. When Newton\u2019s method is applied to maximize the logistic regression log likelihood function \u2113(\u03b8), the resulting method is also called Fisher Scoring.</p> <p>Definition (Hessian)</p> <p>\\(H\\) is a d-by-d matrix (actually, (d+1)-by-(d+1), assuming that we include the intercept term) called the Hessian, whose entries are given by</p> \\[ H_{ij} = \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_i \\partial \\theta_j}. \\] <p>Definition (Newton's Method)</p> <p>Suppose we have some function \\( f : \\mathbb{R} \\mapsto \\mathbb{R} \\), and we wish to find a value of \\(\\theta\\) so that \\( f(\\theta) = 0 \\). Here, \\(\\theta \\in \\mathbb{R}\\) is a real number. Newton's method performs the following update:</p> \\[ \\theta := \\theta - \\frac{f(\\theta)}{f'(\\theta)}. \\] <p>This method has a natural interpretation in which we can think of it as approximating the function \\( f \\) via a linear function that is tangent to \\( f \\) at the current guess \\(\\theta\\), solving for where that linear function equals to zero, and letting the next guess for \\(\\theta\\) be where that linear function is zero.</p> <p> Newton's Method </p> <p>Definition (Newton-Raphson Method)</p> <p>Newton's method gives a way of getting to \\( f(\\theta) = 0 \\). What if we want to use it to maximize some function \\(\\ell\\)? The maxima of \\(\\ell\\) correspond to points where its first derivative \\(\\ell'(\\theta)\\) is zero. So, by letting \\( f(\\theta) = \\ell'(\\theta) \\), we can use the same algorithm to maximize \\(\\ell\\), and we obtain the update rule:</p> \\[ \\theta := \\theta - \\frac{\\ell'(\\theta)}{\\ell''(\\theta)}. \\] <p>Lastly, in our logistic regression setting, \\(\\theta\\) is vector-valued, so we need to generalize Newton's method to this setting. The generalization of Newton's method to this multidimensional setting (also called the Newton-Raphson Method) is given by</p> \\[ \\theta := \\theta - H^{-1} \\nabla_{\\theta} \\ell(\\theta). \\] <p>Note (Limitation of Newton's Method)</p> <p>Newton's Method always bring us to the nearest stationary point instead of the global extreme, whereas GD does not have this limitation.</p>"},{"location":"dl/mdp/","title":"Markov Decision Processes (MDP)","text":""},{"location":"dl/mdp/#markov-decision-processes","title":"Markov Decision Processes","text":"<p>Definition (Markov Decision Processes)</p> <p>A Markov decision process is a tuple \\((S, A, \\{P_{sa}\\}, \\gamma, R)\\), where: </p> <ul> <li> <p>\\(S\\) is a set of states. (e.g. position on a chess board)</p> </li> <li> <p>\\(A\\) is a set of actions. (e.g. next possible move)</p> </li> <li> <p>\\(P_{sa}\\) are the state transition probabilities. For each state \\(s \\in S\\) and action \\(a \\in A\\), \\(P_{sa}\\) is a distribution over the state space. Briefly, \\(P_{sa}\\) gives the distribution over what states we will transition to if we take action \\(a\\) in state \\(s\\).</p> </li> <li> <p>\\(\\gamma \\in [0, 1]\\) is called the discount factor.</p> </li> <li> <p>\\(R : S \\times A \\rightarrow \\mathbb{R}\\) is the reward function. (Also written as a function of a state \\(S\\) only, i.e. \\(R : S \\rightarrow \\mathbb{R}\\)).</p> </li> </ul> <p>The logistic: </p> <p>We start in some state \\(s_0\\), and choose some action \\(a_0 \\in A\\). The state of the MDP randomly transitions to some successor state \\(s_1\\), drawn according to \\(s_1 \\sim P_{s_0 a_0}\\). Then, we choose another action \\(a_1\\). As a result of this action, the state transitions again, now to some \\(s_2 \\sim P_{s_1 a_1}\\). We then pick \\(a_2\\), and so on.... Pictorially, we can represent this process as follows:</p> \\[ s_0 \\xrightarrow{a_0} s_1 \\xrightarrow{a_1} s_2 \\xrightarrow{a_2} s_3 \\xrightarrow{a_3} \\cdots \\] <p>Upon visiting the sequence of states \\(s_0, s_1, \\ldots\\) with actions \\(a_0, a_1, \\ldots\\), our total payoff is given by</p> \\[ R(s_0, a_0) + \\gamma R(s_1, a_1) + \\gamma^2 R(s_2, a_2) + \\cdots. \\] <p>Or, when we are writing rewards as a function of the states only, this becomes</p> \\[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\cdots. \\] <p>For most of our development, we will use the simpler state-rewards \\(R(s)\\), though the generalization to state-action rewards \\(R(s, a)\\) offers no special difficulties. </p> <p>Our goal in reinforcement learning is to choose actions over time so as to maximize the expected value of the total payoff:</p> \\[ E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\cdots \\right] \\] <p>Note that the reward at timestep \\(t\\) is discounted by a factor of \\(\\gamma^t\\). Thus, to make this expectation large, we would like to accrue positive rewards as soon as possible (and postpone negative rewards as long as possible).</p> <p>Definition (Policy)</p> <p>A policy is any function \\(\\pi : S \\rightarrow A\\) mapping from the states to the actions. We say that we are executing some policy \\(\\pi\\) if, whenever we are in state \\(s\\), we take action \\(a = \\pi(s)\\). We also define the value function for a policy \\(\\pi\\) according to</p> \\[ V^\\pi(s) = E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\cdots \\mid s_0 = s, \\pi \\right] \\] <p>\\(V^\\pi(s)\\) is simply the expected sum of discounted rewards upon starting in state \\(s\\), and taking actions according to \\(\\pi\\). Given a fixed policy \\(\\pi\\), its value function \\(V^\\pi\\) satisfies the Bellman equations:</p> \\[ \\begin{align*}     V^\\pi(s) &amp;= E \\left[ R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\cdots \\mid s_0 = s, \\pi \\right] \\\\             &amp;= R(s) + \\gamma E \\left[ R(s_1) + \\gamma R(s_2) + \\gamma^2 R(s_3) + \\cdots \\right] \\tag{s_0 = s} \\\\             &amp;= R(s) + \\gamma E \\left[ V^\\pi(s_1) \\right] \\\\             &amp;= R(s) + \\gamma \\sum_{s' \\in S} P_{s \\pi(s)}(s') V^\\pi(s') \\end{align*} \\] <p>This says that the expected sum of discounted rewards \\(V^\\pi(s)\\) for starting in \\(s\\) consists of two terms: First, the immediate reward \\(R(s)\\) that we get right away simply for starting in state \\(s\\), and second, the expected sum of future discounted rewards. Examining the second term in more detail, we see that the summation term above can be rewritten \\(E_{s' \\sim P_{s \\pi(s)}} [V^\\pi(s')]\\). This is the expected sum of discounted rewards for starting in state \\(s'\\), where \\(s'\\) is distributed according \\(P_{s \\pi(s)}\\), which is the distribution over where we will end up after taking the first action \\(\\pi(s)\\) in the MDP from state \\(s\\). Thus, the second term above gives the expected sum of discounted rewards obtained after the first step in the MDP.  </p> <p>Bellman\u2019s equations can be used to efficiently solve for \\(V^\\pi\\). Specifically, in a finite-state MDP (\\(|S| &lt; \\infty\\)), we can write down one such equation for \\(V^\\pi(s)\\) for every state \\(s\\). This gives us a set of \\(|S|\\) linear equations in \\(|S|\\) variables (the unknown \\(V^\\pi(s)\\)\u2019s, one for each state), which can be efficiently solved for the \\(V^\\pi(s)\\)\u2019s.</p> <p>Important Note (Computing \\(V^{\\pi}\\))</p> <p>Observe that</p> \\[ \\begin{aligned}     V^\\pi(s_1) &amp;= R(s_1) + \\gamma \\sum_{s' \\in S} P_{s_1 \\pi(s_1)}(s') V^\\pi(s') \\\\     V^\\pi(s_2) &amp;= R(s_2) + \\gamma \\sum_{s' \\in S} P_{s_2 \\pi(s_2)}(s') V^\\pi(s') \\\\     &amp;\\ \\vdots \\\\ \\\\     V^\\pi(s_{|S|}) &amp;= \\cdots \\end{aligned} \\] <p>Thus, we define </p> \\[ P^\\pi =  \\begin{bmatrix}     P_{s_1, \\pi(s_1)} &amp; \\cdots &amp; P_{s_1, \\pi(s_{|S|})} \\\\     \\vdots &amp; \\ddots &amp; \\vdots \\\\     P_{s_{|S|}, \\pi(s_1)} &amp; \\cdots &amp; P_{s_{|S|}, \\pi(s_{|S|})} \\end{bmatrix} \\] <p>where \\(P^\\pi\\) is a \\(|S| \\times |S|\\) matrix, with rows and columns indexed by states \\(s_i\\). We then get the abbreviated form of computing \\(V^{\\pi}\\):</p> \\[ \\begin{aligned}     \\mathbf{V}^\\pi &amp;= \\underbrace{\\mathbf{R}}_{|S| \\times 1} + \\gamma \\underbrace{\\mathbf{P}^\\pi}_{|S| \\times |S|} \\underbrace{\\mathbf{V}^\\pi}_{|S| \\times 1} \\\\     \\mathbf{V}^\\pi &amp;= (\\mathbf{I} - \\gamma \\mathbf{P}^\\pi)^{-1} \\mathbf{R} \\end{aligned} \\] <p>Definition (Optimal Value Function)</p> <p>We define the optimal value function according to</p> \\[ V^*(s) = \\max_\\pi V^\\pi(s) \\] <p>In other words, this is the best possible expected sum of discounted rewards that can be attained using any policy. There is also a version of Bellman\u2019s equations for the optimal value function:</p> \\[ V^*(s) = R(s) + \\max_{a \\in A} \\gamma \\sum_{s' \\in S} P_{sa}(s') V^*(s'). \\tag{2} \\] <p>The first term above is the immediate reward as before. The second term is the maximum over all actions \\(a\\) of the expected future sum of discounted rewards we\u2019ll get upon after action \\(a\\).</p> <p>We also define a policy \\(\\pi^* : S \\rightarrow A\\) as follows:</p> \\[ \\pi^*(s) = \\arg \\max_{a \\in A} \\sum_{s' \\in S} P_{sa}(s') V^*(s'). \\tag{3} \\] <p>Note that \\(\\pi^*(s)\\) gives the action \\(a\\) that attains the maximum in the \u201cmax\u201d in Equation (2).</p> <p>It is a fact that for every state \\(s\\) and every policy \\(\\pi\\), we have</p> \\[ V^*(s) = V^{\\pi^*}(s) \\geq V^\\pi(s). \\] <p>The first equality says that the \\(V^{\\pi^*}\\), the value function for \\(\\pi^*\\), is equal to the optimal value function \\(V^*\\) for every state \\(s\\). Further, the inequality above says that \\(\\pi^*\\)\u2019s value is at least as large as the value of any other policy. In other words, \\(\\pi^*\\) as defined in Equation (3) is the optimal policy.  </p> <p>Note (Interesting Property of \\(\\pi^*\\))</p> <p>Note that \\(\\pi^*\\) has the interesting property that it is the optimal policy for all states \\(s\\). Specifically, it is not the case that if we were starting in some state \\(s\\) then there\u2019d be some optimal policy for that state, and if we were starting in some other state \\(s'\\) then there\u2019d be some other policy that\u2019s optimal policy for \\(s'\\). The same policy \\(\\pi^*\\) attains the maximum in Equation (1) for all states \\(s\\). This means that we can use the same policy \\(\\pi^*\\) no matter what the initial state of our MDP is.</p>"},{"location":"dl/mdp/#solving-finite-state-mdps","title":"Solving Finite-State MDPs","text":"<p>We now describe two efficient algorithms for solving finite-state MDPs. For now, we will consider only MDPs with finite state and action spaces. we will also assume that we know the state transition probabilities \\(\\{P_{sa}\\}\\) and the reward function \\(R\\).</p> <p>Algorithm (Value Iteration)</p> <p>The value iteration algorithm follows: </p> <ol> <li> <p>For each state \\(s\\), initialize \\(V(s) := 0\\).</p> </li> <li> <p>Repeat until convergence <code>{</code>      \u00a0\u00a0\u00a0\u00a0 For every state, update \\(V(s) := R(s) + \\max_{a \\in A} \\gamma \\sum_{s'} P_{sa}(s') V(s')\\) <code>}</code> </p> </li> </ol> <p>This algorithm is repeatedly trying to update the estimated value function using Bellman Equations \\((2)\\).</p> <p>Important Note (Two Ways of Updating the Inner Loop)</p> <p>There are two possible ways of performing the updates in the inner loop of the algorithm. In the first, we can first compute the new values for \\(V(s)\\) for every state \\(s\\), and then overwrite all the old values with the new values. This is called a synchronous update. In this case, the algorithm can be viewed as implementing a \u201cBellman backup operator\u201d that takes a current estimate of the value function, and maps it to a new estimate. </p> <p>Alternatively, we can also perform asynchronous updates. Here, we would loop over the states (in some order), updating the values one at a time.</p> <p>Definition (Policy Iteration)</p> <p>The policy iteration algorithm proceeds as follows: </p> <ol> <li> <p>Initialize \\(\\pi\\) randomly. </p> </li> <li> <p>Repeat until convergence <code>{</code>      \u00a0\u00a0\u00a0\u00a0(a) Let \\(V := V^\\pi\\).      \u00a0\u00a0\u00a0\u00a0(b) For each state \\(s\\), let \\(\\pi(s) := \\arg \\max_{a \\in A} \\sum_{s'} P_{sa}(s') V(s')\\).  <code>}</code> </p> </li> </ol> <p>Thus, the inner-loop repeatedly computes the value function for the current policy, and then updates the policy using the current value function. (The policy \\(\\pi\\) found in step (b) is also called the policy that is greedy with respect to \\(V\\).) Note that step (a) can be done via solving Bellman\u2019s equations as described earlier, which in the case of a fixed policy, is just a set of \\(|S|\\) linear equations in \\(|S|\\) variables.</p> <p>After at most a finite number of iterations of this algorithm, \\(V\\) will converge to \\(V^*\\), and \\(\\pi\\) will converge to \\(\\pi^*\\).</p> <p>Both value iteration and policy iteration are standard algorithms for solving MDPs, and there isn\u2019t currently universal agreement over which algorithm is better. For small MDPs, policy iteration is often very fast and converges with very few iterations. However, for MDPs with large state spaces, solving for \\(V^\\pi\\) explicitly would involve solving a large system of linear equations, and could be difficult. In these problems, value iteration may be preferred. For this reason, in practice value iteration seems to be used more often than policy iteration.</p>"},{"location":"dl/nn/","title":"Neural Network Architectures","text":"<p>Note (Naming Conventions)</p> <p>Notice that when we say N-layer neural network, we do not count the input layer. Therefore, a single-layer neural network describes a network with no hidden layers (input directly mapped to output). You may also hear these networks interchangeably referred to as \u201cArtificial Neural Networks\u201d (ANN) or \u201cMulti-Layer Perceptrons\u201d (MLP).</p>"},{"location":"dl/nn/#representational-power","title":"Representational Power","text":"<p>One way to look at Neural Networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: Are there functions that cannot be modeled with a Neural Network?</p> <p>Lemma (Representational Power of Neuron Network)</p> <p>Given any continuous function \\(f(x)\\) and some \\(\\epsilon&gt;0\\), there exists a Neural Network \\(g(x)\\) with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that \\(\\forall x,\\left| f(x)\u2212g(x) \\right| &lt; \\epsilon\\). In other words, the neural network can approximate any continuous function.</p> <p>Note (Why Not One Hidden Layer?)</p> <p>If one hidden layer suffices to approximate any function, why use more layers and go deeper? The answer is that the fact that a two-layer Neural Network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. The fact that deeper networks (with multiple hidden layers) can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.</p> <p>Note (More Layers or Not?)</p> <p>In practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper (4,5,6-layer) rarely helps much more. This is in stark contrast to Convolutional Networks, where depth has been found to be an extremely important component for a good recognition system (e.g. on order of 10 learnable layers). One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.</p> <p>Important Note (Setting Number of Layers and Their Sizes)</p> <p>How do we decide on what architecture to use when faced with a practical problem? First, note that as we increase the size and number of layers in a Neural Network, neurons can collaborate to express many more different functions. </p> <p> A Binary Classification Problem </p> <p>However, overfitting occurs if the NN has too much capability. But this does not mean we should choose a smaller model to avoid overfitting - there are many other preferred ways to prevent overfitting in Neural Networks such as L2 regularization, dropout, input noise. In practice, it is always better to use these methods to control overfitting instead of the number of neurons. The regularization strength is the preferred way to control the overfitting of a neural network. We can look at the results achieved by three different settings:</p> <p> </p> <p>Changing the regularization strength makes its final decision regions smoother with a higher regularization. You can play this here.</p>"},{"location":"dl/nn/#data-preprocessing","title":"Data Preprocessing","text":"<p>There are three common forms of data preprocessing a data matrix <code>X</code>, where we will assume that <code>X</code> is of size <code>[N x D]</code> (<code>N</code> is the number of data, <code>D</code> is their dimensionality).</p> <p>Strategy (Mean Subtraction)</p> <p>Mean subtraction is the most common form of preprocessing. It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension. In numpy, this operation would be implemented as: <code>X -= np.mean(X, axis=0)</code>. With images specifically, for convenience it can be common to subtract a single value from all pixels (e.g. <code>X -= np.mean(X)</code>), or to do so separately across the three color channels.</p> <p>Strategy (Normalization)</p> <p>Normalization refers to normalizing the data dimensions so that they are of approximately the same scale. There are two common ways of achieving this normalization. One is to divide each dimension by its standard deviation, once it has been zero-centered: <code>X /= np.std(X, axis=0)</code>. Another form of this preprocessing normalizes each dimension so that the min and max along the dimension is -1 and 1 respectively. It only makes sense to apply this preprocessing if you have a reason to believe that different input features have different scales (or units), but they should be of approximately equal importance to the learning algorithm. In case of images, the relative scales of pixels are already approximately equal (and in range from 0 to 255), so it is not strictly necessary to perform this additional preprocessing step.</p> Common Data Preprocessing Pipeline <p>Strategy (PCA and Whitening)</p> <p>PCA and Whitening is another form of preprocessing. In this process, the data is first centered as described above. Then, we can compute the covariance matrix that tells us about the correlation structure in the data: </p> <ul> <li> PCA and Whitening</li> </ul> <p> </p> Example (Visualizing Whitened Images) <p>The training set of CIFAR-10 is of size 50,000 x 3072, where every image is stretched out into a 3072-dimensional row vector. We can then compute the [3072 x 3072] covariance matrix and compute its SVD decomposition (which can be relatively expensive). What do the computed eigenvectors look like visually? An image might help:</p> <p> PCA / Whitening </p> <p>Important Note (In Practice)</p> <p>We mention PCA/Whitening in these notes for completeness, but these transformations are not used with Convolutional Networks. However, it is very important to zero-center the data, and it is common to see normalization of every pixel as well.</p> <p>Warning (Only Preprocess Training Data)</p> <p>Any preprocessing statistics (e.g. the data mean) must only be computed on the training data, and then applied to the validation / test data.</p>"},{"location":"dl/nn/#weight-initialization","title":"Weight Initialization","text":"<p>We have seen how to construct a Neural Network architecture, and how to preprocess the data. Before we can begin to train the network we have to initialize its parameters.</p> <p>Warning (Pitfall: All Zero Initialization)</p> <p>At the end of training, with a proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. We should not set all parameters to be 0 because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates.</p> <p>Strategy (Small Random Numbers Initialization)</p> <p>It is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The implementation for one weight matrix might look like <code>W = 0.01* np.random.randn(D,H)</code>, where <code>randn</code> samples from a zero mean, unit standard deviation Gaussian. </p> <p>Warning (Limitation of Small Random Numbers Initialization)</p> <p>It\u2019s not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient is proportional to the value of the weights). This could greatly diminish the \u201cgradient signal\u201d flowing backward through a network, and could become a concern for deep networks.</p> <p>Strategy (Calibrating The Variances with \\(\\frac{1}{\\sqrt{n}}\\))</p> <p>One problem with the above suggestion is that the distribution of the outputs from a randomly initialized neuron has a variance that grows with the number of inputs. It turns out that we can normalize the variance of each neuron\u2019s output to 1 by scaling its weight vector by the square root of its fan-in (i.e. its number of inputs). That is, the recommended heuristic is to initialize each neuron\u2019s weight vector as: <code>w = np.random.randn(n) / sqrt(n)</code>, where <code>n</code> is the number of its inputs. This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.</p> <ul> <li> Proof of this Strategy</li> </ul> <p>Strategy (Sparse Initialization)</p> <p>Another way to address the uncalibrated variances problem is to set all weight matrices to zero, but to break symmetry every neuron is randomly connected (with weights sampled from a small gaussian as above) to a fixed number of neurons below it. A typical number of neurons to connect to may be as small as 10.</p> <p>Important Note (Initializing The Biases)</p> <p>It is common to initialize the biases to be zero. For ReLU non-linearities, some people like to use small constant value such as 0.01 because this ensures that all ReLU units fire in the beginning and therefore obtain and propagate some gradient. However, it is not clear if this is useful and it is more common to simply use 0 bias initialization.</p> <p>Important Note (In Practice)</p> <p>The current recommendation is to use ReLU units and use the <code>w = np.random.randn(n) * sqrt(2.0/n)</code>, as discussed in He et al..</p> <p>Strategy (Batch Normalization)</p> <p>A recently developed technique by Ioffe and Szegedy called Batch Normalization alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian distribution at the beginning of the training.</p>"},{"location":"dl/nn/#regularization","title":"Regularization","text":"<p>There are several ways of controlling the capacity of Neural Networks to prevent overfitting:</p> <p>Strategy (L2 Regularization)</p> <p>For every weight \\(w\\) in the network, we add the term \\(\\frac{1}{2}\\lambda w^2\\) to the objective, where \\(\\lambda\\) is the regularization strength.</p> <p>Strategy (L1 Regularization)</p> <p>For each weight \\(w\\) we add the term \\(\\lambda \\left| w \\right|\\) to the objective.</p> <p>Note (L2 v.s. L1)</p> <p>In practice, if you are not concerned with explicit feature selection, L2 regularization can be expected to give superior performance over L1.</p> <p>Strategy (Elastic Net Regularization)</p> <p>Elastic net regularization is a combination of L1 &amp; L2 regularization: \\(\\lambda_1 \\left| w \\right| + \\lambda_2 w^2\\).</p> <p>Strategy (Max Norm Constraints)</p> <p>Max norm put constraints to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector <code>\\(\\vec{w}\\)</code> of every neuron to satisfy <code>\\(\\|\\vec{w}\\|_2 &lt; c\\)</code>. Typical values of <code>c</code> are on orders of 3 or 4. Some people report improvements when using this form of regularization. One of its appealing properties is that network cannot \"explode\" even when the learning rates are set too high because the updates are always bounded.</p> <p>Strategy (Dropout)</p> <p>Dropout is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in Dropout: A Simple Way to Prevent Neural Networks from Overfitting that complements the other methods (L1, L2, maxnorm). While training, dropout is implemented by only keeping a neuron active with some probability \\(p\\) (a hyperparameter), or setting it to zero otherwise.</p> <p> </p> <p>Important Note (In Practice)</p> <p>It is most common to use a single, global L2 regularization strength that is cross-validated. It is also common to combine this with dropout applied after all layers. The value of \\(p=0.5\\) is a reasonable default, but this can be tuned on validation data.</p> <p>Important Note (Implementing Dropout)</p> <pre><code>def dropout_forward(x, dropout_param):\n    \"\"\"Forward pass for inverted dropout.\n\n    Note that this is different from the vanilla version of dropout.\n    Here, p is the probability of keeping a neuron output, as opposed to\n    the probability of dropping a neuron output.\n    See http://cs231n.github.io/neural-networks-2/#reg for more details.\n\n    Inputs:\n    - x: Input data, of any shape\n    - dropout_param: A dictionary with the following keys:\n    - p: Dropout parameter. We keep each neuron output with probability p.\n    - mode: 'test' or 'train'. If the mode is train, then perform dropout;\n        if the mode is test, then just return the input.\n    - seed: Seed for the random number generator. Passing seed makes this\n        function deterministic, which is needed for gradient checking but not\n        in real networks.\n\n    Outputs:\n    - out: Array of the same shape as x.\n    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout\n    mask that was used to multiply the input; in test mode, mask is None.\n    \"\"\"\n    p, mode = dropout_param[\"p\"], dropout_param[\"mode\"]\n    if \"seed\" in dropout_param:\n        np.random.seed(dropout_param[\"seed\"])\n\n    mask = None\n    out = None\n\n    if mode == \"train\":\n        mask = (np.random.rand(*x.shape) &lt; p) / p\n        out = x * mask\n\n    elif mode == \"test\":\n        out = x\n        mask = None\n\n    cache = (dropout_param, mask)\n    out = out.astype(x.dtype, copy=False)\n\n    return out, cache\n\n\ndef dropout_backward(dout, cache):\n    \"\"\"Backward pass for inverted dropout.\n\n    Inputs:\n    - dout: Upstream derivatives, of any shape\n    - cache: (dropout_param, mask) from dropout_forward.\n    \"\"\"\n    dropout_param, mask = cache\n    mode = dropout_param[\"mode\"]\n\n    dx = None\n    if mode == \"train\":\n        dx = dout * mask\n\n    elif mode == \"test\":\n        dx = dout\n    return dx\n</code></pre> <p>So far, we\u2019ve discussed the static parts of a Neural Networks: how we can set up the network connectivity, the data, and the loss function. Now, we'll discuss the dynamics, or in other words, the process of learning the parameters and finding good hyperparameters.</p>"},{"location":"dl/nn/#gradient-checks","title":"Gradient Checks","text":"<p>Performing a gradient check is as simple as comparing the analytic gradient to the numerical gradient. In practice, the process is much more involved and error prone. Here are some tips, tricks, and issues to watch out for: (We discuss a lot of strategies. Non of them are trivial, however.)</p> <p>Strategy (Gradient Checks: Use The Centered Formula)</p> <p>The formula for the finite difference approximation when evaluating the numerical gradient looks as follows:</p> \\[ \\frac{df(x)}{dx} = \\frac{f(x+h) - f(x)}{h} \\quad (\\text{bad, do not use}) \\] <p>where \\( h \\) is a very small number, approximately 1e-5 or so. In practice, it turns out that it is much better to use the centered difference formula of the form:</p> \\[ \\frac{df(x)}{dx} = \\frac{f(x+h) - f(x-h)}{2h} \\quad (\\text{use instead}) \\] <p>This requires you to evaluate the loss function twice to check every single dimension of the gradient (so it is about 2 times as expensive), but the gradient approximation turns out to be much more precise. To see this, you can use Taylor expansion of \\( f(x+h) \\) and \\( f(x-h) \\) and verify that the first formula has an error on order of \\( O(h) \\), while the second formula only has error terms on order of \\( O(h^2) \\) (i.e. it is a second order approximation). </p> <ul> <li> Proof</li> </ul> <p>Strategy (Gradient Checks: Use Relative Error for The Comparison)</p> <p>How do we know if the two gradients are not compatible? We should not use the difference \\( |f'_a - f'_n| \\) and define the gradient check as failed if that difference is above a threshold. For example, consider the case where their difference is 1e-4. This seems like a very appropriate difference if the two gradients are about 1.0, but if the gradients were both on order of 1e-5 or lower, then we\u2019d consider 1e-4 to be a huge difference and likely a failure. Hence, it is always more appropriate to consider the relative error:</p> \\[ \\frac{|f'_a - f'_n|}{\\max(|f'_a|, |f'_n|)} \\] <p>Notice that normally the relative error formula only includes one of the two terms (either one), but I prefer to max (or add) both to make it symmetric and to prevent dividing by zero in the case where one of the two is zero (which can often happen, especially with ReLUs). However, one must explicitly keep track of the case where both are zero and pass the gradient check in that edge case.   In practice:</p> <ul> <li>relative error &gt; 1e-2 usually means the gradient is probably wrong</li> <li>1e-2 &gt; relative error &gt; 1e-4 should make you feel uncomfortable</li> <li>1e-4 &gt; relative error is usually okay for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.</li> <li>1e-7 and less you should be happy.</li> </ul> <p>Note (Deeper The Network, Higher The Error)</p> <p>Note that the deeper the network, the higher the relative errors will be. So if you are gradient checking the input data for a 10-layer network, a relative error of 1e-2 might be okay because the errors build up on the way. Conversely, an error of 1e-2 for a single differentiable function likely indicates incorrect gradient.</p> <p>Warning (Use Double Precision)</p> <p>A common pitfall is using single precision floating point to compute gradient check. It is often that case that you might get high relative errors (as high as 1e-2) even with a correct gradient implementation. In my experience I\u2019ve sometimes seen my relative errors plummet from 1e-2 to 1e-8 by switching to double precision.</p> <p>Strategy (Gradient Checks: Stick Around Active Range of Floating Point)</p> <p>It's a good idea to read through \u201cWhat Every Computer Scientist Should Know About Floating-Point Arithmetic\u201d, as it may demystify your errors and enable you to write more careful code. For example, in neural nets it can be common to normalize the loss function over the batch. However, if your gradients per datapoint are very small, then additionally dividing them by the number of data points is starting to give very small numbers, which in turn will lead to more numerical issues. This is why I like to always print the raw numerical/analytic gradient, and make sure that the numbers you are comparing are not extremely small (e.g. roughly 1e-10 and smaller in absolute value is worrying). If they are you may want to temporarily scale your loss function up by a constant to bring them to a \u201cnicer\u201d range where floats are more dense - ideally on the order of 1.0, where your float exponent is 0.</p> <p>Strategy (Gradient Checks: Kinks in The Objective)</p> <p>One source of inaccuracy to be aware of during gradient checking is the problem of kinks. Kinks refer to non-differentiable parts of an objective function, introduced by functions such as ReLU (\\(\\max(0, x)\\)), or the SVM loss, Maxout neurons, etc. Consider gradient checking the ReLU function at \\( x = -1e6 \\). Since \\( x &lt; 0 \\), the analytic gradient at this point is exactly zero. However, the numerical gradient would suddenly compute a non-zero gradient because \\( f(x+h) \\) might cross over the kink (e.g. if \\( h &gt; 1e-6 \\)) and introduce a non-zero contribution. You might think that this is a pathological case, but in fact this case can be very common. For example, an SVM for CIFAR-10 contains up to 450,000 \\(\\max(0, x)\\) terms because there are 50,000 examples and each example yields 9 terms to the objective. Moreover, a Neural Network with an SVM classifier will contain many more kinks due to ReLUs.</p> <p>Note that it is possible to know if a kink was crossed in the evaluation of the loss. This can be done by keeping track of the identities of all \u201cwinners\u201d in a function of form \\(\\max(x, y)\\); That is, was \\( x \\) or \\( y \\) higher during the forward pass. If the identity of at least one winner changes when evaluating \\( f(x+h) \\) and then \\( f(x-h) \\), then a kink was crossed and the numerical gradient will not be exact.</p> <p>Strategy (Gradient Checks: Use Only Few Datapoints)</p> <p>One fix to the above problem of kinks is to use fewer datapoints, since loss functions that contain kinks (e.g. due to use of ReLUs or margin losses etc.) will have fewer kinks with fewer datapoints, so it is less likely for you to cross one when you perform the finite different approximation. Moreover, if your gradcheck for only ~2 or 3 datapoints then you would almost certainly gradcheck for an entire batch. Using very few datapoints also makes your gradient check faster and more efficient.</p> <p>Strategy (Gradient Checks: Be Careful with The Step Size \\( h \\))</p> <p>It is not necessarily the case that smaller is better, because when \\( h \\) is much smaller, you may start running into numerical precision problems. Sometimes when the gradient doesn\u2019t check, it is possible that you change \\( h \\) to be 1e-4 or 1e-6 and suddenly the gradient will be correct. This wikipedia article contains a chart that plots the value of \\( h \\) on the x-axis and the numerical gradient error on the y-axis.</p> <p>Strategy (Gradient Checks: Gradcheck During a \u201cCharacteristic\u201d Mode of Operation)</p> <p>It is important to realize that a gradient check is performed at a particular (and usually random), single point in the space of parameters. Even if the gradient check succeeds at that point, it is not immediately certain that the gradient is correctly implemented globally. Additionally, a random initialization might not be the most \u201ccharacteristic\u201d point in the space of parameters and may in fact introduce pathological situations where the gradient seems to be correctly implemented but isn\u2019t. For instance, an SVM with very small weight initialization will assign almost exactly zero scores to all datapoints and the gradients will exhibit a particular pattern across all datapoints. An incorrect implementation of the gradient could still produce this pattern and not generalize to a more characteristic mode of operation where some scores are larger than others. Therefore, to be safe it is best to use a short burn-in time during which the network is allowed to learn and perform the gradient check after the loss starts to go down. The danger of performing it at the first iteration is that this could introduce pathological edge cases and mask an incorrect implementation of the gradient.</p> <p>Strategy (Gradient Checks: Don\u2019t Let The Regularization Overwhelm The Data)</p> <p>It is often the case that a loss function is a sum of the data loss and the regularization loss (e.g. L2 penalty on weights). One danger to be aware of is that the regularization loss may overwhelm the data loss, in which case the gradients will be primarily coming from the regularization term (which usually has a much simpler gradient expression). This can mask an incorrect implementation of the data loss gradient. Therefore, it is recommended to turn off regularization and check the data loss alone first, and then the regularization term second and independently. One way to perform the latter is to hack the code to remove the data loss contribution. Another way is to increase the regularization strength so as to ensure that its effect is non-negligible in the gradient check, and that an incorrect implementation would be spotted.</p> <p>Strategy (Gradient Checks: Remember to Turn off Dropout/Augmentations)</p> <p>When performing gradient check, remember to turn off any non-deterministic effects in the network, such as dropout, random data augmentations, etc. Otherwise, these can clearly introduce huge errors when estimating the numerical gradient. The downside of turning off these effects is that you wouldn\u2019t be gradient checking them (e.g. it might be that dropout isn\u2019t backpropagated correctly). Therefore, a better solution might be to force a particular random seed before evaluating both \\( f(x+h) \\) and \\( f(x-h) \\), and when evaluating the analytic gradient.</p> <p>Strategy (Gradient Checks: Check Only Few Dimensions)</p> <p>In practice the gradients can have sizes of million parameters. In these cases it is only practical to check some of the dimensions of the gradient and assume that the others are correct. Be careful: One issue to be careful with is to make sure to gradient check a few dimensions for every separate parameter. In some applications, people combine the parameters into a single large parameter vector for convenience. In these cases, for example, the biases could only take up a tiny number of parameters from the whole vector, so it is important to not sample at random but to take this into account and check that all parameters receive the correct gradients.</p> <p>Important Note (Gradient Checks in Code)</p> <pre><code>def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n    \"\"\"\n    a naive implementation of numerical gradient of f at x\n    - f should be a function that takes a single argument\n    - x is the point (numpy array) to evaluate the gradient at\n    \"\"\"\n\n    fx = f(x)  # evaluate function value at original point\n    grad = np.zeros_like(x)\n    # iterate over all indexes in x\n    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n    while not it.finished:\n\n        # evaluate function at x+h\n        ix = it.multi_index\n        oldval = x[ix]\n        x[ix] = oldval + h  # increment by h\n        fxph = f(x)  # evalute f(x + h)\n        x[ix] = oldval - h\n        fxmh = f(x)  # evaluate f(x - h)\n        x[ix] = oldval  # restore\n\n        # compute the partial derivative with centered formula\n        grad[ix] = (fxph - fxmh) / (2 * h)  # the slope\n        if verbose:\n            print(ix, grad[ix])\n        it.iternext()  # step to next dimension\n\n    return grad\n\ndef eval_numerical_gradient_array(f, x, df, h=1e-5):\n    \"\"\"\n    Evaluate a numeric gradient for a function that accepts a numpy\n    array and returns a numpy array.\n    \"\"\"\n    grad = np.zeros_like(x)\n    it = np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n    while not it.finished:\n        ix = it.multi_index\n\n        oldval = x[ix]\n        x[ix] = oldval + h\n        pos = f(x).copy()\n        x[ix] = oldval - h\n        neg = f(x).copy()\n        x[ix] = oldval\n\n        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n        it.iternext()\n    return grad\n\ndef eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n    \"\"\"\n    Compute numeric gradients for a function that operates on input\n    and output blobs.\n\n    We assume that f accepts several input blobs as arguments, followed by a\n    blob where outputs will be written. For example, f might be called like:\n\n    f(x, w, out)\n\n    where x and w are input Blobs, and the result of f will be written to out.\n\n    Inputs:\n    - f: function\n    - inputs: tuple of input blobs\n    - output: output blob\n    - h: step size\n    \"\"\"\n    numeric_diffs = []\n    for input_blob in inputs:\n        diff = np.zeros_like(input_blob.diffs)\n        it = np.nditer(input_blob.vals, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n        while not it.finished:\n            idx = it.multi_index\n            orig = input_blob.vals[idx]\n\n            input_blob.vals[idx] = orig + h\n            f(*(inputs + (output,)))\n            pos = np.copy(output.vals)\n            input_blob.vals[idx] = orig - h\n            f(*(inputs + (output,)))\n            neg = np.copy(output.vals)\n            input_blob.vals[idx] = orig\n\n            diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n\n            it.iternext()\n        numeric_diffs.append(diff)\n    return numeric_diffs\n\ndef eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n    return eval_numerical_gradient_blobs(\n        lambda *args: net.forward(), inputs, output, h=h\n    )\n\ndef grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n    \"\"\"\n    sample a few random elements and only return numerical\n    in this dimensions.\n    \"\"\"\n\n    for i in range(num_checks):\n        ix = tuple([randrange(m) for m in x.shape])\n\n        oldval = x[ix]\n        x[ix] = oldval + h  # increment by h\n        fxph = f(x)  # evaluate f(x + h)\n        x[ix] = oldval - h  # increment by h\n        fxmh = f(x)  # evaluate f(x - h)\n        x[ix] = oldval  # reset\n\n        grad_numerical = (fxph - fxmh) / (2 * h)\n        grad_analytic = analytic_grad[ix]\n        rel_error = abs(grad_numerical - grad_analytic) / (\n            abs(grad_numerical) + abs(grad_analytic)\n        )\n        print(\n            \"numerical: %f analytic: %f, relative error: %e\"\n            % (grad_numerical, grad_analytic, rel_error)\n        )\n</code></pre>"},{"location":"dl/nn/#before-learning-sanity-checks-tipstricks","title":"Before Learning: Sanity Checks Tips/Tricks","text":"<p>Here are a few sanity checks you might consider running before you plunge into expensive optimization:</p> <p>Strategy (Look for Correct Loss at Chance Performance)</p> <p>Make sure you\u2019re getting the loss you expect when you initialize with small parameters. It\u2019s best to first check the data loss alone (so set regularization strength to zero). For example, for CIFAR-10 with a Softmax classifier we would expect the initial loss to be 2.302, because we expect a diffuse probability of 0.1 for each class (since there are 10 classes), and Softmax loss is the negative log probability of the correct class so: -ln(0.1) = 2.302. For The Weston Watkins SVM, we expect all desired margins to be violated (since all scores are approximately zero), and hence expect a loss of 9 (since margin is 1 for each wrong class). If you\u2019re not seeing these losses there might be issue with initialization.</p> <p>Note (Second Sanity Check)</p> <p>As a second sanity check, increasing the regularization strength should increase the loss.</p> <p>Strategy (Overfit a Tiny Subset of Data)</p> <p>Lastly and most importantly, before training on the full dataset try to train on a tiny portion (e.g. 20 examples) of your data and make sure you can achieve zero cost. For this experiment it\u2019s also best to set regularization to zero, otherwise this can prevent you from getting zero cost. Unless you pass this sanity check with a small dataset it is not worth proceeding to the full dataset. Note that it may happen that you can overfit very small dataset but still have an incorrect implementation. For instance, if your datapoints\u2019 features are random due to some bug, then it will be possible to overfit your small training set but you will never notice any generalization when you fold it your full dataset.</p>"},{"location":"dl/nn/#babysitting-the-learning-process","title":"Babysitting The Learning Process","text":"<p>There are multiple useful quantities you should monitor during training of a neural network. These plots are the window into the training process and should be utilized to get intuitions about different hyperparameter settings and how they should be changed for more efficient learning.</p> <p>The x-axis of the plots below are always in units of epochs, which measure how many times every example has been seen during training in expectation (e.g. one epoch means that every example has been seen once). It is preferable to track epochs rather than iterations since the number of iterations depends on the arbitrary setting of batch size.</p> <p>Important Note (Tracking Loss Function)</p> <p>The first quantity that is useful to track during training is the loss, as it is evaluated on the individual batches during the forward pass. Below is a cartoon diagram showing the loss over time, and especially what the shape might tell you about the learning rate:</p> <p> Effects of Different Learning Rates </p> <p>The amount of \u201cwiggle\u201d in the loss is related to the batch size. When the batch size is 1, the wiggle will be relatively high. When the batch size is the full dataset, the wiggle will be minimal because every gradient update should be improving the loss function monotonically (unless the learning rate is set too high).</p> <p> Typical Loss Function over Time </p> <p>Some people prefer to plot their loss functions in the log domain. Since learning progress generally takes an exponential form shape, the plot appears as a slightly more interpretable straight line, rather than a hockey stick. Additionally, if multiple cross-validated models are plotted on the same loss graph, the differences between them become more apparent.  </p> <p>Sometimes loss functions can look funny: LossFunctions</p> <p>Important Note (Interpreting Train/Val Accuracy)</p> <p>The second important quantity to track while training a classifier is the validation/training accuracy. This plot can give you valuable insights into the amount of overfitting in your model:</p> <p> </p> <p>The gap between the training and validation accuracy indicates the amount of overfitting. Two possible cases are shown in the diagram on the left. The blue validation error curve shows very small validation accuracy compared to the training accuracy, indicating strong overfitting (note, it's possible for the validation accuracy to even start to go down after some point). When you see this in practice you probably want to increase regularization (stronger L2 weight penalty, more dropout, etc.) or collect more data. The other possible case is when the validation accuracy tracks the training accuracy fairly well. This case indicates that your model capacity is not high enough: make the model larger by increasing the number of parameters.</p> <p>Important Note (Tracking Ratio of Weights: Updates)</p> <p>The last quantity you might want to track is the ratio of the update magnitudes to the value magnitudes. Note: updates, not the raw gradients (e.g. in vanilla sgd this would be the gradient multiplied by the learning rate). You might want to evaluate and track this ratio for every set of parameters independently. A rough heuristic is that this ratio should be somewhere around 1e-3. If it is lower than this then the learning rate might be too low. If it is higher then the learning rate is likely too high. Here is a specific example:</p> <pre><code># assume parameter vector W and its gradient vector dW\nparam_scale = np.linalg.norm(W.ravel())\nupdate = -learning_rate*dW # simple SGD update\nupdate_scale = np.linalg.norm(update.ravel())\nW += update # the actual update\nprint update_scale / param_scale # want ~1e-3\n</code></pre> <p>Instead of tracking the min or the max, some people prefer to compute and track the norm of the gradients and their updates instead. These metrics are usually correlated and often give approximately the same results.</p> <p>Important Note (Activation / Gradient Distributions Per Layer)</p> <p>An incorrect initialization can slow down or even completely stall the learning process. Luckily, this issue can be diagnosed relatively easily. One way to do so is to plot activation/gradient histograms for all layers of the network. Intuitively, it is not a good sign to see any strange distributions - e.g. with tanh neurons we would like to see a distribution of neuron activations between the full range of [-1,1], instead of seeing all neurons outputting zero, or all neurons being completely saturated at either -1 or 1.</p> <p>Strategy (First-layer Visualizations)</p> <p>When one is working with image pixels it can be helpful and satisfying to plot the first-layer features visually: </p> <p> </p> <p>Left: Noisy features indicate could be a symptom: Unconverged network, improperly set learning rate, very low weight regularization penalty. </p> <p>Right: Nice, smooth, clean and diverse features are a good indication that the training is proceeding well.</p>"},{"location":"dl/nn/#parameter-updates","title":"Parameter Updates","text":"<p>Once the analytic gradient is computed with backpropagation, the gradients are used to perform a parameter update. There are several approaches for performing the update, which we discuss next. </p> <p>We note that optimization for deep networks is currently a very active area of research. In this section we highlight some established and common techniques you may see in practice and briefly describe their intuition.</p> <p>Algorithm (Vanilla Update)</p> <p>The Vanilla update is the simplest form of update: change the parameters along the negative gradient direction (since the gradient indicates the direction of increase, but we usually wish to minimize a loss function). Assuming a vector of parameters <code>x</code> and the gradient <code>dx</code>, the simplest update has the form:</p> <pre><code># Vanilla update\nx += - learning_rate * dx\n</code></pre> <p>where learning_rate is a hyperparameter. When evaluated on the full dataset, and when the learning rate is low enough, this is guaranteed to make non-negative progress on the loss function.</p> <p>Algorithm (SGD)</p> <p>Important Note (Implementing sgd)</p> <pre><code>def update(w, dw, config=None):\n    \"\"\"\n    Inputs:\n    - w: A numpy array giving the current weights.\n    - dw: A numpy array of the same shape as w giving the gradient of the\n        loss with respect to w.\n    - config: A dictionary containing hyperparameter values such as learning\n        rate, momentum, etc. If the update rule requires caching values over many\n        iterations, then config will also hold these cached values.\n\n    Returns:\n    - next_w: The next point after the update.\n    - config: The config dictionary to be passed to the next iteration of the\n        update rule.\n\n    NOTE: For most update rules, the default learning rate will probably not\n    perform well; however the default values of the other hyperparameters should\n    work well for a variety of different problems.\n\n    For efficiency, update rules may perform in-place updates, mutating w and\n    setting next_w equal to w.\n    \"\"\"\n\ndef sgd(w, dw, config=None):\n    \"\"\"\n    Performs vanilla stochastic gradient descent.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    \"\"\"\n    if config is None:\n        config = {}\n    config.setdefault(\"learning_rate\", 1e-2)\n\n    w -= config[\"learning_rate\"] * dw\n    return w, config\n</code></pre> <p>Algorithm (Momentum Update)</p> <p>Important Note (Implementing Momentum (with sgd))</p> <pre><code>def sgd_momentum(w, dw, config=None):\n    \"\"\"\n    Performs stochastic gradient descent with momentum.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - momentum: Scalar between 0 and 1 giving the momentum value.\n    Setting momentum = 0 reduces to sgd.\n    - velocity: A numpy array of the same shape as w and dw used to store a\n    moving average of the gradients.\n    \"\"\"\n    if config is None:\n        config = {}\n    config.setdefault(\"learning_rate\", 1e-2)\n    config.setdefault(\"momentum\", 0.9)\n    v = config.get(\"velocity\", np.zeros_like(w))\n\n    next_w = None\n\n    v = config[\"momentum\"] * v - config[\"learning_rate\"] * dw\n    next_w = w + v\n\n    config[\"velocity\"] = v\n\n    return next_w, config\n</code></pre> <p>Algorithm (rmsprop)</p> <p>Important Note (Implementing rmsprop)</p> <pre><code>def rmsprop(w, dw, config=None):\n\"\"\"\nUses the RMSProp update rule, which uses a moving average of squared\ngradient values to set adaptive per-parameter learning rates.\n\nconfig format:\n- learning_rate: Scalar learning rate.\n- decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n  gradient cache.\n- epsilon: Small scalar used for smoothing to avoid dividing by zero.\n- cache: Moving average of second moments of gradients.\n\"\"\"\nif config is None:\n    config = {}\nconfig.setdefault(\"learning_rate\", 1e-2)\nconfig.setdefault(\"decay_rate\", 0.99)\nconfig.setdefault(\"epsilon\", 1e-8)\nconfig.setdefault(\"cache\", np.zeros_like(w))\n\nnext_w = None\n\nconfig['cache'] = config['decay_rate'] * config['cache'] + (1 - config['decay_rate']) * (dw ** 2)\nnext_w = w - config['learning_rate'] / np.sqrt(config['cache'] + config['epsilon']) * dw\n\nreturn next_w, config\n</code></pre> <p>Algorithm (Adam)</p> <p>Important Note (Implementing Adam)</p> <pre><code>def adam(w, dw, config=None):\n    \"\"\"\n    Uses the Adam update rule, which incorporates moving averages of both the\n    gradient and its square and a bias correction term.\n\n    config format:\n    - learning_rate: Scalar learning rate.\n    - beta1: Decay rate for moving average of first moment of gradient.\n    - beta2: Decay rate for moving average of second moment of gradient.\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n    - m: Moving average of gradient.\n    - v: Moving average of squared gradient.\n    - t: Iteration number.\n    \"\"\"\n    if config is None:\n        config = {}\n    config.setdefault(\"learning_rate\", 1e-3)\n    config.setdefault(\"beta1\", 0.9)\n    config.setdefault(\"beta2\", 0.999)\n    config.setdefault(\"epsilon\", 1e-8)\n    config.setdefault(\"m\", np.zeros_like(w))\n    config.setdefault(\"v\", np.zeros_like(w))\n    config.setdefault(\"t\", 0)\n\n    next_w = None\n\n    keys = [\"learning_rate\", \"beta1\", \"beta2\", \"epsilon\", \"m\", \"v\", \"t\"]\n    lr, beta1, beta2, epsilon, m, v, t = (config.get(key) for key in keys)\n\n    t += 1\n    m = beta1 * m + (1 - beta1) * dw\n    mt = m / (1 - beta1 ** t)\n    v = beta2 * v + (1 - beta2) * (dw**2)\n    vt = v / (1 - beta2 ** t)\n    next_w = w - lr * mt / (np.sqrt(vt) + epsilon)\n\n    config['t'] = t\n    config['m'] = m\n    config['v'] = v\n\n    return next_w, config\n</code></pre>"},{"location":"dl/nn/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":""},{"location":"dl/nn/#evaluation","title":"Evaluation","text":"<p>Strategy (Model Ensembles)</p> <p>In practice, one reliable approach to improving the performance of Neural Networks by a few percent is to train multiple independent models, and at test time average their predictions. As the number of models in the ensemble increases, the performance typically monotonically improves (though with diminishing returns). Moreover, the improvements are more dramatic with higher model variety in the ensemble. There are a few approaches to forming an ensemble:</p> <ul> <li>Same model, different initializations. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.</li> <li>Top models discovered during cross-validation. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesn\u2019t require additional retraining of models after cross-validation.</li> <li>Different checkpoints of a single model. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that it is very cheap.</li> <li>Running average of parameters during training. Related to the last point, a cheap way of almost always getting an extra percent or two of performance is to maintain a second copy of the network\u2019s weights in memory that maintains an exponentially decaying sum of previous weights during training. This way you\u2019re averaging the state of the network over last several iterations. You will find that this \u201csmoothed\u201d version of the network almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode.</li> </ul> <p>One disadvantage of model ensembles is that they take longer to evaluate on test examples. An interested reader may find the recent work from Geoff Hinton on \u201cDark Knowledge\u201d inspiring, where the idea is to \u201cdistill\u201d a good ensemble back to a single model by incorporating the ensemble log likelihoods into a modified objective. </p>"},{"location":"dl/perceptron/","title":"Multiplayer Perceptrons","text":"<p>Linearity is not always the case for models in reality. We sometimes want to find a much more complicated relations between features and targets. </p>"},{"location":"dl/perceptron/#multilayer-perceptrons","title":"Multilayer Perceptrons","text":"<p>Model (Multilayer Perceptron)</p> <p>We can overcome the limitations of linear models by incorporating one or more hidden layers. This architecture is commonly called a multilayer perceptron (MLP). </p> <p> Multilayer Perceptron </p> <p>As before, we denote by the matrix \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) a minibatch of \\(n\\) examples where each example has \\(d\\) inputs (features). For a one-hidden-layer MLP whose hidden layer has \\(h\\) hidden units, we denote by \\(\\mathbf{H} \\in \\mathbb{R}^{n \\times h}\\) the outputs of the hidden layer, which are hidden representations. Since the hidden and output layers are both fully connected, we have hidden-layer weights \\(\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}\\) and biases \\(\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}\\) and output-layer weights \\(\\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times q}\\) and biases \\(\\mathbf{b}^{(2)} \\in \\mathbb{R}^{1 \\times q}\\). This allows us to calculate the outputs \\(\\mathbf{O} \\in \\mathbb{R}^{n \\times q}\\) of the one-hidden-layer MLP as follows:</p> \\[\\begin{align*} \\mathbf{H} &amp;= \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)} \\\\ \\mathbf{O} &amp;= \\mathbf{H} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} \\end{align*}\\] <p>However, this is not enough as using two layers which are computed linearly will still result in a linear model.</p> <p>To see this formally we can just collapse out the hidden layer in the above definition, yielding an equivalent single-layer model with parameters \\(\\mathbf{W} = \\mathbf{W}^{(1)} \\mathbf{W}^{(2)}\\) and \\(\\mathbf{b} = \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}\\):</p> \\[ \\mathbf{O} = \\left(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}\\right) \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\] <p>In order to realize the potential of multilayer architectures, we need one more key ingredient: a nonlinear activation function \\(\\sigma\\) to be applied to each hidden unit following the affine transformation. The outputs of activation functions \\(\\sigma(\\cdot)\\) are called activations. In general, with activation functions in place, it is no longer possible to collapse our MLP into a linear model:</p> \\[\\begin{align*} \\mathbf{H} &amp;= \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}) \\\\ \\mathbf{O} &amp;= \\mathbf{H} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} \\end{align*}\\]"},{"location":"dl/perceptron/#activation-functions","title":"Activation Functions","text":"<p>We'll first introduce two uncommon activation functions, and then a useful one.</p> <p>Definition (Sigmoid/Logistic)</p> <p>The sigmoid function, or logistic function, transforms those inputs whose values lie in the domain \\(\\mathbb{R}\\), to outputs that lie on the interval \\((0, 1)\\). For that reason, the sigmoid is often called a squashing function: it squashes any input in the range \\((-\\infty, \\infty)\\) to some value in the range \\((0, 1)\\):</p> \\[ \\text{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}. \\] <p>Below, we plot the sigmoid function. Note that when the input is close to 0, the sigmoid function approaches a linear transformation.</p> <p> Sigmoid Function </p> <p>The derivative of the sigmoid function is given by the following equation:</p> \\[ \\frac{d}{dx} \\text{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\text{sigmoid}(x) \\left(1 - \\text{sigmoid}(x)\\right). \\] <p>The derivative of the sigmoid function is plotted below. Note that when the input is 0, the derivative of the sigmoid function reaches a maximum of 0.25. As the input diverges from 0 in either direction, the derivative approaches 0.</p> <p> Derivative of Sigmoid Function </p> <p>Important Note (Limitation of Sigmoid Function)</p> <p>In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:</p> <ul> <li> <p>Sigmoids saturate and kill gradients. When the neuron\u2019s activation saturates at either tail of 0 or 1, the gradient at these regions is almost zero. During backpropagation, local gradient will be multiplied to the gradient of the gate\u2019s output. Therefore, if the local gradient is very small, it will \u201ckill\u201d the gradient and almost no signal will flow through the neuron. Additionally, if the initial weights are too large then most neurons would become saturated and the network will barely learn, so one must pay extra caution when initializing the weights. </p> </li> <li> <p>Sigmoid outputs are not zero-centered. Neurons in later layers in a Neural Network would be receiving data that is not zero-centered.If the data coming into a neuron is always positive, then the gradient on the weights during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights.</p> </li> </ul> <p>Definition (Tanh)</p> <p>Like the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs, transforming them into elements on the interval between \\((-1,1)\\):</p> \\[ \\tanh(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}. \\] <p>We plot the tanh function below. Note that as input nears 0, the tanh function approaches a linear transformation. Although the shape of the function is similar to that of the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system.</p> <p> Tanh Function </p> <p>The derivative of the tanh function is:</p> \\[ \\frac{d}{dx} \\tanh(x) = 1 - \\tanh^2(x). \\] <p>It is plotted below. As the input nears 0, the derivative of the tanh function approaches a maximum of 1. And as we saw with the sigmoid function, as input moves away from 0 in either direction, the derivative of the tanh function approaches 0.</p> <p> Derivative of Tanh Function </p> <p>Definition (ReLU Function)</p> <p>Rectified Linear Unit (ReLU) provides a very simple nonlinear transformation. Given an element \\(x\\), the function is defined as the maximum of that element and \\(0\\):</p> \\[ \\text{ReLU}(x) = \\max(x, 0) \\] <p> ReLU Function </p> <p>When the input is negative, the derivative of the ReLU function is 0, and when the input is positive, the derivative of the ReLU function is 1. Note that the ReLU function is not differentiable when the input takes value precisely equal to 0. In these cases, we default to the left-hand-side derivative and say that the derivative is 0 when the input is 0. We can get away with this because the input may never actually be zero.</p> <p> Derivative of ReLU Function </p> <p>Important Note (Pros and Cons of ReLU)</p> <ul> <li>(+) It was found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form. </li> <li>(+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero. </li> <li>(-) ReLU units can be fragile during training and can \u201cdie\u201d. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. For example, you may find that as much as 40% of your network can be \u201cdead\u201d if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.</li> </ul> <p>Definition (Leaky ReLU)</p> <p>Leaky ReLUs are one attempt to fix the \u201cdying ReLU\u201d problem. Instead of the function being zero when x &lt; 0, a leaky ReLU will instead have a small positive slope (of 0.01, or so). That is, the function computes:</p> \\[\\begin{align*} f(x) = 1(x&lt;0)(\\alpha x) + 1(x\\ge 0)(x) \\end{align*}\\] <p>where \\(\\alpha\\) is a small constant. Some people report success with this form of activation function, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, as seen in pReLU neurons.</p> <p>Definition (pReLU)</p> <p>The parametrized ReLU (pReLU) adds a linear term to ReLU, so some information still gets through, even when the argument is negative:</p> \\[\\begin{align*} \\text{pReLU}(x) = \\max(0,x) + \\alpha\\min(0,x) \\end{align*}\\] <p>Definition (Maxout)</p> <p>Maxout neuron (introduced by Goodfellow et al.) generalizes the ReLU and its leaky version. The Maxout neuron computes the function: </p> \\[\\begin{align*} \\max(w_1^Tx + b_1, w_2^Tx + b_2) \\end{align*}\\] <p>Note that both ReLU and Leaky ReLU are a special case of this form (e.g. for ReLU we have \\(w_1, b_1 = 0\\)). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.</p> <p>Note (Mixing Activation Functions)</p> <p>It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.</p> <p>Strategy (What Neuron Should I Use?)</p> <p>Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of \u201cdead\u201d units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.</p>"},{"location":"dl/softmax/","title":"Linear Regression: Softmax","text":"<p>In addition to the house price prediction problem, we are interested in another kind of linear regression problem: classification. Instead of asking \"how much,\" we now ask \"which.\"</p>"},{"location":"dl/softmax/#classification-problem","title":"Classification Problem","text":"<p>Definition (Classification Problem)</p> <p>There are two kinds of classification problem: \u00a0\u00a0\u00a0\u00a01. Hard assignments of examples to categories \u00a0\u00a0\u00a0\u00a02. Assess the probaility that each category applies</p> Example (Classification Problem Example) <p>Consider a \\(2 \\times 2\\) image. Let \\(x_1, x_2, x_3, x_4\\) denote the scalar value of each pixel. These are the four features in our model. Furthermore, assume that each image belongs to one of the categories \"cat,\" \"chicken,\" or \"dog.\" We want to determine the class to which a given image belongs.</p> <p>Definition (One-hot Encoding)</p> <p>For a sample in an \\(n\\)-category classification problem, the hot-encoding of that sample is a vector with \\(n\\) components, and the only component corresponding to the sample's category is set to 1 and all other components are set to 0.</p> Example (One-hot Encoding Example) <p>In the previous example, \\(n = 3\\), so \"cat\" can be encoded as \\((1, 0, 0)\\), \"chicken\" ... \\((0, 1, 0)\\), \"dog\" ... \\((0, 0, 1)\\).</p> <p>Note (Ordered-Category Encoding)</p> <p>If categories had some natural ordering among them, we can encode them in a much more intuitive way. For example, say we want to predict \\(\\{\\text{baby, toddler, adolescent}\\}\\), then it might make sense to cast this as an ordinal regression problem and keep the labels in this format: \\(\\text{label} \\in \\{1, 2, 3\\}\\)</p> <p>Model (Linear Model for Classification)</p> <p>Consider a network with one output layer and one input layer. For classification problems, #nodes in the output layer \\(=\\) #category. Let \\(\\mathbf{o}\\) denotes the output (a vector) of neural network, we have:</p> \\[\\mathbf{o} = \\mathbf{W}\\mathbf{x} + \\mathbf{b}\\] <p>Linear Model for Classification Problem  The above model represents a \\(2 \\times 2\\) image and 3-category classification problem. The output \\(\\mathbf{o}\\) is thus calculated by:</p> \\[ \\begin{align*} o_1 &amp;= w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 \\\\ o_2 &amp;= w_5x_1 + w_6x_2 + w_7x_3 + w_8x_4 \\\\ o_3 &amp;= w_9x_1 + w_{10}x_2 + w_{11}x_3 + w_{12}x_4 \\end{align*} \\] <p>At this point, we can, assuming a suitable loss function, try to minimize the difference between \\(\\mathbf{o}\\) and the hot-encoding of the sample. In fact, this works surprisingly well. However, we need to consider two drawbacks of this method: \u00a0\u00a0\u00a0\u00a01. \\(\\sum o_i \\neq 1\\) \u00a0\u00a0\u00a0\u00a02. \\(o_i\\) may be negative To address these issues, we use softmax.</p>"},{"location":"dl/softmax/#softmax-function","title":"Softmax Function","text":"<p>Definition (Softmax Function)</p> \\[ \\hat{\\mathbf{y}} = \\text{softmax}(\\mathbf{o}) \\; \\text{ where } \\; \\hat{y}_i = \\frac{\\text{exp}(o_i)}{\\sum_{j}\\text{exp}(o_j)}. \\] <p>Note</p> <p>By softmax definition, the largest coordinate of \\(\\mathbf{o}\\) corresponds to the most likely class. We do not even need to compute softmax to determine which class has the highest possibility.</p> <p>Important Note (Vectorization)</p> <p>To improve computational efficiency, we vectorize calculations in minibatches of data. Assume that we are given a minibatch \\(\\mathbf{X} \\in \\mathbb{R}^{n\\times d}\\) (\\(n\\) samples with \\(d\\) dimensions). Moreover, assume we have \\(q\\) categories. Then we have \\(\\mathbf{W} \\in \\mathbb{R}^{d\\times q}\\) and bias \\(\\mathbf{b} \\in \\mathbb{R}^{1\\times d}\\). Finally, we have:</p> \\[ \\begin{align} \\mathbf{O} &amp;= \\mathbf{X}\\mathbf{W} + \\mathbf{b} \\\\ \\hat{\\mathbf{Y}} &amp;= \\text{softmax}(\\mathbf{O}) \\end{align} \\] <p>This accelerates the dominant operation into a matrix\u2013matrix product.</p> <p>Important Note (Loss for Softmax)</p> <p>Define \\(f(x_i;W) = Wx_i\\). The Loss for Softmax of ith image is:</p> \\[\\begin{align*} L_i = -\\log(\\frac{e^f_{y_i}}{\\sum_j e^{f_j}}) \\end{align*}\\] <p>In code:</p> <pre><code>loss -= np.log(softmax[y[i]])\n</code></pre> <p>Important Note (Gradient of Softmax)</p> <p>Note that:</p> \\[\\begin{align*} L = -\\log(p_{y_i}) \\end{align*}\\] <p>Then after the calculation of the its derivative for \\(W\\), we have: </p> \\[ \\frac{\\partial L}{\\partial W_{p,q}} = (p_q - 1(q = y[i]))x_p \\] <p>In code:</p> <pre><code>softmax[y[i]] -= 1\ndW += np.outer(X[i], softmax)       # gradient\n</code></pre> <p>Remark (Viewing Softmax Output as Likelihood)</p> <p>The softmax function gives us a vector \\(\\hat{\\mathbf{y}}\\). We can interpret it as the estimated conditional probabilities of each category, given any input \\(\\mathbf{x}\\). e.g. \\(\\hat{y}_1=P(y=(1,0,0) \\mid \\mathbf{x})\\). Thus we have </p> \\[ P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}) \\] <p>To maximize \\(P(\\mathbf{Y} \\mid \\mathbf{X})\\), we minimize \\(-\\log{P(\\mathbf{y} \\mid \\mathbf{x})}\\): </p> \\[\\begin{align*} -\\log{P(\\mathbf{Y} \\mid \\mathbf{X})} = \\sum_{i=1}^n{-\\log{P(\\mathbf{y} \\mid \\mathbf{x})}} = \\sum_{i=1}^n{l({\\mathbf{y}}^{(i)}, {\\hat{\\mathbf{y}}}^{(i)})} \\end{align*}\\] <p>where </p> \\[\\begin{align*} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = -\\sum^q_{j=1}y_j\\log{\\hat{y_j}} \\end{align*}\\] <p>\\(-\\sum^q_{j=1}y_j\\log{\\hat{y_j}}\\) is also called Cross-Entrophy, which will be introduced later in information theory.</p>"},{"location":"dl/softmax/#brief-information-theory","title":"Brief Information Theory","text":"<p>Definition (Self-Information)</p> <p>Shannon defined the self-information \\(I(X)\\) of event \\(X\\) which has a probability of \\(p\\) as </p> \\[\\begin{align*} I(X) = -\\log_2{p} \\end{align*}\\] <p>as the bits of information we have received for event \\(X\\). For example, </p> \\[I(\\text{\"0010\"}) = -\\log_2{(p(\\text{\"0010\"}))} = -\\log_2{(\\frac{1}{2^4})} = 4 \\text{bits}\\] <p>Note (Log Base 2)</p> <p>When discussing information theory, the default base of \\(\\log\\) is \\(2\\) instead of \\(e\\) as \\(2\\) nicely corresponds to the unit \"bit\".</p> <p>Definition (Entrophy)</p> <p>For any random variable \\( X \\) that follows a probability distribution \\( P \\) with a probability density function (p.d.f.) or a probability mass function (p.m.f.) \\( p(x) \\), we measure the expected amount of information through entropy \\(H(X)\\) (or Shannon entropy):</p> \\[ H(X) = -E_{x \\sim P}[\\log p(x)]. \\] <p>To be specific, if \\( X \\) is discrete,</p> \\[ H(X) = -\\sum_i p_i \\log p_i, \\text{ where } p_i = P(X_i). \\] <p>Otherwise, if \\( X \\) is continuous, we also refer to entropy as differential entropy</p> \\[ H(X) = -\\int_x p(x) \\log{p(x)} \\, dx. \\] <p>Important Note (Why Expectation?)</p> <p>Why Expectation? Suppose there's a soccer match between China and Brazil. The probability that China wins is \\(0.001\\) and Brazil \\(0.999\\). If the news says Brazil wins, then there's hardly any information as this is hardly surprising. If China wins, then this is very surprising (abnormal) and contains more information. However, though China winning has more information, it does not mean that the whole system has more information. To estimate the entrophy of the entire system, we thus want to use expectation, which adds up self-information times possibility of events.</p> <p>Note (Why Log?)</p> <p>Why Log? - We want the entropy formula to be additive over independent random variables.</p> <p>Note (Why Negative?)</p> <p>Why Negative? - More frequent events should contain less information than less common events, since we often gain more information from an unusual case than from an ordinary one. \\(\\log\\) is monotonically increasing with the probabilities, and indeed negative for all values in \\([0,1]\\). Hence, we add a negative sign in front of function to construct a monotonically decreasing relationship between the probability of events and their entropy, which will ideally be always positive.</p> <p>How can we compare the difference between two distribution? A naive way may be to calculate their entrophy difference. However, this does not make any sense as two distinct distribution may have the same entrophy. A more proper way is to take one distribution as the standard, and measure \"surprises\" when the other distribution \"see\" the standard. e.g. in people's mind, China wins Brazil has \\(0.001\\) possibility, but if China actually wins, people will be very surprised.</p> <p>Definition (Kullback-Leibler Divergence)</p> <p>Given a random variable \\( X \\) that follows the probability distribution \\( P \\) with a p.d.f. or a p.m.f. \\( p(x) \\), and we estimate \\( P \\) by another probability distribution \\( Q \\) with a p.d.f. or a p.m.f. \\( q(x) \\). Then the Kullback-Leibler (KL) divergence (or relative entropy) between \\( P \\) and \\( Q \\) is</p> \\[\\begin{align*} D_{KL}(P \\parallel Q) &amp;=\\sum_{i=1}p_i\\cdot(f_Q(q_i)-f_P(p_i)) \\\\                       &amp;=\\sum_{i=1}p_i\\cdot(-\\log{q_i}-(-\\log{p_i})) \\\\                        &amp;= E_{x \\sim P} \\left[ \\log \\frac{p(x)}{q(x)} \\right] \\end{align*}\\] <p>where \\(f_Q(q_i)-f_P(p_i)\\) is the entrophy difference between the ith event.</p> <p>Definition (Cross-Entrophy)</p> <p>Note that </p> \\[\\begin{align*} D_{KL}(P \\parallel Q) &amp;=\\sum_{i=1}{p_i\\cdot(f_Q(q_i)-f_P(p_i))} \\\\                       &amp;=\\sum_{i=1}{p_i\\cdot(-\\log{q_i}-(-\\log{p_i}))} \\\\                        &amp;=\\sum_{i=1}{p_i\\cdot (-\\log q_i)} - \\sum_{i=1}{p_i\\cdot (-\\log p_i)} \\\\                       &amp;=\\sum_{i=1}{p_i\\cdot (-\\log q_i)} - \\text{entrophy of distribution P} \\end{align*}\\] <p>by Gibbs' inequality, we know that </p> \\[ - \\sum_{i=1}^{n} p_i \\log p_i \\leq - \\sum_{i=1}^{n} p_i \\log q_i \\] <p>Thus, to minimize KL divergence, it suffices to minimize \\(\\sum_{i=1}{p_i\\cdot (-\\log q_i)}\\). This term is called Cross-Entrophy. Formally, for a random variable \\( X \\), we can measure the divergence between the estimating distribution \\( Q \\) and the true distribution \\( P \\) via cross-entropy,</p> \\[ CE(P, Q) = -E_{x \\sim P}[\\log(q(x))]. \\] <p>By using properties of entropy discussed above, we can also interpret it as the summation of the entropy \\( H(P) \\) and the KL divergence between \\( P \\) and \\( Q \\), i.e.,</p> \\[ CE(P, Q) = H(P) + D_{KL}(P \\parallel Q) \\]"},{"location":"dl/unsupervised/","title":"Unsupervised Learning","text":"<p>Unlike supervised learning, unsupervised learning problems do not have labels over items.</p>"},{"location":"dl/unsupervised/#the-k-means-clustering-algorithm","title":"The k-Means Clustering Algorithm","text":"<p>Definition (k-Means Clustering Algorithm)</p> <p>In the clustering problem, we are given a training set \\(\\{x^{(1)}, \\ldots, x^{(n)}\\}\\), and want to group the data into a few cohesive \u201cclusters.\u201d Here, \\(x^{(i)} \\in \\mathbb{R}^d\\) as usual; but no labels \\(y^{(i)}\\) are given. So, this is an unsupervised learning problem.</p> <p>The k-means clustering algorithm is as follows:</p> <ol> <li> <p>Initialize cluster centroids \\(\\mu_1, \\mu_2, \\ldots, \\mu_k \\in \\mathbb{R}^d\\) randomly.</p> </li> <li> <p>Repeat until convergence: <code>{</code> </p> <p>For every \\(i\\), set</p> \\[ c^{(i)} := \\arg \\min_j \\|x^{(i)} - \\mu_j\\|^2. \\] <p>For each \\(j\\), set</p> \\[ \\mu_j := \\frac{\\sum_{i=1}^n 1\\{c^{(i)} = j\\} x^{(i)}}{\\sum_{i=1}^n 1\\{c^{(i)} = j\\}}. \\] <p>where \\(1\\{c^{(i)} = j\\}\\) is an indicator function that is 1 if the \\(i\\)-th data point is assigned to cluster \\(j\\), and 0 otherwise.</p> </li> </ol> <p> <code>}</code></p> <p>In the algorithm above, \\(k\\) (a parameter of the algorithm) is the number of clusters we want to find; and the cluster centroids \\(\\mu_j\\) represent our current guesses for the positions of the centers of the clusters. To initialize the cluster centroids (in step 1 of the algorithm above), we could choose \\(k\\) training examples randomly, and set the cluster centroids to be equal to the values of these \\(k\\) examples. (Other initialization methods are also possible.)</p> <p>The inner-loop of the algorithm repeatedly carries out two steps: (i) \u201cAssigning\u201d each training example \\(x^{(i)}\\) to the closest cluster centroid \\(\\mu_j\\), and (ii) Moving each cluster centroid \\(\\mu_j\\) to the mean of the points assigned to it. </p> <p> k-means Clustering </p> <p>Is the k-means algorithm guaranteed to converge? Yes it is, in a certain sense. In particular, let us define the distortion function.</p> <p>Definition (Distortion Function)</p> <p>Define the distortion function to be:</p> \\[ J(c, \\mu) = \\sum_{i=1}^n \\|x^{(i)} - \\mu_{c(i)}\\|^2 \\] <p>\\(J\\) measures the sum of squared distances between each training example \\(x^{(i)}\\) and the cluster centroid \\(\\mu_{c(i)}\\) to which it has been assigned. It can be shown that k-means is exactly coordinate descent on \\(J\\). Specifically, the inner-loop of k-means repeatedly minimizes \\(J\\) with respect to \\(c\\) while holding \\(\\mu\\) fixed, and then minimizes \\(J\\) with respect to \\(\\mu\\) while holding \\(c\\) fixed. Thus, \\(J\\) must monotonically decrease, and the value of \\(J\\) must converge. Usually, this implies that \\(c\\) and \\(\\mu\\) will converge too.</p> <p>Definition (Coordinate Descent)</p> <p>Coordinate descent is an optimization algorithm used to minimize a function by iteratively selecting one coordinate (or variable) at a time and optimizing the objective function with respect to that coordinate while keeping the other coordinates fixed.</p> <p>This is how it works:</p> <ol> <li> <p>Initialization: Start with an initial guess for the variables \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\).</p> </li> <li> <p>Iterative Optimization:</p> <ul> <li>For each coordinate \\(i\\), optimize the objective function \\(f(\\mathbf{x})\\) with respect to \\(x_i\\) while keeping all other coordinates fixed.</li> <li>Update \\(x_i\\) to the value that minimizes \\(f(\\mathbf{x})\\) in this step.</li> </ul> </li> <li> <p>Repeat: Continue iterating over all coordinates until convergence, i.e., until the change in the objective function or the variables is below a predefined threshold.</p> </li> </ol> <p>Note (Limitation of k-means)</p> <p>The distortion function \\(J\\) is a non-convex function, and so coordinate descent on \\(J\\) is not guaranteed to converge to the global minimum. In other words, k-means can be susceptible to local optima. Very often k-means will work fine and come up with very good clusterings despite this. But if you are worried about getting stuck in bad local minima, one common thing to do is run k-means many times (using different random initial values for the cluster centroids \\(\\mu_{j}\\) ). Then, out of all the different clusterings found, pick the one that gives the lowest distortion \\(J(c, \\mu)\\).</p> <ul> <li> Mixture of Gaussians (GMM)</li> <li> EM-Algorithm</li> <li> Factor Analysis</li> <li> Principal Components Analysis (PCA)</li> <li> Independent Components Analysis (ICA)</li> </ul>"},{"location":"dp/basics/","title":"Basics of Dynamic Programming","text":""},{"location":"dp/basics/#dp-overview","title":"DP Overview","text":"<p>Definition (Intuitive Definition of Dynamic Programming)</p> <p>Dynamic programming is a combination of recursion and memorization (and GUESSING). It is an exhaustive search. One can think of it as a \"careful brute-force.\" There are two uses for dynamic programming: \u00a0\u00a0\u00a0\u00a01.  Finding an optimal solution: We want a solution to be as large as possible or as small as possible  \u00a0\u00a0\u00a0\u00a02.  Counting the number of solutions: We want to calculate number of all possible solutions</p> <p>Definition (Bellman Equation)</p> <p>Bellman Equation is the formula of how subproblems can be used to calculate the main problem. e.g. \\(f(x) = f(x-1) + f(x-2)\\) is a Bellman Equation.</p> <p>Definition (Subproblem Dependency DAG)</p> <p>The dependency relation between subproblems form a directed acyclic graph - Subproblem Dependency DAG.  </p> <p>Warning (Limitation of DP)</p> <p>To use memorization, the subproblem dependency graph must be acyclic (it is called DAG).      </p> <p>Definition (Bottom-Up Implementation)</p> <p>Bottom-up implementing a DP algorithm means following topological order of the subproblem dependency DAG to do the exact same computation as up-bottom (recursion). For example, when calculating the ith term of Fibonacci sequence, recursively call \\(f(x) = f(x-1) + f(x-2)\\) is up-bottom; calculating the first term to the ith term is bottom-up. </p> <p>Methodology (Solving DP Problems)</p> <p>\u00a0\u00a0\u00a0\u00a01.  Define subproblems \\(\\rightarrow\\) #subproblems \u00a0\u00a0\u00a0\u00a02.  Guess (try all possibility after settling something) \\(\\rightarrow\\) #choices for guess \u00a0\u00a0\u00a0\u00a03.  Relate subproblem solutions \\(\\rightarrow\\) time/subproblem (often similar to guess) \u00a0\u00a0\u00a0\u00a04.  Recurs &amp; memorize (or build DP table bottom-up) \\(\\rightarrow\\) check if subproblem recurrence is acyclic \u00a0\u00a0\u00a0\u00a05.  Solve original problem.</p> <p>Note (DP Time Complexity)</p> <p>The time Complexity of DP can be calculated by </p> \\[ \\text{time} = \\Theta(\\text{#subproblems} \\times \\text{time per subproblem}) \\] <p>Note that we treat recursive calls in subproblems as \\(\\Theta(1)\\) as they have been memorized!</p> <p>Exercise (The Triangle - IOI 94')</p> <pre><code>        7\n      3   8\n    8   1   0\n  2   7   4   4\n4   5   2   6   5\n</code></pre> <p>Figure 1 shows a number triangle.</p> <p>Write a program that calculates the highest sum of numbers passed on a route that starts at the top and ends somewhere on the base.</p> <p>Each step can go either diagonally down to the left or diagonally down to the right.</p> Solution <p>Define \\(F[i][j]\\) be the maximum sum from top to \\((i,j)\\). The Bellman equation is:</p> \\[\\begin{align*} F[i][j] = A[i][j] + \\max     \\begin{cases}          F[i-1][j] \\\\         F[i-1][j-1] \\; \\text{if j&gt;1}\\\\     \\end{cases} \\end{align*}\\] <p>Exercise (Coin Problem)</p> <p>Consider a set of coins of value \\(\\{1, 3, 4\\}\\). Find the minimum number of coins s.t. the total value is equal to \\(S\\).</p> Solution <p>Note that this problem ask for the \"minimum.\" We think of using DP. Define \\(f(x)\\) be the minimum #coins where the total value equals to \\(x\\). Observe that to find \\(f(S)\\), it suffices to find \\(\\min\\{f(S-1), f(S-3), f(S-4)\\}\\) (We are guessing now). The Bellman Equation is:</p> \\[ f(x) = \\min\\{f(x-1), f(x-3), f(x-4)\\} + 1 \\] <p>The Bottom-up implementation would be:</p> <pre><code>f[1]=1, f[2]=2, f[3]=1, f[4]=1; // base case\nfor(int i=5;i&lt;=S;i++)\n    f[i]=min(min(f[i-1],f[i-3]),f[i-4])+1;\n</code></pre> <p>Exercise (Shortest Path: Bellman-Ford)</p> <p>Consider a non-negative weighted directed graph \\(G = (V, E)\\). Find the shortest path between vertex \\(s\\) and vertex \\(t\\). Note that the graph may contain cycles.</p> Solution <p>Note that this problem ask for the \"minimum.\" We think of using DP. Define \\(\\mathcal{S}(u,v)\\) be the shortest path from \\(u\\) to \\(v\\). A naive way of writing the Bellman Equation is </p> \\[\\begin{align*} \\mathcal{S}(s,t) = \\min_{(u,t)\\in E}(\\mathcal{S}(s,u) + w_{u,t}) \\end{align*}\\] <p>So far, by Bottom-up implementation, we'll go over all the vertices and edges once. The time complexity is \\(\\Theta(n + m)\\). However, this cannot deal with cyclic graph. To find the shortest path in a cyclic graph, we consider convert cyclic \\(G = (V, E)\\) to acyclic \\(G' = (V', E')\\) following \"steps\": </p> <p> </p> <p>Define \\(\\mathcal{S}_k(u,v)\\) be the shortest path from \\(u\\) to \\(v\\) in \\(k\\) steps. We now come up with a new Bellman Equation:</p> \\[\\begin{align*} \\mathcal{S}_k(s,t) = \\min_{(u,t)\\in E}(\\mathcal{S}_{k-1}(s,u) + w_{u,t}) \\end{align*}\\] <p>Since all edges in \\(G\\) are non-negatively weighted, the maximum steps can take is \\(n-1\\). The #subproblems is now \\(n^2\\) (\\(k\\) can be 0; \\(k\\) has \\(n\\) choices, vertex \\(t\\) has \\(n\\) choices). The time complexity is \\(\\Theta(nm)\\) because for each \\(k\\), we go over all edges exactly once.</p> <p>The Bottom-up implementation would be:</p> <pre><code>set all values in s equal to 1e9\nS[0][s] = 0;\nfor(int k=1;k&lt;n;k++)\n    for(int t=1;t&lt;=n;t++)\n        for(all (u,t) in E)\n            S[k][u]=min(S[k][u], S[k-1][u]+w[u][t]);\n</code></pre> <p>Important Note (Alternative View of DP)</p> <p>For optimization problem, we can view DP as searching for the shortest (longest) path on a DAG.</p> <p>Definition (Parent Pointer)</p> <p>The parent pointer is used to record \"plan\" rather than the \"value.\" It keeps track of how the optimal solution arrives.</p> <p>Exercise (Text Justification)</p> <p>In a text justification problem, define the \"badness\" of a line starting from ith word and ending in jth word to be: </p> \\[\\begin{align*} \\text{badness}(i,j) =      \\begin{cases}      {(\\text{page width} - \\text{words width sum})}^2 \\\\     \\infty \\\u00a0 \\text{ if unfit}      \\end{cases} \\end{align*}\\] <p>Find the smallest badness sum of the text. In addition, find all the indexes of words that start a line when the smallest badness sum is achieved.</p> Solution <p>Define the subproblem <code>dp[i]</code> to be the smallest badness sum of the text starting from ith word. This means there are total \\(n\\) subproblems. After setting <code>i</code>, we need to guess which word to end the line (\\(\\mathcal{O}(n)\\)). Thus, we have the Bellman equation to be:</p> \\[\\begin{align*} \\text{dp[i]} = \\min(\\text{dp[j]} + \\text{badness}(i,j) \\text{ for j in range(i+1, n+1)})  \\end{align*}\\] <p>Check the topological order: the order is \\(i = n, n-1, n-2, ..., 0\\).  The time complexity is \\(\\mathcal{O}(n^2)\\).  Base Case: <code>dp[0] = 0</code>  The parent point is \\(\\text{parent[i]} = \\text{argmin}(...) = j \\text{ value}\\).  To print the plan, we access parent pointers this order: 0, parent[0], parent[parent[0]], ...   </p>"},{"location":"dp/basics/#dp-for-string-problem","title":"DP for String Problem","text":"<p>Methodology (Picking Subproblem for String/Sequence Input)</p> <p>When the input is a string or a sequence, consider choosing these as the subproblem:  \u00a0\u00a0\u00a0\u00a01.  suffixes <code>x[i:]</code> \\(\\mathcal{O}(n)\\) \u00a0 topo: right to left  \u00a0\u00a0\u00a0\u00a02.  prefixes <code>x[:i]</code> \\(\\mathcal{O}(n)\\)   \u00a0 topo: left to right  \u00a0\u00a0\u00a0\u00a03.  substring <code>x[i:j]</code> \\(\\mathcal{O}(n^2)\\) \u00a0 topo: short to long (length)</p> <p>Exercise (Parenthesization)</p> <p>Consider the problem of matrix multiplication. Given \\(n\\) matrices \\(A_1, A_2, ..., A_n\\). Find the minimum cost of multiplication \\(A_1A_2...A_n\\). Suppose the cost of \\((a,b) \\times (b,c)\\) is \\(a\\times b \\times c\\). You can change the order of multiplication.</p> Solution <p>We choose substring as our subproblem. Define <code>dp[i][j]</code> to be the minimum cost of multiplying \\(A_iA_{i+1}...A_j\\). #subproblem \\(= \\mathcal{O}(n^2)\\). The Bellman equation is:</p> \\[\\begin{align*} \\text{dp}[i][j] = \\min_{i\\le k &lt; j}(dp[i][k] + dp[k+1][j] + \\text{ cost of } A_{i:k} \\times A_{k+1:j}) \\end{align*}\\] <p>The time complexity: \\(\\Theta(n^3)\\). Topological order: increasing substring size.</p> <p>Bottom-up implementation:</p> <pre><code>for(int len=2;len&lt;=n;len++){\n    for(int i=1;i+len-1&lt;=n;i++){\n        int j=i+len-1;\n        for(int k=i;k&lt;j;k++){\n            dp[i][j] = min(dp[i][j], dp[i][k]+dp[k+1][j]+cost);\n        }\n    }\n}\n</code></pre> <p>Exercise (Edit Distance)</p> <p>Given two strings \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\), what's the cheapest possible sequence of character edits to turn \\(\\mathbf{x}\\) into \\(\\mathbf{y}\\)? Here are three choices of editing (you can do these operations anywhere in the string):  \u00a0\u00a0\u00a0\u00a0 1.  insert C, cost \\(c_i\\)  \u00a0\u00a0\u00a0\u00a0 2.  delete C, cost \\(c_d\\)  \u00a0\u00a0\u00a0\u00a0 3.  replace C with C', cost \\(c_{C',r}\\) </p> Solution <p>The subproblem is suffixes: edit distance on <code>x[i:]</code> &amp; <code>y[j:]</code> for all i, j. #subproblems = \\(nm\\). Now guess. To convert \\(\\mathbf{x}\\) to \\(\\mathbf{y}\\), we must somehow make the first character \\(C_x\\) in \\(\\mathbf{x}\\) equal to the first character \\(C_y\\) in \\(\\mathbf{y}\\). We now have three choices:  \u00a0\u00a0\u00a0\u00a0 1. insert \\(C_y\\) at the head of \\(\\mathbf{x}\\)  \u00a0\u00a0\u00a0\u00a0 2. delete \\(C_x\\)  \u00a0\u00a0\u00a0\u00a0 3. replace \\(C_x\\) with \\(C_y\\). </p> <p>These choices cover all the possibility. Thus, we have the Bellman equation:</p> \\[\\begin{align*} \\text{dp[i][j]} = \\min(\\text{dp[i][j+1]}+c_i, \\text{dp[i+1][j+1]}+c_d, \\text{dp[i+1][j+1]}+c_{C_y,r}) \\end{align*}\\] <p>Now, draw the dependency table. Note that dp[i][j] only depends on dp[i][j+1] and dp[i+1][j+1].</p> <p> </p> <p>The Bottom-up implementation would be from buttom to top of the table (right to left is also fine):</p> <pre><code>for(int i=n;i&gt;=0;i--)\n    for(int j=m;j&gt;=0;j--)\n        dp[i][j] = min(dp[i][j+1]+ci, min(dp[i+1][j+1]+cd, dp[i+1][j+1]+c[j]));\n</code></pre> <p>The final answer would be dp[0][0].  Each subproblem takes constant time. The total time complexity is \\(\\Theta(nm)\\).</p> <p>Exercise (LCS: Longest Common Subsequence)</p> <p>Given two strings \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). Drop some characters in each string to make the remaining strings equal. What is length of the longest remaining string (Longest Common Subsequence)?</p> Solution <p>Note that if we do not allow replacement in the \"Edit Distance\" problem, we are doing something very similar to LCS which only allows dropping. The precise way of converting \"Edit Distance\" problem to LCS is by defining:  \u00a0\u00a0\u00a0\u00a01.  insert C, cost \\(1\\)  \u00a0\u00a0\u00a0\u00a02.  delete C, cost \\(1\\)  \u00a0\u00a0\u00a0\u00a03.  replace C with C', cost \\(0\\) if C = C', else cost \\(\\infty\\) </p> <p>Detailed solution: Define \\(F[i][j]\\) be the LCS of \\(\\mathbf{x}[1:i]\\) and \\(\\mathbf{y}[1:j]\\). The Bellman equation is:</p> \\[\\begin{align*} F[i][j] = \\max     \\begin{cases}          F[i-1][j] \\\\         F[i][j-1] \\\\         F[i-1][j-1] + 1 \\qquad \\text{if } A[i] = B[j]     \\end{cases} \\end{align*}\\] <p>Exercise (LIS: Longest Increasing Subsequence)</p> <p>Given a sequence \\(\\mathbf{A}\\) with length \\(N\\), find the length of the longest increasing subsequence.</p> Solution <p>This time we use prefixes. However, to guess all the possibility, we now define our subproblem in a way that enforces an element to be in the LIS: Define \\(F[i]\\) to be the length of LIS that has \\(\\mathbf{A}[i]\\) as its last element. The Bellman equation is </p> \\[\\begin{align*} F[i] = \\max_{0\\le j &lt; i, \\mathbf{A}[j]&lt;\\mathbf{A}[i]}{F[j] + 1} \\end{align*}\\] <p>The key point here is: we cannot define \\(F[i]\\) to be the LIS in \\(A[1:i]\\) as we do not know whether \\(F[1:N]\\) also uses the same subsequence in \\(F[i]\\). In other word, we are not counting all the possibility. </p> <p>Methodology (Two Kinds of Guessing)</p> <p>\u00a0\u00a0\u00a0\u00a01.  Guess which subproblem to use to solve bigger subproblem.  \u00a0\u00a0\u00a0\u00a02.  Add more subproblems to guess/remember more (e.g. knapsack)</p> <p>Exercise (Knapsack)</p> <p>Given a knapsack (a bag) with capacity \\(C\\) and a list of items of weight \\(w_i\\). Find the maximum weight one can take.</p> Solution <p>Subproblem = suffix of items. Define dp[i][c] to be the maximum weight when the knapsack has c capacity and item[i:] left to put in. #subproblems = \\(\\Theta(nC)\\). Guessing: is item i in the knapsack or not? Two choices. Bellman equation:</p> \\[\\begin{align*} \\text{dp[i][c]} = \\max(\\text{dp[i+1][c]}, \\text{dp}[i+1][c-w_i] + w_i) \\end{align*}\\] <p>The time complexity is \\(\\Theta(nC)\\).</p> <p>We now have gone through many basic problems of dynamic programming. In other chapters of DP, we will see more variation of these problems (LCS, LIS, Knapsack...).</p>"},{"location":"dp/interval/","title":"Interval DP","text":"<p>Interval DP is a special kind of linear DP. It can be viewed as \"DP for String Problems\" which use substring as subproblems.</p> <p>Exercise (Merging Stones)</p> <p>Given \\(n\\) piles of rocks, the ith pile has weight \\(A_i\\), merging two adjacent piles cost the sum of their weight, find the minimum cost to merge all the piles into one pile.</p> Solution <p>Note that every pile of rocks can be viewed as a merge of several piles. Define \\(f[l,r]\\) be the minimum cost to merge lth pile and rth pile. The Bellman equation is:</p> \\[\\begin{align*} f[l,r] = \\underset{l\\le k&lt;r}{\\min}{\\{f[l,k] + f[k+1,r]\\} + \\sum_{i=1}^{r}A_i} \\end{align*}\\] <p>Note that when we use substrings as subproblems, we update array from smallest interval to largest interval. So given <code>l</code>, we'll use <code>r = l + len -1</code>. We'll use prefixes sum to calculate \\(\\sum_{i=1}^{r}A_i\\).</p> <pre><code>memset(f, 0x3f, sizeof(f));  // infinitely large\nfor(int i=1;i&lt;=n;i++){\n    f[i][i]=0;\n    sum[i]=sum[i-1]+a[i];\n}\nfor(int len=2;len&lt;=n;len++){\n    for(int l=1;l&lt;=n-len+1;l++){\n        int r = l + len -1;\n        for(int k=l;k&lt;r;k++)\n            f[l][r] = min(f[l][r], f[l][k]+f[k+1][r]);\n        f[l][r] += sum[r] - sum[l-1];\n    }\n}\n\ncout&lt;&lt;f[1][N];\n</code></pre>"},{"location":"dp/knapsack/","title":"Knapsack","text":"<p>Knapsack problem is a special topic in linear DP. It can be viewed as \"DP for String Problems\" which use prefixes as subproblems. We have already discussed one classic knapsack problem in the previous chapter. In this chapter, we will dive deeper into variations of Knapsack.</p>"},{"location":"dp/knapsack/#0-1-knapsack","title":"0-1 Knapsack","text":"<p>Model (0-1 Knapsack)</p> <p>Given \\(N\\) items. \\(W_i\\) denotes the weight/volume/size of the ith item. \\(V_i\\) denotes the value of the ith item. Now, we have a knapsack with capacity \\(M\\). Want to know the maximum value when putting items into the knapsack without exceeding the capacity. Each item can only be chosen once.</p> <p>We call this type of problems as \"0-1 Knapsack\" because we are guessing whether an item should be in the knapsack or not. The common strategy to solve 0-1 Knapsack is to use prefixes (items that have been already decided) + updated capacity as subproblems (Later we will find a more elegant way to define subproblems for 0-1 Knapsack). Thus, the solution to above problem is:</p> <p>Define \\(F[i][j]\\) to be the maximum value of first i items + j capacity. The Bellman equation is:</p> \\[\\begin{align*} F[i][j] = \\max \\begin{cases} F[i-1][j] \\\\ F[i-1][j-W_i]+V_i\\qquad \\text{if } j \\ge W_i \\\\ \\end{cases} \\end{align*}\\] <p>The bottom-up implementation is:</p> <pre><code>memset(f, 0xcf, sizeof(f)); // setting inital value to be negative infinite (-808464433)\nf[0][0] = 0; // Base Case\nfor(int i=1;i&lt;=n;i++)\n    for(int j=0;j&lt;=m;j++)\n        if(j&gt;=w[i])\n            f[i][j] = max(f[i-1][j], f[i-1][j-w[i]]+v[i]);\n        else\n            f[i][j] = f[i-1][j];\n</code></pre> <p>Strategy (Rolling Array)</p> <p>Note that \\(F[i][j]\\) only relies on \\(F[i-1][0:j]\\). We can use Rolling Array to optimize space cost:</p> <pre><code>int f[2][MAX_M+1];\nmemset(f, 0xcf, sizeof(f));\nf[0][0];\nfor(int i=1;i&lt;=n;i++){\n    for(int j=0;j&lt;=m;j++)\n        if(j&gt;=w[i])\n            f[i&amp;1][j] = max(f[(i-1)&amp;1][j], f[(i-1)&amp;1][j-w[i]]+v[i]);\n        else\n            f[i&amp;1][j] = f[(i-1)&amp;1][j];\n}    \n\nint ans = 0;\nfor(int j=0;j&lt;=m;j++){\n    ans = max(ans, f[n&amp;1][j]);\n}\n</code></pre> <p>Note that <code>i&amp;1</code> is \\(1\\) when <code>i</code> is odd, \\(0\\) when <code>i</code> is even. The space complexity is thus \\(\\mathcal{O}(M)\\) instead of \\(\\mathcal{O}(NM)\\).</p> <p>Important Note (Simplifying Template)</p> <p>Note that each \\(F[i-1][j]\\) is only responsible for \\(F[i][j]\\), i.e. one copy. Thus, we can make the code even simpler by using only one dimension: \\(F[j]\\). The final version of 0-1 Knapsack code is:</p> <pre><code>int f[MAX_M+1];\nmemset(f, 0xcf, sizeof(f));\nf[0]=0;\nfor(int i=1;i&lt;=n;i++)\n    for(int j=m;j&gt;=v[i];j--)\n        f[j] = max(f[j], f[j-v[i]] + w[i]);\n</code></pre> <p>We need to update <code>f[j]</code> in reverse order (from <code>m</code> to <code>v[i]</code>) because <code>f[j]</code> is updated by <code>f[j-v[i]]</code>, which is less than <code>j</code> so it would be updated before we update <code>f[j]</code> in forward order (from <code>v[i]</code> to <code>m</code>). </p> <p>Moreover, let us think about the meaning of updating <code>f[j]</code> in forward order. If we update <code>f[j-v[i]]</code> before updating <code>f[j]</code>, what might happen is: <code>f[j-v[i]]</code> takes ith item, i.e. </p> \\[f[j-v[i]] = max(f[j], f[j-v[i]] + w[i]) = f[j-v[i]] + w[i]\\] <p>Then, if <code>f[j]</code> again takes in ith item, we took ith item for two times. This indicates that by looping in forward order, we allow items to be taken multiple times instead of just one time.</p> <p>Exercise (Counting in 0-1 Knapsack)</p> <p>Given a set of \\(n\\) integers \\(A = \\{a_1, a_2, \\cdots, a_n\\}\\). Count the number of subsets of \\(A\\) such that the sum of the subset equals to \\(M\\).</p> Solution <p>This is exactly a 0-1 Knapsack problem. The only difference is we add up \\(dp[j]\\) instead of using \\(\\max\\). Note that the base case is <code>dp[0]=1</code>. </p> <pre><code>#include&lt;bits/stdc++.h&gt;\nusing namespace std;\nint dp[10200];\nint main(){\n    int n,m;\n    cin&gt;&gt;n&gt;&gt;m;\n    dp[0] = 1;\n    for(int i=1;i&lt;=n;i++){\n        int w;\n        cin&gt;&gt;w;\n        for(int j=m;j&gt;=w;j--){\n            dp[j] += dp[j-w];\n        }\n    }\n\n    cout&lt;&lt;dp[m];\n}\n</code></pre>"},{"location":"dp/knapsack/#unbounded-knapsack-ukp","title":"Unbounded Knapsack (UKP)","text":"<p>Model (Unbounded Knapsack)</p> <p>Given \\(N\\) items. \\(W_i\\) denotes the weight/volume/size of the ith item. \\(V_i\\) denotes the value of the ith item. Now, we have a knapsack with capacity \\(M\\). Want to know the maximum value when putting items into the knapsack without exceeding the capacity. Each item has infinite copies.  </p> <p>The naive way of thinking this problem is to view each item to be multiple items and then use the exact same method as 0-1 Knapsack. Define \\(s\\) to be the maximum number of ith item we can take under capacity \\(j\\). Then, the Bellman equation is:</p> \\[\\begin{align*} F[i][j] = \\max \\begin{cases} F[i-1][j] \\\\ F[i-1][j-W_i]+V_i\\; \\\\ F[i-1][j-2W_i]+2V_i\\; \\\\ \\cdots                  \\\\ F[i-1][j-sW_i]+sV_i\\;     \\\\ \\end{cases} \\end{align*}\\] <p>However, this is too slow. We need a better way to do this:</p> <p>Observe the difference between the above equation and the second equation which replaces \\(j\\) with \\(j-W_i\\): </p> \\[\\begin{align*} F[i][j] = \\max \\begin{cases} F[i-1][j] \\\\ F[i-1][j-W_i]+V_i\\; \\\\ F[i-1][j-2W_i]+2V_i\\; \\\\ \\cdots                  \\\\ F[i-1][j-sW_i]+sV_i\\;     \\\\ \\end{cases} \\end{align*}\\] \\[\\begin{align*} F[i][j-W_i] = \\max \\begin{cases} F[i-1][j-W_i] \\\\ F[i-1][j-2W_i]+V_i\\; \\\\ F[i-1][j-3W_i]+2V_i\\; \\\\ \\cdots                  \\\\ F[i-1][j-sW_i]+(s-1)V_i\\;     \\\\ \\end{cases} \\end{align*}\\] <p>From above, notice that </p> \\[\\begin{align*} F[i][j] = \\max(f[i-1][j], f[i][j-W_i] + V_i) \\end{align*}\\] <p>This indicates that we can update \\(f[i][j-W_i]\\) first. Intuitively, we can interpret this equation as the maximum value of choosing first \\(i\\) items under \\(j\\) capacity is the maximum between choosing \\(0\\) ith item and choosing another ith item in addition to \\(F[i][j-W_i]\\), which is the maximum value of choosing first \\(i\\) items under \\(j-W_i\\) capacity. The final Bellman equation is:</p> \\[\\begin{align*} F[i][j] = \\max \\begin{cases} F[i-1][j] \\\\ F[\\mathbf{i}][j-W_i]+V_i \\qquad \\text{if } j \\ge W_i \\\\ \\end{cases} \\end{align*}\\] <p>Let us compare this Bellman equation with 0-1 Knapsack's:</p> \\[\\begin{align*} F[i][j] = \\max \\begin{cases} F[i-1][j] \\\\ F[\\mathbf{i-1}][j-W_i]+V_i \\qquad \\text{if } j \\ge W_i \\\\ \\end{cases} \\end{align*}\\] <p>The change is from \\(i-1\\) to \\(i\\). In brief, in Unbounded Knapsack, we allow ith item to be picked multiple time.</p> <p>Similar to 0-1 Knapsack, we can also omit one dimension of subproblems. In fact, we have already discussed the implementation of Unbounded Knapsack in 0-1 Knapsack. The bottom-up implementation is:</p> <pre><code>int f[MAX_M+1];\nmemset(f, 0xcf, sizeof(f));\nf[0]=0;\nfor(int i=1;i&lt;=n;i++)\n    for(int j=v[i];j&lt;=m;j++)\n        f[j] = max(f[j], f[j-v[i]] + w[i]);\n</code></pre> <p>Important Note (Uniqueness of Unbounded Knapsack)</p> <p>In all Knapsack problems that have subproblems reduced to one dimension, only Unbounded Knapsack is implemented in forward looping order. If not Unbounded Knapsack, the looping is always from <code>m</code> to <code>v[i]</code> (from larger to smaller) instead of <code>v[i]</code> to <code>m</code> (from smaller to larger). </p> <p>Exercise (Counting in UKP)</p> <p>Given a currency system with \\(n\\) denominations, count the number of ways to pay for a \\(m\\) value bill.</p> Solution <p>This is exactly a Unbounded Knapsack problem. The only difference is we add up \\(dp[j]\\) instead of using \\(\\max\\). Note that the base case is <code>dp[0]=1</code>. </p> <pre><code>#include&lt;bits/stdc++.h&gt;\nusing namespace std;\nlong long dp[3020];\nint main(){\n    int n,m;\n    cin&gt;&gt;n&gt;&gt;m;\n    dp[0]=1;\n    for(int i=1;i&lt;=n;i++){\n        long long c;\n        cin&gt;&gt;c;\n        for(int j=1;j&lt;=m;j++){\n            if(j&gt;=c) dp[j]+=dp[j-c];\n        }\n    }\n\n    cout&lt;&lt;dp[m];\n}\n</code></pre> <p>Exercise (Simplifying Monetary System)</p> <p>Define two monetary systems \\( A \\) and \\( B \\) as equivalent if every value that can be formed using the currency denominations in \\( A \\) can also be formed using the denominations in \\( B \\), and every value that cannot be formed using the denominations in \\( A \\) also cannot be formed using the denominations in \\( B \\). Given a monetary system \\( A \\) with \\( n \\) different denominations, determine the minimum number of denominations \\( m \\) for a system \\( B \\) such that \\( A \\) is equivalent to \\( B \\).</p> Solution <p>Consider the sorted denominations \\(a_1, a_2, \\cdots, a_n\\). Note that we must not pick denominations less than \\(a_1\\) because they are not representable in system \\(A\\). We must pick \\(a_1\\). For \\(a_2\\), if it can be formed by \\(a_1\\), then we should not pick it; otherwise, we must pick it or we cannot form \\(a_2\\). Simiarly, when we want to determine whether to pick \\(a_i\\), we want to know if \\(a_i\\) can be formed by \\(a_1, a_2, \\cdots, a_{i-1}\\). This can be solved using the method similar to \"Counting in UKP.\" The only difference is, instead of adding up \\(dp[j]\\), we now seek maximum.</p> <pre><code>#include&lt;bits/stdc++.h&gt;\nusing namespace std;\nint a[25020], dp[25020];\nint main(){\n    int n,ans;\n    cin&gt;&gt;n;\n    for(int i=1;i&lt;=n;i++)\n        cin&gt;&gt;a[i];\n    sort(a+1,a+n+1);\n    memset(dp, 0, sizeof(dp));\n    dp[0] = 1;\n    ans = 0;\n    for(int i=1;i&lt;=n;i++){\n        if(dp[a[i]]==1) continue;\n        ans++;\n        for(int j=a[i];j&lt;=25000;j++){\n            dp[j] = max(dp[j], dp[j-a[i]]);\n        }\n    }\n    cout&lt;&lt;ans&lt;&lt;endl;\n}\n</code></pre>"},{"location":"dp/knapsack/#bounded-knapsack-bkp","title":"Bounded Knapsack (BKP)","text":"<p>Model (Bounded Knapsack)</p> <p>Given \\(N\\) items. \\(W_i\\) denotes the weight/volume/size of the ith item. \\(V_i\\) denotes the value of the ith item. Now, we have a knapsack with capacity \\(M\\). Want to know the maximum value when putting items into the knapsack without exceeding the capacity. The ith item has \\(C_i\\) copies.  </p> <p>We can solve this by using 0-1 Knapsack, which would take \\(\\mathcal{O}(M * \\sum^{N}_{i=1}C_i)\\).</p> <p>Strategy (Binary Splitting)</p> <p>Let \\(m \\in \\mathbb{N}\\). Define \\(p\\) to be the maximum integer s.t\u00a0 \\(2^0 + 2^1 + 2^2 + \\cdots + 2^p \\le m\\). Define \\(R\\) to be the difference between \\(2^0 + 2^1 + 2^2 + \\cdots + 2^p\\) and \\(m\\), i.e. </p> \\[R = m - (2^0 + 2^1 + \\cdots + 2^p) = m - 2^{p+1} + 1\\] <p>Define set \\(B = \\{2^x \\mid x \\in \\mathbb{N} \\land x \\le p\\} \\cup \\{R\\}\\). Then we have a surjection:</p> \\[\\begin{align*} \\{s \\in \\mathbb{N} \\mid s \\; \\text{is the sum of some subset of B} \\} \\twoheadrightarrow \\{x \\in \\mathbb{N} \\mid x \\le m\\} \\end{align*}\\] <p>Note that \\(|B| = p + 2\\). For Bounded Knapsack problems, we can use Binary Splitting to divide \\(C_i\\) ith items into at most \\(p+2\\) items. Their weights are \u00a0 </p> \\[2^0 * V_i, 2^1 * V_i, \\cdots, 2^p * V_i, R * V_i\\] <p>The time complexity is reduced from \\(\\mathcal{O}(M * \\sum^{N}_{i=1}C_i)\\) to \\(\\mathcal{O}(M * \\sum^{N}_{i=1}\\log{C_i})\\).</p>"},{"location":"dp/knapsack/#multiple-choice-knapsack-problem-mckp","title":"Multiple-Choice Knapsack Problem (MCKP)","text":"<p>Model (Multiple-Choice Knapsack)</p> <p>Given \\(N\\) categories of items. The ith category has \\(C_i\\) different items. \\(W_{ij}\\) denotes the weight/volume/size of the jth item in the ith category. \\(V_{ij}\\) denotes the value of the jth item in the ith category. Now, we have a knapsack with capacity \\(M\\). Want to know the maximum value when putting items into the knapsack without exceeding the capacity. You can choose at most one item from a particular category.  </p> <p>Define \\(F[i,j]\\) be the maximal value for the first \\(i\\) categories with capacity \\(j\\). The Bellman equation is:</p> \\[\\begin{align*} F[i,j] = \\max  \\begin{cases}     F[i-1,j] \\\\     \\underset{1\\le k \\le C_i}{\\max} \\{F[i-1, j-V_{ik}] + W_{ik}\\} \\\\ \\end{cases} \\end{align*}\\] <p>Similar to 0-1 Knapsack, we can ignore the first dimension. The bottom-up implementation then would be:</p> <pre><code>memset(f, 0xcf, sizeof(f));\nf[0] = 0;\nfor (int i=1;i&lt;=n;i++)\n    for(int j=m;j&gt;=0;j--)\n        for(int k=1;k&lt;=c[i];k++)\n            if(j&gt;=w[i][k])\n                dp[j] = max(dp[j], dp[j-w[i][k]] + w[i][k]);    \n</code></pre> <p>Please be aware of the order of loops. Multiple-Choice Knapsack Problem is the foundation of many Tree DP problems. We'll discuss them in later chapters.</p>"},{"location":"dp/linear/","title":"Linear DP","text":""},{"location":"dp/tree/","title":"Tree DP","text":""},{"location":"ds/basics/","title":"Basic Data Structure","text":""},{"location":"ds/basics/#stack","title":"Stack","text":"<p>Strategy (Implementing Stack)</p> <p>For convenience, we can implement a stack using an array:</p> <pre><code>int s[STACK_SIZE]; // using array as a stack\nint p=0; // position\n\nvoid push(int x){\n    s[++p] = x;\n}\n\nint pop(){\n    if(p==0) cout&lt;&lt;\"The stack is empty!\";\n    return s[p--];\n}\n</code></pre> <p>Strategy (Opposite Stacks)</p> <p>Putting two stacks on the opposite direction to simulate Editor-like effect.</p> <p>Exercise (In/out Stack)</p> <p>Given \\(n\\) integers \\(1, 2, \\cdots, n\\), we want every number to be pushed into an infinite stack once and then popped once. If the in-stack order is \\(1, 2, \\cdots, n\\), then how many possible out-stack orders are there?</p> Solution 1 (Iteration) <p>Consider the position of \\(1\\) in the out-stack order. If \\(1\\) is on position \\(k\\), then the process is: \u00a0\u00a0\u00a0\u00a0 1. push \\(1\\) into the stack \u00a0\u00a0\u00a0\u00a0 2. push \\(2 ~ k\\) into the stack and then pop them in some unknown order \u00a0\u00a0\u00a0\u00a0 3. pop \\(1\\) \u00a0\u00a0\u00a0\u00a0 4. push \\(k+1 ~ n\\) and then pop them in some unknown order Then we get the iterative formula:</p> \\[\\begin{align*} S_n = \\sum^{n}_{k=1}{S_{k-1} * S_{n-k}} \\end{align*}\\] <p>The time complexity is \\(\\mathcal{O}(n^2)\\).</p> Solution 2 (DP) <p>Define \\(f[i,j]\\) be the number of possibility that there are \\(i\\) elements not pushed and \\(j\\) elements left in the stack. Under any situation, there are two choices: push an element or pop an element. The Bellman Equation is:</p> \\[\\begin{align*} f[i,j] = f[i-1,j+1] + f[i,j-1] \\end{align*}\\] <p>The time complexity is \\(\\mathcal{O}(n^2)\\).</p> Solution 3 (Math) <p>This question is equivalent to calculating the nth Catalan number, i.e. </p> \\[\\begin{align*} \\frac{C^n_{2n}}{n+1} \\end{align*}\\] <p>The time complexity is \\(\\mathcal{O}(n)\\).</p> <p>Definition (Infix, Prefix, Postfix Notation)</p> <p>Infix Notation: operators are in-between every pair of operands. e.g. \\(3 * (1 - 2)\\) Prefix Notation (Polish Notation): operators are before two expressions. e.g. \\(* \\; 3 - 1 2\\) Postfix Notation (Reverse Polish Notation): operators are after two expressions. e.g. \\(1 2 - 3 \\; *\\)</p> <p>Important Note (Computing Postfix Expression)</p> <p>Use a stack. Scan the expression following these steps: \u00a0\u00a0\u00a0\u00a0 1. if encounter a number, push it into the stack \u00a0\u00a0\u00a0\u00a0 2. if encounter an operator, pop out two elements from the stack, calculate and push the result into the   stack  The time complexity is \\(\\mathcal{O}(n)\\).</p> <p>Important Note (Computing Infix Expression)</p> <p>The fastest way to compute an infix expression is to first turn it into a postfix expression. We can do this following these steps: \u00a0\u00a0\u00a0\u00a0 1. if encounter a number, output it \u00a0\u00a0\u00a0\u00a0 2. if encounter a <code>(</code>, push it into the stack \u00a0\u00a0\u00a0\u00a0 3. if encounter a <code>)</code>, pop and output elements until we popped a <code>(</code> \u00a0\u00a0\u00a0\u00a0 4. if encounter an operator, pop and output elements until the priority of the operator &gt; the element we intend to pop, then push the operator into the stack; The priority ranking is: <code>*/</code> &gt; <code>+-</code> &gt; <code>(</code>.   After scanning all elements in the expression, we pop all the elements from the stack.</p> <p>The key idea here is to use stack to \"wait\" for <code>)</code> if encounter a <code>(</code>, or another number if encouter an operator.</p>"},{"location":"ds/basics/#monotonic-stack","title":"Monotonic Stack","text":"<p>Exercise (Largest Rectangle in a Histogram)</p> <p>Find the maximum area of the rectangle that can be outlined in a histogram.</p> <p> </p> Solution <p>Consider if the each small rectangle in the histogram is increasing height, then we can simply enumerate heights and ignore all the rectangle on the left for a certain height. When a shorter rectangle comes, we can first view all previous rectangles as increasing height histogram, and then ignore the heights that are larger than the new shorter rectangle. In other word, we are maintaining a Monotonic Stack. To implement this, we add a rectangle of height 0 at the end to activate final pop.</p> <pre><code>a[n+1] = 0; // adding a 0 height rectangle at the end of the histogram\nposition = 0; \nfor(int i=1;i&lt;=n;i++){\n    if(a[i] &gt; stack[position]){\n        // push a rectangle into the stack if the stack is monotonic\n        stack[++p] = a[i]; \n\n        // the width of small rectangle can vary as \n        // it might be the combination of multiple rectangle\n        width[position] = 1; \n    }else{\n        int width_ = 0;\n        while(stack[position] &gt; a[i]){\n            // accumulate width while popping\n            width_ += width[position]; \n\n            // calculate the area if using rectangle 'position' as height\n            ans = max(ans, width_ * stack[position]); \n            p--; // pop\n        }\n\n        // push the new rectangle\n        stack[++p] = a[i]; \n\n        // the new rectangle has the accumulated width + 1\n        width[position] = width_ + 1; \n    }\n}\n</code></pre> <p>This is the famous monotonic stack, with time complexity \\(\\mathcal{O}(N)\\). The key idea here is to erase impossible choices and maintain the set to be effective and in order.</p>"},{"location":"ds/basics/#queue","title":"Queue","text":"<p>Strategy (Implementing Queue)</p> <p>For convenience, we can implement a queue using an array. Actually, this is a deque. Note that we cannot <code>push_front</code> using array. Better try STL <code>deque</code>.</p> <pre><code>int q[QUEUE_SIZE]; // using array as a queue\nint l = 1, r = 0; // left index and right index of the queue\n\nvoid push_end(int x){\n    q[++r] = x;\n}\n\nint pop_front(){\n    if(l &gt; r) cout&lt;&lt;\"The stack is empty!\";\n    return q[l++];\n}\n\nint pop_end(){\n    if(l &gt; r) cout&lt;&lt;\"The stack is empty!\";\n    return q[r--];\n}\n</code></pre>"},{"location":"ds/basics/#monotonic-queue","title":"Monotonic Queue","text":"<p>Exercise (Maximal Interval Sum)</p> <p>Given a sequence of \\(n\\) integers, find a consecutive interval within length \\(m\\) (at least one) that has the maximal sum. Note that there may be negative numbers. </p> Solution <p>We use prefix sum to calculate interval sum. Then, we enumerate right endpoint, which is equivalent to sliding a window of length \\(m\\). Now the tricky part is how to quickly find the left endpoint in that window.   Given a right endpoint \\(i\\), consider two position \\(j\\) and \\(k\\): if \\(k&lt;j&lt;i\\) and \\(S[k] \\ge S[j]\\), then for all right endpoints on the right side of i, k will never be the solution. This is because \\(k\\) is farther away from \\(i\\) and \\(k\\) is worse than \\(k\\), so \\(k\\) will no longer be a choice for all right endpoitns on the right side of \\(i\\). We can simply maintain a monotonic queue to keep record of current possible choices for left endpoints. We use a deque to store positions, and array <code>s</code> to store prefix sum. Detailed steps are: \u00a0\u00a0\u00a0\u00a0 1. if the deque stores elements more than \\(m\\) away from \\(i\\), pop from the front \u00a0\u00a0\u00a0\u00a0 2. update answer, <code>s[i] - s[q[l]]</code> is the maximal sum when <code>i</code> is the right endpoint \u00a0\u00a0\u00a0\u00a0 3. pop the elements from the end until the end is less than <code>s[i]</code>(maintaining monotonic queue) \u00a0\u00a0\u00a0\u00a0 4. push <code>i</code> </p> <p> </p> <pre><code>q[l] = 0, r = 1; // save choice j = 0\nfor(int i=1;i&lt;=n;i++){\n    // clean the outdated choices when we consider i as the right endpoint\n    while(l&lt;=r&amp;&amp;q[l]&lt;i-m) l++; \n\n    // compute the maximum at right endpoint i\n    ans = max(ans, s[i] - s[q[l]]); \n\n    // when i come in, get rid of all useless choices \n    while(l&lt;=r&amp;&amp;s[q[r]]&gt;=s[i]) r--; \n\n    // i come in\n    q[++r] = i; \n}\n</code></pre> <p>The order of these operations? The logistic is: \u00a0\u00a0\u00a0\u00a0 1. When we settle a new i, before we update the answer, we must first clean the outdated choices \u00a0\u00a0\u00a0\u00a0 2. We can update our data because the queue is maintained to be monotonic \u00a0\u00a0\u00a0\u00a0 3. After we put i into the queue, we must maintain a monotonic queue</p> <p>Note that there should be at least one element in the queue, or we cannot ensure there is at least one element in the queue. Thus, it is vital important to put <code>q[++r] = i;</code> at the end of the loop. The right endpoint we are considering at the begining of the loop is \\(i\\), and notice that \\(i-1\\) will always be in the queue as it is push into the queue at last without popping, which guaranteed that we have at least one element in the interval (in the queue).</p>"},{"location":"ds/basics/#linked-list","title":"Linked List","text":"<p>Strategy (Implementing Linked List)</p> <pre><code>struct Node {\n    int value;\n    Node *prev,  *next;\n};\nNode *head, *tail;\n\nvoid init(){\n    head = new Node();\n    tail = new Node();\n    head-&gt;next = tail;\n    tail-&gt;prev = head;\n}\n\nvoid insert(Node *p, int val){\n    q = new Node();\n    q-&gt;value = val;\n    p-&gt;next-&gt;prev = q;\n    q-&gt;next = p-&gt;next;\n    p-&gt;next = q;\n    q-&gt;prev = p;\n}\n\nvoid remove(Node *p){\n    p-&gt;prev-&gt;next = p-&gt;next;\n    p-&gt;next-&gt;prev = p-&gt;prev;\n    delete p;\n}\n\nvoid recycle(){\n    while(head!=tail){\n        head = head-&gt;next;\n        delete head-&gt;prev;\n    }\n    delete tail;\n}\n</code></pre> <p>Note (Advantage of Linked List)</p> <p>The key advantage of linked list is that it allows us to insert or remove elements between elements in \\(\\mathcal{O}(1)\\).</p> <p>Exercise (Neighbor Value Search)</p> <p>Given a sequence \\(A\\) of length \\(n\\), for each element in the sequence \\(A_i\\), find:</p> \\[\\begin{align*} \\min_{1\\le j&lt;i}\\left| A_i - A_j\\right| \\end{align*}\\] <p>and corresponding \\(j\\). If multiple minimum exist, pick smaller \\(A_j\\).</p> Solution <p>Note that the answer for \\(A_i\\) only depends on \\(A_1, \\cdots, A_{i-1}\\), so we can process the sequence from the back to the front, deleting already processed elements using linked list. More precisely, sort \\(A\\) and construct a linked list for the sorted elements. Use array <code>B</code> to record the index of the original sequence, i.e., \\(B_i\\) (a pointer) points to the element \\(x\\) in the linked list, where \\(x\\) is indexed \\(i\\) in \\(A\\). Then, for each \\(A_i\\), the <code>prev</code> and <code>next</code> in the linked list are the closest elements we need to compare. We process \\(B_n\\), and then delete the element \\(B_n\\) points to; and then we process \\(B_{n-1}\\), delete the element \\(B_{n-1}\\) points to, and so on...</p> <p>Exercise (Running Median)</p> <p>See ...</p>"},{"location":"ds/basics/#adjacency-list","title":"Adjacency List","text":"<p>It is said that adjacency list and forward star representation are different in a way that adjacency list uses linked list and forward star uses array, but they share the same idea. In OI, I'll just use forward star.</p> <p>Definition (Forward Star)</p> <pre><code>// adding an edge from vertex x to vertex y, (x,y), with weight z\nvoid add(int x, int y, int z){\n    ver[++tot] = y, edge[tot] = z;\n    next[tot] = head[x], head[x] = tot;\n}\n\n// search all edges that start from vertex x \nfor(int i=head[x]; i; i=next[i]){\n    int y = ver[i], z = edge[i];\n    // an edge (x,y) with weight z\n}\n</code></pre>"},{"location":"math/numbers/","title":"BASIC NUMBER THEORY","text":""},{"location":"math/numbers/#12-properties","title":"12 PROPERTIES","text":"<p>DEFINITION (12 properties of numbers)</p> <p>We should take a close look at the basic properties of numbers and properly define our mathematical language before we can do any analysis on these numbers. Here I will list the 12 basic properties of numbers, and talk about the implications of these properties later. Each letter (\\(a\\), \\(b\\), \\(c\\), etc.) denotes any number: (P1) \\(a + (b + c) = (a + b) + c\\) (P2) \\(a + 0 = 0 + a = a\\) (P3) \\(a + (-a) = (-a) + a = 0\\) (P4) \\(a + b = b + a\\) (P5) \\(a \\cdot (b \\cdot c) = (a \\cdot b) \\cdot c\\) (P6) \\(a \\cdot 1 = 1 \\cdot a = a, 1 \\not= 0\\) (P7) \\(a \\cdot a^{-1} = a^{-1} \\cdot a = 1\\), for \\(a \\not=0\\) (P8) \\(a \\cdot b = b \\cdot a\\) (P9) \\(a \\cdot (b + c) = a \\cdot b + a \\cdot c\\) For the following properties, \\(P\\) is the collection of all positive numbers (P10) \\(\\forall a\\), one and only one of the following holds:</p> \\[ \\qquad\\begin{aligned} \\text{(i)}\\qquad&amp;a = 0\\text{,}\\\\ \\qquad \\text{(ii)}\\qquad&amp;a\\text{ is in the collection }P\\text{,}\\\\ \\qquad \\text{(iii)}\\qquad&amp;-a\\text{ is in the collection }P\\end{aligned} \\] <p>(P11) If \\(a\\) and \\(b\\) are both in the collection \\(P\\), then \\(a + b\\) is in \\(P\\) (P12) If \\(a\\) and \\(b\\) are both in the collection \\(P\\), then \\(a \\cdot b\\) is in \\(P\\)</p>"},{"location":"math/prime/","title":"Prime","text":""},{"location":"math/prime/#sieve-of-eratosthenes","title":"Sieve of Eratosthenes","text":"<p>Algorithm (Sieve of Eratosthenes)</p> <p>To find all prime numbers up to any given limit \\(N\\), the Sieve of Eratosthenes is an ancient algorithm based on the fact that any multiplication of integer \\(x\\) is not prime. The algorithm is to scan each integer \\(x_i\\) within the limit \\(N\\) starting at \\(2\\) and \"erase\" \\(\\{2x_i, 3x_i, \\cdots, \u230aN/x_i\u230b * x_i\\}\\). Note that when an integer \\(x_j\\) is scanned but not marked, we know \\(x_j\\) is a prime. We can proof this by contradiction easily.</p> <p> Sieve of Eratosthenes <sup>1</sup> </p> <p>Code:</p> <pre><code>void primes(int n){\n    memset(v,0,sizeof(v));\n    for(int i=2;&lt;=n;i++){\n        if(v[i]) continue;\n        cout&lt;&lt;i&lt;&lt;endl;\n        for(int j=i;j&lt;=n/i;j++)\n            v[i*j]=1;\n    }\n}\n</code></pre> <p>The time complexity of Sieve of Eratosthenes is \\(\\mathcal{O}(\\sum_{\\text{prime} \\; p \\le N}{\\frac{N}{p}}) = \\mathcal{O}(N\\log\\log N)\\).</p> <ol> <li> <p>Wikipedia \u21a9</p> </li> </ol>"},{"location":"math/linear_alg/21-241/","title":"21-241 Matrix and Linear Transformation","text":""},{"location":"math/linear_alg/21-241/#linear-independence","title":"Linear Independence","text":"<p>Definition (Linear Independence)</p> <p>A set of vectors \\((v_1, v_2, \\cdots, v_p) \\in \\mathbb{R}^n\\) is linear independent if the vector equation \\(x_1v_1 + x_2v_2 + \\cdots + x_pv_p = 0\\) has only the trivial solution. </p> <p>Definition (Linear Dependence)</p> <p>A set of vectors \\((v_1, v_2, \\cdots, v_p) \\in \\mathbb{R}^n\\) is linear dependent if the vector equation \\(x_1v_1 + x_2v_2 + \\cdots + x_pv_p = 0\\) has non-trivial solution, i.e. \\(\\exists c_1, c_2, \\cdots, c_p \\text{ (not all zero), } c_1v_1 + c_2v_2 + \\cdots + c_pv_p = 0\\). </p> <p>Important Note (Linear Independence of Matrix Columns)</p> <p>The columns of a matrix \\(\\mathbf{A}\\) are linear independent iff the equation \\(\\mathbf{A}x = 0\\) has only the trivial solution.</p> <p>Theorem (1.7.4(a))</p> <p>A set \\(S = \\{v_1, v_2, \\cdots, v_p\\}\\) is linearly dependent iff at least one of the vectors in \\(S\\) is a linear combination of the others.</p> <p>Theorem (1.7.4(b))</p> <p>If a set contains more vectors than entries in each vector, then the set is linearly dependent, i.e. </p> \\[\\begin{align*} \\{v_1, \\cdots, v_p\\} \\in \\mathbb{R}^n \\text{ is linearly dependent if } p &gt; n \\end{align*}\\] Proof <p>If \\(p &gt; n\\) then there must be a free variable in the equation \\([v_1, v_2, \\cdots, v_p] \\cdot \\mathbf{x} = 0\\). Therefore, non-trivial solution must exist.</p> <p>Theorem (1.7.5)</p> <p>If a set \\(S = \\{v_1, v_2, \\cdots, v_p\\} \\in \\mathbb{R}^n\\) contains the zero vector, then \\(S\\) is linearly dependent.</p> Proof <p>If \\(v'=0\\), then \\(v_1 \\cdot 0 + \\cdots v' \\cdot 1 + \\cdots v_p \\cdot 0 = 0\\).</p>"},{"location":"math/linear_alg/21-241/#introduction-to-linear-transformation","title":"Introduction to Linear Transformation","text":"<p>Definition (Transformation)</p> <p>A transformation (or function) \\(\\mathcal{T}\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^m\\) is a rule that assigns each vector \\(\\mathbf{x}\\in \\mathbb{R}^n\\) a vector \\(\\mathcal{T}(\\mathbf{x})\\in \\mathbb{R}^m\\), where \\(\\mathbb{R}^n\\) is called the domain of \\(\\mathcal{T}\\) and \\(\\mathbb{R}^m\\) is called the codomain of \\(\\mathcal{T}\\).</p> <p>Important Note (Matrix Transform)</p> <p>For each matrix \\(A\\) of shape \\(m\\times n\\), we can define a transform from \\(\\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) as follows: </p> \\[\\begin{align*} \\mathcal{T}(x) = Ax \\end{align*}\\] <p>Definition (Linear Transformation)</p> <p>A transformation \\(\\mathcal{T}\\) is linear if: </p> <p>\u00a0\u00a0\u00a0\u00a0(i) \\(\\mathcal{T}(u+v) = \\mathcal{T}(u) + \\mathcal{T}(v)\\) for all \\(u,v \\in \\text{ domain of } \\mathcal{T}\\)</p> <p>\u00a0\u00a0\u00a0\u00a0(ii) \\(\\mathcal{T}(cu) = c\\mathcal{T}(u)\\) for all \\(u \\in \\text{domain of } \\mathcal{T}\\) and scalar \\(c\\)</p> <p>Important Note</p> <p>All transformation defined by matrices are linear, i.e. </p> \\[\\begin{align*} \\mathcal{T}(x) &amp;= Ax \\\\ \\mathcal{T}(X+Y) &amp;= A(X+Y) = Ax + Ay = \\mathcal{T}(X) + \\mathcal{T}(Y)\\\\ \\mathcal{T}(cX) &amp;= A(cX) = cAx = c\\mathcal{T}(X)\\\\ \\end{align*}\\] <p>Important Note (Properties of Linear Transformation)</p> <p>If \\(\\mathcal{T}\\) is a linear transformation, then </p> \\[\\begin{align*} \\mathcal{T}(0) &amp;= 0 \\\\ \\mathcal{T}(cu + dv) &amp;= c \\mathcal{T}(u) + d \\mathcal{T}(v) \\\\&amp;\\text{ for all vectors } u,v \\in \\text{ the domain} \\text{ and all scalars } c,d \\\\ \\mathcal{T}(c_1u_1 + c_2u_2 + \\cdots + c_nu_n) &amp;= c_1 \\mathcal{T}(u_1) + c_2 \\mathcal{T}(u_2) + \\cdots + c_n \\mathcal{T}(u_n) \\\\&amp;\\text{ for all vectors } u_1,u_2,\\cdots,u_n \\in \\text{ the domain} \\text{ and all scalars } c_1, c_2, \\cdots, c_n \\\\ \\end{align*}\\] <p>Note (Range v.s. Codomain)</p> <p>Note that the range of \\(\\mathcal{T} \\neq \\text{the codomain of } \\mathcal{T}\\). </p> <p>Theorem (1.9.1)</p> <p>Let \\(\\mathcal{T}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation. Then there exists a unique matrix \\(A\\) s.t. \\(\\mathcal{T}(x) = Ax\\) for all \\(x \\in \\mathbb{R}^n\\). In fact, \\(A\\), called the standard matrix for the linear transformation, is a \\(m \\times n\\) matrix whose jth column is the vector \\(\\mathcal{T}(e_j)\\), i.e. </p> \\[\\begin{align*} A = [\\mathcal{T}(e_1), \\mathcal{T}(e_2), \\cdots, \\mathcal{T}(e_n)] \\end{align*}\\] <p>Definition (One-to-one Mapping)</p> <p>A mapping \\(\\mathcal{T}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) is said to be one-to-one if each \\(b \\in \\mathbb{R}^m\\) is the image of at most one \\(x \\in \\mathbb{R}^n\\).</p> <p>Definition (A Mapping ONTO \\(\\mathbb{R}^m\\))</p> <p>A mapping \\(\\mathcal{T}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) is said to be onto \\(\\mathbb{R}^m\\) if each \\(b \\in \\mathbb{R}^m\\) is the image of at least one \\(x \\in \\mathbb{R}^n\\).</p> <p>Theorem (1.9.3)</p> <p>Let \\(\\mathcal{T}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation. \\(\\mathcal{T}\\) is one-to-one iff \\(\\mathcal{T}(x) = 0\\) has only the trivial solution.</p> <p>Theorem (1.9.4)</p> <p>Let \\(\\mathcal{T}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation and let \\(A\\) be the standard matrix. </p> <p>\u00a0\u00a0\u00a0\u00a0(a) \\(\\mathcal{T}\\) maps \\(\\mathbb{R}^n\\) onto \\(\\mathbb{R}^m\\) iff the columns of \\(A\\) span \\(\\mathbb{R}^m\\).</p> <p>\u00a0\u00a0\u00a0\u00a0(b) \\(\\mathcal{T}\\) is one-to-one iff the columns of \\(A\\) are linearly independent.</p> <p>\u00a0\u00a0\u00a0\u00a0(c) If \\(n&gt;m\\), then \\(\\mathcal{T}\\) cannot be one-to-one because the columns of \\(A\\) are linearly dependent.</p> <p>\u00a0\u00a0\u00a0\u00a0(d) If \\(m &gt; n\\), \\(\\mathcal{T}\\) cannot map \\(\\mathbb{R}^n\\) onto \\(\\mathbb{R}^m\\) because the columns of \\(A\\) cannot span \\(\\mathbb{R}^m\\).</p>"},{"location":"math/linear_alg/elimination/","title":"Elimination Matrix","text":""},{"location":"math/linear_alg/elimination/#elimination","title":"Elimination","text":"<p>We shall begin this chapter with a systematic way to solve linear equations with n variables. Consider the following set of equations where n = 3:</p> \\[ \\begin{cases} &amp;x &amp;+ &amp;2&amp;y &amp;+ &amp;z &amp;= &amp;2\\\\ 3&amp;x &amp;+ &amp;8&amp;y &amp;+ &amp;z &amp;= &amp;12\\\\ &amp;&amp;&amp;4&amp;y &amp;+ &amp;z &amp;= &amp;2 \\end{cases} \\] <p>Which would correspond to the coefficient matrix \\(A\\):</p> \\[ A= \\begin{bmatrix} 1&amp;2&amp;1\\\\3&amp;8&amp;1\\\\0&amp;4&amp;1 \\end{bmatrix} \\] <p>A way to systematically solve the equation is called elimination. We begin by eliminating \\(x\\) in the second equation, and from then on eliminate \\(x\\) &amp; \\(y\\) in the third equation.</p> Intermediate Steps \\[ \\begin{aligned} &amp;\\begin{cases} &amp;x &amp;+ &amp;2&amp;y &amp;+ &amp;z &amp;= &amp;2\\\\ 3&amp;x &amp;+ &amp;8&amp;y &amp;+ &amp;z &amp;= &amp;12\\\\ &amp;&amp;&amp;4&amp;y &amp;+ &amp;z &amp;= &amp;2 \\end{cases}\\\\ \\Rightarrow &amp;\\begin{cases} &amp;x &amp;+ &amp;2&amp;y &amp;+ &amp;&amp;z &amp;= &amp;2\\\\ &amp;&amp;&amp;2&amp;y &amp;- &amp;2&amp;z &amp;= &amp;6\\\\ &amp;&amp;&amp;4&amp;y &amp;+ &amp;&amp;z &amp;= &amp;2 \\end{cases}\\\\ \\Rightarrow &amp;\\begin{cases} &amp;x &amp;+ &amp;2&amp;y &amp;+ &amp;&amp;z &amp;= &amp;2\\\\ &amp;&amp;&amp;2&amp;y &amp;- &amp;2&amp;z &amp;= &amp;6\\\\ &amp;&amp;&amp;&amp;&amp;&amp;5&amp;z &amp;= &amp;-10 \\end{cases} \\end{aligned} \\] <p>In this case, we would end up with a matrix like this:</p> \\[ U= \\begin{bmatrix} 1&amp;2&amp;1\\\\0&amp;2&amp;-2\\\\0&amp;0&amp;5 \\end{bmatrix} \\] <p>Such a matrix is called an Upper Triangular Matrix, denoted by \\(U\\). If we want to solve the equation, we can add the right sides of the equations to the matrix as a column on the right side of the matrix to get an Augmented Matrix.From then on we can go over the same process of elimination. But this is not the point for now.</p>"},{"location":"math/linear_alg/elimination/#elimination-matrix_1","title":"Elimination Matrix","text":"<p>If we look back on what we just did, we were essentially doing row operations: subtracting three times the first row from the second row, etc. Now if you remember what we just discovered in the first chapter about row operations, you might think that we might be able to multiply something by the matrix \\(A\\) to get the same effect, namely:</p> \\[ EA=U \\] <p>where \\(E\\) is a matrix we call the Elementary Matrix. Since \\(U\\) and \\(A\\) are three by three matrices, \\(E\\) should be as well. Think about it step by step: first let us leave the third row and get the \\(x\\) in the second row eliminated. The operation here is to subtract three times the first row from the second row, and leave everything else unchanged. Since the first row and third row are not changed, the first and third row of \\(E\\) should be \\(\\begin{bmatrix}1&amp;0&amp;0\\end{bmatrix}\\) and \\(\\begin{bmatrix}0&amp;0&amp;1\\end{bmatrix}\\) respectively. Since we want to subtract three times the first row from the second row of \\(A\\), the second row of \\(E\\) would be: \\(\\begin{bmatrix}-3&amp;1&amp;0\\end{bmatrix}\\). The resulting matrix \\(E\\) is then:</p> \\[ E_{21}= \\begin{bmatrix} 1&amp;0&amp;0\\\\-3&amp;1&amp;0\\\\0&amp;0&amp;1 \\end{bmatrix} \\] Identity Matrix <p>As you may have discovered, we can also multiply something by the matrix \\(A\\) for it to remain unchanged. Such a matrix is called the Identity Matrix \\(I\\). It takes the following form:</p> \\[ I= \\begin{bmatrix} 1&amp;0&amp;0&amp;\\cdots&amp;0\\\\ 0&amp;1&amp;0&amp;\\cdots&amp;0\\\\ 0&amp;0&amp;1&amp;\\cdots&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ 0&amp;0&amp;0&amp;\\cdots&amp;1 \\end{bmatrix} \\] <p>And it satisfies:</p> \\[ \\forall A, AI = IA = A \\] <p>given that \\(A\\) is a square matrix.</p> <p>Through a similar process, we can get the elementary matrix for the next step:</p> \\[ E_{32} =  \\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;-2&amp;1 \\end{bmatrix} \\] <p>Such that</p> \\[ U = E_{32} (E_{21}A) \\] <p>A very important fact is that matrix multiplication follows the Assiciative Law, and we can rewrite the equation as:</p> \\[ \\begin{aligned} U &amp;= (E_{32}E_{21}A)\\\\ &amp;= E_{31}A \\end{aligned} \\] <p>But it's also important to know that switching orders is not valid in most matrix multiplications:</p> \\[ AB \\not = BA \\] <p>in most cases.</p> Permutation Matrix <p>Of course, you can also switch the rows of a matrix by multiplication, for example:</p> \\[ \\begin{bmatrix} 0&amp;1\\\\ 1&amp;0 \\end{bmatrix} \\begin{bmatrix} a&amp;b\\\\ c&amp;d \\end{bmatrix} =\\begin{bmatrix} c&amp;d\\\\ a&amp;b \\end{bmatrix} \\] <p>Notably,</p> \\[ \\begin{bmatrix} a&amp;b\\\\ c&amp;d \\end{bmatrix} \\begin{bmatrix} 0&amp;1\\\\ 1&amp;0 \\end{bmatrix} =\\begin{bmatrix} b&amp;a\\\\ d&amp;c \\end{bmatrix} \\]"},{"location":"math/linear_alg/intro/","title":"Starting Point of Linear Algebra","text":""},{"location":"math/linear_alg/intro/#geometries-of-equations","title":"Geometries of Equations","text":"<p>Essentially, Linear Algebra starts with solving n equations with m variables. To make things easy, we make n equal to m. For example:</p> \\[ \\begin{cases} 2x - y = 0\\\\ -x + 2y = 3 \\end{cases} \\] <p>In the language of Linear Algebra would be:</p> \\[ \\begin{bmatrix} 2&amp;-1\\\\ -1&amp;2 \\end{bmatrix} \\begin{bmatrix} x\\\\ y \\end{bmatrix}= \\begin{bmatrix} 0\\\\ 3 \\end{bmatrix} \\] <p>Where each of the numbers in the matrix represent a coefficient on the left hand side, and \\(x\\) &amp; \\(y\\) represent the variables.</p>"},{"location":"math/linear_alg/intro/#row-picture-and-column-picture","title":"Row Picture and Column Picture","text":"<p>If you carefully think about it, there are two ways you can represent the equation with a picture: First, you can think about it as two lines intersecting each other, with \\(2x - y = 0\\) being the first one, and \\(-x + 2y = 3\\) being the second one:</p> Generated Using Geogebra <p>Or you can think about it as a Vector Addition problem:</p> \\[ x\\begin{bmatrix}2\\\\-1\\end{bmatrix} + y \\begin{bmatrix}-1\\\\2\\end{bmatrix} =  \\begin{bmatrix}0\\\\3\\end{bmatrix} \\] <p>Which means that it would become a Linear Combination  of the two columns of the coefficient matrix.</p> Generated Using Geogebra"},{"location":"math/linear_alg/intro/#matrix-multiplication","title":"Matrix Multiplication","text":"<p>We can think about vectors as matrices with one of the dimensions being 1. The following way would be an easy introduction to Matrix Multiplication:</p> \\[ \\begin{aligned} &amp;\\begin{bmatrix}1\\\\2\\end{bmatrix}\\\\ \\begin{bmatrix}2&amp;-1\\\\-1&amp;2\\end{bmatrix} &amp;\\begin{bmatrix}a_1\\\\a_2\\end{bmatrix} \\end{aligned} \\] <p>where the vector \\(\\begin{bmatrix}a_1\\\\a_2\\end{bmatrix}\\) is the product, and \\(a_1\\) and \\(a_2\\) would be the \"inner product\" of the row on their left and the column on their top. Therefore:</p> \\[ \\begin{cases} a_1 = \\begin{bmatrix}2&amp;-1\\end{bmatrix}\\begin{bmatrix}1\\\\2\\end{bmatrix}=2\\cdot1 + 2\\cdot(-1) = 0\\\\ a_2 = \\begin{bmatrix}-1&amp;2\\end{bmatrix}\\begin{bmatrix}1\\\\2\\end{bmatrix}=-1\\cdot1 + 2\\cdot2 = 3 \\end{cases} \\] <p>But in fact, there is another more essential way to think about matrix multiplications.</p> \\[ \\begin{bmatrix}2&amp;-1\\\\-1&amp;2\\end{bmatrix}\\begin{bmatrix}1\\\\2\\end{bmatrix} \\] <p>can be interpreted as a Linear Combination of the Columns of the matrix:</p> \\[ \\begin{bmatrix}2&amp;-1\\\\-1&amp;2\\end{bmatrix}\\begin{bmatrix}1\\\\2\\end{bmatrix}= 1\\begin{bmatrix}2\\\\-1\\end{bmatrix}+2\\begin{bmatrix}-1\\\\2\\end{bmatrix}=\\begin{bmatrix}0\\\\3\\end{bmatrix} \\] <p>If you think about this further, you will find that if we multiply a row vector by the matrix, we would essentially get a linear combination of the Rows of the matrix:</p> \\[ \\begin{aligned} \\begin{bmatrix}1&amp;2\\end{bmatrix}\\begin{bmatrix}2&amp;-1\\\\-1&amp;2\\end{bmatrix}&amp;=1\\begin{bmatrix}2&amp;-1\\end{bmatrix}+2\\begin{bmatrix}-1&amp;2\\end{bmatrix} \\\\&amp;=\\begin{bmatrix}0&amp;3\\end{bmatrix} \\end{aligned} \\] <p>One observation is that if we multiply the matrix by something we can manipulate the columns of the matrix, and if we multiply something by the matrix we can manipulate the rows of the matrix. Such interpretations may seem trivial and cumbersome for now, but in the next chapter we shall see they come into use.</p>"},{"location":"math/linear_alg/inverses/","title":"Inverses","text":""},{"location":"math/linear_alg/inverses/#inverses_1","title":"Inverses","text":"<p>Df</p> <p>An Inverse of the matrix \\(A\\) is \\(A^{-1}\\) such that \\(A^{-1}A=I\\) where \\(I\\) is the Identity Matrix. Note that \\(AA^{-1}=I\\) also holds Inverses do not exist when there is a non-trivial vector \\(\\vec x\\) such that \\(A\\vec x=\\vec0\\)</p> proof <p>Suppose \\(A^{-1}\\) exists in this case. Then \\(A^{-1}A\\vec x=A^{-1}\\vec 0\\) \\(\\Rightarrow I\\vec x = \\vec 0\\) \\(\\Rightarrow \\vec x = \\vec 0\\)</p>"},{"location":"math/linear_alg/inverses/#finding-inverses","title":"Finding Inverses","text":"<p>Gause-Jordan Method</p> <p>Consider a matrix \\(A\\):</p> \\[ A= \\begin{bmatrix} 1&amp;3\\\\ 2&amp;7 \\end{bmatrix} \\] <p>and we want to find \\(A^{-1}\\) such that \\(A^{-1}A=I\\). Suppose that \\(A^{-1}=\\begin{bmatrix}a&amp;b\\\\c&amp;d\\end{bmatrix}\\), then it is actually two equation sets:</p> \\[ \\begin{cases} \\begin{bmatrix}a&amp;b\\end{bmatrix} \\begin{bmatrix}1&amp;3\\\\2&amp;7\\end{bmatrix} =\\begin{bmatrix}1&amp;0\\end{bmatrix}\\\\ \\begin{bmatrix}c&amp;d\\end{bmatrix} \\begin{bmatrix}1&amp;3\\\\2&amp;7\\end{bmatrix} =\\begin{bmatrix}0&amp;1\\end{bmatrix} \\end{cases} \\] <p>One way to find \\(A^{-1}\\) is the Gause-Jordan method, and it is achieved first by adding the Identity Matrix to the right of \\(A\\) to form an Augmented Matrix \\(\\begin{bmatrix}1&amp;3&amp;1&amp;0\\\\2&amp;7&amp;0&amp;1\\end{bmatrix}\\). Then we perform elimination until the left side of the matrix is the Identity Matrix: \\(\\begin{bmatrix}1&amp;0&amp;7&amp;-3\\\\0&amp;1&amp;-2&amp;1\\end{bmatrix}\\), and the right side of the matrix, \\(\\begin{bmatrix}7&amp;-3\\\\-2&amp;1\\end{bmatrix}\\) is \\(A^{-1}\\). You can verify for yourself.</p> Proof that this is valid <p>Essentially we can rewrite the elimination process with a Elimination matrix, \\(E\\): what we are doing is essentially \\(E\\begin{bmatrix}A&amp;I\\end{bmatrix}\\). Since the left side of the resulting product is \\(I\\), we know that \\(EA=I\\), and so \\(E=A^{-1}\\). Since the right side of the original augmented matrix is \\(I\\), the resulting right side is \\(EI=E=A^{-1}\\).</p>"},{"location":"math/linear_alg/matmul/","title":"Matrix Multiplication","text":""},{"location":"math/linear_alg/matmul/#some-clarifications","title":"Some Clarifications","text":"<p>Just some minor problems, but I thought I'd put it clear here. When we talk about matrices, \\(a_{41}\\) is the first element in the forth row, and an \\(m \\times n\\) matrix is a matrix of \\(m\\) rows and \\(n\\) columns.</p>"},{"location":"math/linear_alg/matmul/#different-ways-to-do-multiplication","title":"Different Ways to Do Multiplication","text":"<p>Apart from the inner product method (or we can call it the \"row time column\" method) in the first chapter, there are some other ways. Consider:</p> \\[ A= \\begin{bmatrix} 1&amp;3\\\\2&amp;4\\\\3&amp;0 \\end{bmatrix} \\begin{bmatrix} 3&amp;2\\\\3&amp;1 \\end{bmatrix} \\] <p>Column</p> \\[ \\begin{aligned} A&amp;= \\begin{bmatrix} \\begin{bmatrix} 1&amp;3\\\\2&amp;4\\\\3&amp;0 \\end{bmatrix} \\begin{bmatrix} 3\\\\3 \\end{bmatrix} &amp;\\begin{bmatrix} 1&amp;3\\\\2&amp;4\\\\3&amp;0 \\end{bmatrix} \\begin{bmatrix} 2\\\\1 \\end{bmatrix} \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} 10&amp;5\\\\18&amp;8\\\\9&amp;6 \\end{bmatrix} \\end{aligned} \\] <p>Which implicates that each column of the product is a combination of the columns of the matrix \\(\\begin{bmatrix}1&amp;3\\\\2&amp;4\\\\3&amp;0\\end{bmatrix}\\).</p> <p>Row</p> \\[ \\begin{aligned} A&amp;= \\begin{bmatrix} \\begin{bmatrix} 1&amp;3 \\end{bmatrix} \\begin{bmatrix} 3&amp;2\\\\3&amp;1 \\end{bmatrix} \\\\\\begin{bmatrix} 2&amp;4 \\end{bmatrix} \\begin{bmatrix} 3&amp;2\\\\3&amp;1 \\end{bmatrix} \\\\\\begin{bmatrix} 3&amp;0 \\end{bmatrix} \\begin{bmatrix} 3&amp;2\\\\3&amp;1 \\end{bmatrix} \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} 10&amp;5\\\\18&amp;8\\\\9&amp;6 \\end{bmatrix} \\end{aligned} \\] <p>Column times Row</p> \\[ \\begin{aligned} A&amp;= \\begin{bmatrix} 1\\\\2\\\\3 \\end{bmatrix} \\begin{bmatrix} 3&amp;2 \\end{bmatrix} +\\begin{bmatrix} 3\\\\4\\\\0 \\end{bmatrix} \\begin{bmatrix} 3&amp;0 \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} 10&amp;5\\\\18&amp;8\\\\9&amp;6 \\end{bmatrix} \\end{aligned} \\] <p>Chunk</p> <p>For this one, suppose two big matrices \\(A\\) and \\(B\\) that can be divided in to four chunks:</p> \\[ \\begin{cases} A=\\begin{bmatrix} A_1&amp;A_2\\\\A_3&amp;A_4 \\end{bmatrix}\\\\ B=\\begin{bmatrix} B_1&amp;B_2\\\\B_3&amp;B_4 \\end{bmatrix} \\end{cases} \\] <p>where \\(A_1\\), \\(A_2\\), \\(A_3\\), \\(A_4\\), \\(B_1\\), \\(B_2\\), \\(B_3\\), \\(B_4\\) are all matrices. Suppose they have the right dimensions. Their product is:</p> \\[ AB=\\begin{bmatrix} A_1B_1+A_2B_3&amp;A_1B_2+A_2B_4\\\\ A_3B_1+A_4B_3&amp;A_3B_2+A_4B_4 \\end{bmatrix} \\]"},{"location":"probability/discrete%20random%20variables/","title":"Discrete Random Variables","text":"<p>Definition (Discrete Random Variables)</p> <p>A discrete random variable \\(X\\) on the probability space \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) is defined to be a mapping \\(X: \\Omega \\rightarrow \\mathbb{R}\\) such that </p> \\[ \\begin{align*}     &amp;\\text{the image } X(\\Omega) \\text{ is a countable subset of } \\mathbb{R} \\tag{2.1} \\\\     &amp;\\{\\omega \\in \\Omega : X(\\omega) = x\\} \\in \\mathcal{F} \\text{ for } x \\in \\mathbb{R} \\tag{2.2}  \\end{align*} \\] <p>Note (Interpretation of (2.2))</p> <p>An intuitive Interpretation of (2.2) is: the set of all samples \\(\\omega\\) that make \\(X(\\omega) = x\\) must be in event space \\(\\mathcal{F}\\).</p> <p>Definition (Probability Mass Function (pmf))</p> <p>The probability mass function (pmf) of the discrete random variable \\(X\\) is the function \\( p_X : \\mathbb{R} \\to [0, 1] \\) defined by </p> \\[\\begin{align*} p_X(x) = \\mathbb{P}(X = x). \\tag{2.4} \\end{align*}\\] <p>Thus, \\( p_X(x) \\) is the probability that the mapping \\( X \\) takes the value \\( x \\). Note that \\( \\text{Im } X \\) is countable for any discrete random variable \\( X \\), and</p> \\[ \\begin{align*}     p_X(x) &amp;= 0 \\quad \\text{if} \\quad x \\notin \\text{Im } X, \\tag{2.5} \\\\     \\sum_{x \\in \\text{Im } X} p_X(x) &amp;= \\mathbb{P} \\left( \\bigcup_{x \\in \\text{Im } X} \\left\\{ \\omega \\in \\Omega : X(\\omega) = x \\right\\} \\right) \\\\     &amp;= \\mathbb{P}(\\Omega) = 1. \\tag{1.14} \\end{align*} \\] <p>Because only countable \\(x\\) has non-zero probability, equation (2.6) equivalent to </p> \\[ \\begin{align*}     \\sum_{x \\in \\mathbb{R}} p_X(x) &amp;= 1, \\tag{2.6} \\end{align*} \\] <p>Condition (2.6) essentially characterizes mass functions of discrete random variables in the sense of the following theorem.</p> <p>Theorem (2.7)</p> <p>Let \\( S = \\{ s_i : i \\in I \\} \\) be a countable set of distinct real numbers, and let \\( \\{\\pi_i : i \\in I\\} \\) be a collection of real numbers satisfying</p> \\[ \\begin{align*}     \\pi_i &amp;\\geq 0 \\quad \\text{for} \\quad i \\in I, \\\\     \\sum_{i \\in I} \\pi_i &amp;= 1. \\end{align*} \\] <p>There exists a probability space \\( (\\Omega, \\mathcal{F}, \\mathbb{P}) \\) and a discrete random variable \\( X \\) on \\( (\\Omega, \\mathcal{F}, \\mathbb{P}) \\) such that the probability mass function of \\( X \\) is given by</p> \\[ \\begin{align*}     p_X(s_i) &amp;= \\pi_i \\quad \\text{for} \\quad i \\in I, \\\\     p_X(s) &amp;= 0 \\quad \\text{if} \\quad s \\notin S. \\end{align*} \\] Proof <p>Take \\( \\Omega = S \\), \\( \\mathcal{F} \\) to be the set of all subsets of \\( \\Omega \\), and</p> \\[ \\mathbb{P}(A) = \\sum_{i: s_i \\in A} \\pi_i \\quad \\text{for} \\quad A \\in \\mathcal{F}. \\] <p>Finally, define \\( X : \\Omega \\to \\mathbb{R} \\) by \\( X(\\omega) = \\omega \\) for \\( \\omega \\in \\Omega \\). \\(\\qed\\)</p> <p>This theorem is very useful, since for many purposes it allows us to forget about sample spaces, event spaces, and probability measures; we need only say \"let \\( X \\) be a random variable taking the value \\( s_i \\) with probability \\( \\pi_i \\), for \\( i \\in I \\),\" and we can be sure that such a random variable exists without having to construct it explicitly.</p> <p>Definition (Covariance)</p> <p>For two random variables \\(X, Y\\), the covariance of \\(X, Y\\), denoted \\(\\text{Cov}(X,Y)\\) is defined as</p> \\[\\begin{align*} \\text{Cov}(X, Y) = \\mathbb{E}(XY) - \\mathbb{E}(X)\\mathbb{E}(Y) \\end{align*}\\] <p>Definition (Uncorrelated)</p> <p>If \\(X, Y\\) have \\(\\text{Cov}(X,Y) = 0\\), we say that they are uncorrelated.</p> <p>Theorem</p> <p>If \\(X\\) and \\(Y\\) are independent then they are uncorrelated (but the converse does not hold in general).</p> Proof <p>Theorem</p> \\[\\begin{align*} \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2\\text{Cov}(X,Y) \\end{align*}\\] \\[\\begin{align*} \\text{Var}(X_1 + X_2 + \\cdots + X_n) &amp;= \\sum_{i=1}^{n}\\text{Var}(X_i) + 2\\sum_{1\\le i &lt; j \\le n}{\\text{Cov}(X_i, X_j)} \\\\ &amp;= \\sum_{i=1}^{n}\\text{Var}(X_i) + \\sum_{i \\neq j}{\\text{Cov}(X_i, X_j)} \\end{align*}\\]"},{"location":"probability/discrete%20random%20variables/#random-graph","title":"Random Graph","text":""},{"location":"probability/intro/","title":"Introduction to Probability Theory","text":"<p>Definition (Event Space \\(\\mathcal{F}\\))</p> <p>The collection \\(\\mathcal{F}\\) of subsets of the sample space \\(\\Omega\\) is called an event space if </p> \\[\\begin{align*} &amp; \\mathcal{F} \\text{ is non-empty},\\tag{1} \\\\ &amp; \\text{if } A \\in \\mathcal{F} \\text{ then } \\Omega \\setminus A \\in \\mathcal{F}, \\tag{2} \\\\ &amp; \\text{if } A_1, A_2, \\dots \\in \\mathcal{F} \\text{ then } \\bigcup_{i=1}^{\\infty} A_i \\in \\mathcal{F}. \\tag{3} \\end{align*}\\] <p>i.e. \\(\\mathcal{F}\\) is closed under the operations of taking complements and countable unions.</p> <p>Remark (Intuitive Understanding of Event Space)</p> <p>We can call a non-empty set \\(\\mathcal{F}\\) Event Space as long as any event \\(A\\) in \\(\\mathcal{F}\\) satisfies: </p> <p>\u00a0\u00a0\u00a0\u00a01. \\(A\\) not happening is also an event in \\(\\mathcal{F}\\) by the axiom (2).  \u00a0\u00a0\u00a0\u00a02. Either \\(A\\) or any other event \\(B \\in \\mathcal{F}\\) happening is also an event in \\(\\mathcal{F}\\) by the axiom (3).</p> <p>Note (Properties of Event Space \\(\\mathcal{F}\\))</p> <p>(a) An event space \\(\\mathcal{F}\\) must contain the empty set \\(\\varnothing\\) and the whole set \\(\\Omega\\). </p> Proof <p>By (1), there exists some \\(A \\in \\mathcal{F}\\). By (2), \\(A^c \\in \\mathcal{F}\\). We set \\(A_1 = A\\), \\(A_i = A^c\\) for \\(i \\geq 2\\) in (3), and deduce that \\(\\mathcal{F}\\) contains the union \\(\\Omega = A \\cup A^c\\). By (2) again, the complement \\(\\Omega \\setminus \\Omega = \\varnothing\\) lies in \\(\\mathcal{F}\\) also.</p> <p>(b) An event space is closed under the operation of finite unions.</p> Proof <p>Let \\(A_1, A_2, \\dots, A_m \\in \\mathcal{F}\\), and set \\(A_i = \\varnothing\\) for \\(i &gt; m\\). Then \\(A := \\bigcup_{i=1}^{m} A_i\\) satisfies \\(A = \\bigcup_{i=1}^{\\infty} A_i\\), so that \\(A \\in \\mathcal{F}\\) by (3).</p> <p>(c) An event space is also closed under the operations of taking finite or countable intersections. </p> Proof <p>Note that \\((A \\cap B)^c = A^c \\cup B^c\\).</p> Example (Examples of Event Space \\(\\mathcal{F}\\)) <p>Example 1 \\(\\Omega\\) is any non-empty set and \\(\\mathcal{F}\\) is the power set of \\(\\Omega\\). \u25b3</p> <p>Example 2 \\(\\Omega\\) is any non-empty set and \\(\\mathcal{F} = \\{\\varnothing, A, \\Omega \\setminus A, \\Omega\\}\\), where \\(A\\) is a given non-trivial subset of \\(\\Omega\\). \u25b3</p> <p>Example 3 \\(\\Omega = \\{1, 2, 3, 4, 5, 6\\}\\) and \\(\\mathcal{F}\\) is the collection  \\(\\{\\varnothing, \\{1, 2\\}, \\{3, 4\\}, \\{5, 6\\}, \\{1, 2, 3, 4\\}, \\{3, 4, 5, 6\\}, \\{1, 2, 5, 6\\}, \\Omega\\}\\) of subsets of \\(\\Omega\\).   \u25b3</p> <p>Definition (Probability Measure)</p> <p>A mapping \\(\\mathbb{P} : \\mathcal{F} \\rightarrow \\mathbb{R}\\) is called a probability measure on \\((\\Omega, \\mathcal{F})\\) if</p> <p>(a) \\(\\mathbb{P}(A) \\geq 0\\) for \\(A \\in \\mathcal{F}\\), (b) \\(\\mathbb{P}(\\Omega) = 1\\) and \\(\\mathbb{P}(\\varnothing) = 0\\), (c) if \\(A_1, A_2, \\dots\\) are disjoint events in \\(\\mathcal{F}\\) (in that \\(A_i \\cap A_j = \\varnothing\\) whenever \\(i \\neq j\\)) then  </p> \\[\\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} \\mathbb{P}(A_i). \\tag{4}\\] <p>Note (Why \\(\\mathbb{P}(\\varnothing) = 0\\) ?)</p> <p>Define the disjoint events \\(A_1 = \\Omega\\), \\(A_i = \\varnothing\\) for \\(i \\geq 2\\). By condition (c),</p> \\[ \\mathbb{P}(\\Omega) = \\mathbb{P}\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\mathbb{P}(\\Omega) + \\sum_{i=2}^{\\infty} \\mathbb{P}(\\varnothing). \\] <p>Definition (Probability Space)</p> <p>A probability space is a triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) of objects such that</p> <p>(a) \\(\\Omega\\) is a non-empty set, (b) \\(\\mathcal{F}\\) is an event space of subsets of \\(\\Omega\\), (c) \\(\\mathbb{P}\\) is a probability measure on \\((\\Omega, \\mathcal{F})\\).</p> <p>Important Note (Properties of Probability Space)</p> <p>Property 1  If \\(A, B \\in \\mathcal{F}\\), then \\(A \\setminus B \\in \\mathcal{F}\\). </p> Proof <p>The complement of \\(A \\setminus B\\) equals \\((\\Omega \\setminus A) \\cup B\\), which is the union of events and is therefore an event. Hence \\(A \\setminus B\\) is an event. \u25a1</p> <p>Property 2  If \\(A, B \\in \\mathcal{F}\\), then \\(\\mathbb{P}(A \\cup B) + \\mathbb{P}(A \\cap B) = \\mathbb{P}(A) + \\mathbb{P}(B)\\).</p> Proof <p>The set \\(A\\) is the union of the disjoint sets \\(A \\setminus B\\) and \\(A \\cap B\\), and hence</p> \\[\\mathbb{P}(A) = \\mathbb{P}(A \\setminus B) + \\mathbb{P}(A \\cap B) \\quad \\text{IMPORTANT}.\\] <p>A similar remark holds for the set \\(B\\), giving that</p> \\[\\begin{aligned} \\mathbb{P}(A) + \\mathbb{P}(B) &amp;= \\mathbb{P}(A \\setminus B) + 2\\mathbb{P}(A \\cap B) + \\mathbb{P}(B \\setminus A) \\\\ &amp;= \\mathbb{P}((A \\setminus B) \\cup (A \\cap B)) \\cup (B \\setminus A)) + \\mathbb{P}(A \\cap B) \\\\ &amp;= \\mathbb{P}(A \\cup B) + \\mathbb{P}(A \\cap B). \\end{aligned}\\] <p>Property 3 If \\(A_1, A_2, \\dots \\in \\mathcal{F}\\), then \\(\\bigcap_{i=1}^{\\infty} A_i \\in \\mathcal{F}\\).</p> Proof <p>The complement of \\(\\bigcap_{i=1}^{\\infty} A_i\\) equals \\(\\bigcup_{i=1}^{\\infty} (\\Omega \\setminus A_i)\\), which is the union of the complements of events and is therefore an event. Hence the intersection of the \\(A_i\\) is an event also, as before. \u25a1</p> <p>Property 4 If \\(A, B \\in \\mathcal{F}\\) and \\(A \\subseteq B\\) then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\).</p> Proof <p>We have that \\(\\mathbb{P}(B) = \\mathbb{P}(A) + \\mathbb{P}(B \\setminus A) \\geq \\mathbb{P}(A)\\). \u25a1</p> <p>Definition (Conditional Probability)</p> <p>If \\(A, B \\in \\mathcal{F}\\) and \\(\\mathbb{P}(B) &gt; 0\\), the conditional probability of \\(A\\) given \\(B\\) is denoted by \\(\\mathbb{P}(A \\mid B)\\) and defined by</p> \\[ \\mathbb{P}(A \\mid B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}. \\tag{5} \\] <p>Remark</p> <p>Formula (5) is a definition rather than a theorem. An intuition to define conditional probability is that \\(\\mathbb{P}(A \\mid A) = 1\\). (Draw Venn Diagram)</p> <p>Theorem</p> <p>If \\(B \\in \\mathcal{F}\\) and \\(\\mathbb{P}(B) &gt; 0\\) then \\((\\Omega, \\mathcal{F}, \\mathbb{Q})\\) is a probability space where \\(\\mathbb{Q} : \\mathcal{F} \\rightarrow \\mathbb{R}\\) is defined by \\(\\mathbb{Q}(A) = \\mathbb{P}(A \\mid B)\\).</p> Proof <p>We need only check that \\(\\mathbb{Q}\\) is a probability measure on \\((\\Omega, \\mathcal{F})\\). Certainly \\(\\mathbb{Q}(A) \\geq 0\\) for \\(A \\in \\mathcal{F}\\) and</p> \\[ \\mathbb{Q}(\\Omega) = \\mathbb{P}(\\Omega \\mid B) = \\frac{\\mathbb{P}(\\Omega \\cap B)}{\\mathbb{P}(B)} = 1, \\] <p>and it remains to check that \\(\\mathbb{Q}\\) satisfies (4). Suppose that \\(A_1, A_2, \\dots\\) are disjoint events in \\(\\mathcal{F}\\). Then </p> \\[\\begin{align*} \\mathbb{Q}\\left(\\bigcup_i A_i\\right) &amp;= \\frac{1}{\\mathbb{P}(B)} \\mathbb{P}\\left(\\left(\\bigcup_i A_i\\right) \\cap B\\right) \\\\                                      &amp;= \\frac{1}{\\mathbb{P}(B)} \\mathbb{P}\\left(\\bigcup_i (A_i \\cap B)\\right) \\\\                                      &amp;= \\frac{1}{\\mathbb{P}(B)} \\sum_i \\mathbb{P}(A_i \\cap B) \\quad \\text{since $\\mathbb{P}$ satisfies (4)} \\\\                                      &amp;= \\sum_i \\mathbb{Q}(A_i). \\end{align*}\\] <p>Therefore, \\(\\mathbb{Q}\\) is a probability measure on \\((\\Omega, \\mathcal{F})\\). \u25a1</p> <p>Definition (Naive Definition of Independent Events)</p> <p>We call two events \\(A\\) and \\(B\\) independent if the occurrence of one of them does not affect the probability that the other occurs. More formally, we define: </p> <p>if \\(\\mathbb{P}(A), \\mathbb{P}(B) &gt; 0\\), then</p> \\[\\mathbb{P}(A \\mid B) = \\mathbb{P}(A) \\quad \\text{and} \\quad \\mathbb{P}(B \\mid A) = \\mathbb{P}(B). \\tag{6}\\] <p>Definition (Generalized Definition of Two Independent Events)</p> <p>Writing \\(\\mathbb{P}(A \\mid B) = \\mathbb{P}(A \\cap B)/\\mathbb{P}(B)\\), we have:</p> <p>Events \\(A\\) and \\(B\\) of a probability space \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) are called independent if</p> \\[ \\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\mathbb{P}(B), \\tag{7} \\] <p>and dependent otherwise.</p> <p>This definition is slightly more general since it allows the events \\(A\\) and \\(B\\) to have zero probability. </p> <p>It is easily generalized as follows to more than two events.:</p> <p>Definition (Definition of Independent Events)</p> <p>A family \\(\\mathcal{A} = (A_i \\mid i \\in I)\\) of events is called independent if, for all finite subsets \\(J\\) of \\(I\\),</p> \\[ \\mathbb{P}\\left(\\bigcap_{i \\in J} A_i\\right) = \\prod_{i \\in J} \\mathbb{P}(A_i). \\tag{8} \\] <p>Definition (Pairwise Independent)</p> <p>The family \\(\\mathcal{A}\\) is called pairwise independent if (4) holds whenever \\(|J| = 2\\). </p> <p>There are families of events which are pairwise independent but not independent.</p> Example (Pairwise Independent but Dependent) <p>Suppose that we throw a fair four-sided die (you may think of this as a square die thrown in a two-dimensional universe). We may take \\(\\Omega = \\{1, 2, 3, 4\\}\\), where each \\(\\omega \\in \\Omega\\) is equally likely to occur. The events \\(A = \\{1, 2\\}\\), \\(B = \\{1, 3\\}\\), \\(C = \\{1, 4\\}\\) are pairwise independent but not independent.</p> <p>Theorem (Partition Theorem)</p> <p>If \\(\\{B_1, B_2, \\dots \\}\\) is a partition of \\(\\Omega\\) with \\(\\mathbb{P}(B_i) &gt; 0\\) for each \\(i\\), then</p> \\[ \\mathbb{P}(A) = \\sum_i \\mathbb{P}(A \\mid B_i)\\mathbb{P}(B_i) \\quad \\text{for } A \\in \\mathcal{F}. \\] Proof <p>We have that</p> \\[\\begin{align*} \\mathbb{P}(A) &amp;= \\sum_i \\mathbb{P}(A \\cap B_i) \\tag{Venn Diagram} \\\\               &amp;= \\sum_i \\mathbb{P}(A \\mid B_i)\\mathbb{P}(B_i). \\end{align*}\\] <p>Remark (Intuition of Partition Theorem)</p> <p>The partition theorem is saying if we have a set of event that partition \\(\\Omega\\), we can use it to crop an arbitrary event \\(E\\) into several pieces, and the combination of all those pieces is exactly \\(E\\).</p> <p>Exercise (Application of Partition Theorem)</p> <p>Tomorrow there will be either rain or snow but not both; the probability of rain is \\(\\frac{2}{5}\\) and the probability of snow is \\(\\frac{3}{5}\\). If it rains, the probability that I will be late for my lecture is \\(\\frac{1}{5}\\), while the corresponding probability in the event of snow is \\(\\frac{3}{5}\\). What is the probability that I will be late?</p> Solution <p>Let \\(A\\) be the event that I am late and \\(B\\) be the event that it rains. The pair \\(B, B^c\\) is a partition of the sample space (since exactly one of them must occur). By Partition Theorem,</p> \\[\\begin{align*} \\mathbb{P}(A) &amp;= \\mathbb{P}(A \\mid B)\\mathbb{P}(B) + \\mathbb{P}(A \\mid B^c)\\mathbb{P}(B^c) \\\\             &amp;= \\frac{1}{5} \\cdot \\frac{2}{5} + \\frac{3}{5} \\cdot \\frac{3}{5} = \\frac{11}{25}. \\end{align*}\\] <p>Theorem (Bayes' Theorem)</p> <p>Let \\(\\{B_1, B_2, \\dots\\}\\) be a partition of the sample space \\(\\Omega\\) such that \\(\\mathbb{P}(B_i) &gt; 0\\) for each \\(i\\). For any event \\(A\\) with \\(\\mathbb{P}(A) &gt; 0\\),</p> \\[ \\mathbb{P}(B_j \\mid A) = \\frac{\\mathbb{P}(A \\mid B_j)\\mathbb{P}(B_j)}{\\sum_i \\mathbb{P}(A \\mid B_i)\\mathbb{P}(B_i)}. \\] Proof <p>By the definition of conditional probability,</p> \\[\\begin{align*} \\mathbb{P}(B_j \\mid A) &amp;= \\frac{\\mathbb{P}(A \\mid B_j)\\mathbb{P}(B_j)}{\\mathbb{P}(A)}, \\\\                        &amp;= \\frac{\\mathbb{P}(A \\mid B_j)\\mathbb{P}(B_j)}{\\sum_i \\mathbb{P}(A \\mid B_i)\\mathbb{P}(B_i)} \\tag{Partition Theorem} \\end{align*}\\] <p>Exercise (False Positives)</p> <p>A rare but potentially fatal disease has an incidence of 1 in \\(10^5\\) in the population at large. There is a diagnostic test, but it is imperfect. If you have the disease, the test is positive with probability \\(\\frac{9}{10}\\); if you do not, the test is positive with probability \\(\\frac{1}{20}\\). Your test result is positive. What is the probability that you have the disease?</p> Solution <p>Write \\(D\\) for the event that you have the disease, and \\(P\\) for the event that the test is positive. By Bayes' theorem, </p> \\[\\begin{align*} \\mathbb{P}(D \\mid P) &amp;= \\frac{\\mathbb{P}(P \\mid D)\\mathbb{P}(D)}{\\mathbb{P}(P \\mid D)\\mathbb{P}(D) + \\mathbb{P}(P \\mid D^c)\\mathbb{P}(D^c)} \\\\                     &amp;= \\frac{\\frac{9}{10} \\cdot \\frac{1}{10^5}}{\\frac{9}{10} \\cdot \\frac{1}{10^5} + \\frac{1}{20} \\cdot \\frac{10^5-1}{10^5}} \\approx 0.0002. \\end{align*}\\]"}]}