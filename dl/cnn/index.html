
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://tsljgj.github.io/ML-note/dl/cnn/">
      
      
        <link rel="prev" href="../linearClassifier/">
      
      
        <link rel="next" href="../nn/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>Convolutional Neural Networks (CNN) - ML Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.35f28582.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      
  
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
    
    
  
  
  <style>:root{--md-admonition-icon--df:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M471.6 21.7c-21.9-21.9-57.3-21.9-79.2 0l-30.1 30 97.9 97.9 30.1-30.1c21.9-21.9 21.9-57.3 0-79.2zm-299.2 220c-6.1 6.1-10.8 13.6-13.5 21.9l-29.6 88.8c-2.9 8.6-.6 18.1 5.8 24.6s15.9 8.7 24.6 5.8l88.8-29.6c8.2-2.7 15.7-7.4 21.9-13.5l167.3-167.4-98-98zM96 64c-53 0-96 43-96 96v256c0 53 43 96 96 96h256c53 0 96-43 96-96v-96c0-17.7-14.3-32-32-32s-32 14.3-32 32v96c0 17.7-14.3 32-32 32H96c-17.7 0-32-14.3-32-32V160c0-17.7 14.3-32 32-32h96c17.7 0 32-14.3 32-32s-14.3-32-32-32z"/></svg>');--md-admonition-icon--nt:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-40-176h24v-64h-24c-13.3 0-24-10.7-24-24s10.7-24 24-24h48c13.3 0 24 10.7 24 24v88h8c13.3 0 24 10.7 24 24s-10.7 24-24 24h-80c-13.3 0-24-10.7-24-24s10.7-24 24-24m40-208a32 32 0 1 1 0 64 32 32 0 1 1 0-64"/></svg>');--md-admonition-icon--rm:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M32 32C32 14.3 46.3 0 64 0h256c17.7 0 32 14.3 32 32s-14.3 32-32 32h-29.5l11.4 148.2c36.7 19.9 65.7 53.2 79.5 94.7l1 3c3.3 9.8 1.6 20.5-4.4 28.8S362.3 352 352 352H32c-10.3 0-19.9-4.9-26-13.3s-7.7-19.1-4.4-28.8l1-3c13.8-41.5 42.8-74.8 79.5-94.7L93.5 64H64c-17.7 0-32-14.3-32-32m128 352h64v96c0 17.7-14.3 32-32 32s-32-14.3-32-32z"/></svg>');--md-admonition-icon--eg:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M272 384c9.6-31.9 29.5-59.1 49.2-86.2 5.2-7.1 10.4-14.2 15.4-21.4 19.8-28.5 31.4-63 31.4-100.3C368 78.8 289.2 0 192 0S16 78.8 16 176c0 37.3 11.6 71.9 31.4 100.3 5 7.2 10.2 14.3 15.4 21.4 19.8 27.1 39.7 54.4 49.2 86.2h160zm-80 128c44.2 0 80-35.8 80-80v-16H112v16c0 44.2 35.8 80 80 80m-80-336c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-61.9 50.1-112 112-112 8.8 0 16 7.2 16 16s-7.2 16-16 16c-44.2 0-80 35.8-80 80"/></svg>');--md-admonition-icon--ex:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M156.6 384.9 125.7 354c-8.5-8.5-11.5-20.8-7.7-32.2 3-8.9 7-20.5 11.8-33.8H24c-8.6 0-16.6-4.6-20.9-12.1s-4.2-16.7.2-24.1l52.5-88.5c13-21.9 36.5-35.3 61.9-35.3H200c2.4-4 4.8-7.7 7.2-11.3C289.1-4.1 411.1-8.1 483.9 5.3c11.6 2.1 20.6 11.2 22.8 22.8 13.4 72.9 9.3 194.8-111.4 276.7-3.5 2.4-7.3 4.8-11.3 7.2v82.3c0 25.4-13.4 49-35.3 61.9l-88.5 52.5c-7.4 4.4-16.6 4.5-24.1.2S224 496.7 224 488V380.8c-14.1 4.9-26.4 8.9-35.7 11.9-11.2 3.6-23.4.5-31.8-7.8zM384 168a40 40 0 1 0 0-80 40 40 0 1 0 0 80"/></svg>');--md-admonition-icon--tm:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M234.5 5.7c13.9-5 29.1-5 43.1 0l192 68.6c25.4 9.1 42.4 33.2 42.4 60.3v242.9c0 27-17 51.2-42.5 60.3l-192 68.6c-13.9 5-29.1 5-43.1 0l-192-68.6C17 428.6 0 404.5 0 377.4V134.6c0-27 17-51.2 42.5-60.3zM256 66 82.3 128 256 190l173.7-62zm32 368.6 160-57.1v-188l-160 57.1z"/></svg>');--md-admonition-icon--sl:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M438.6 105.4c12.5 12.5 12.5 32.8 0 45.3l-256 256c-12.5 12.5-32.8 12.5-45.3 0l-128-128c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L160 338.7l233.4-233.3c12.5-12.5 32.8-12.5 45.3 0z"/></svg>');--md-admonition-icon--im:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M316.9 18c-5.3-11-16.5-18-28.8-18s-23.4 7-28.8 18L195 150.3 51.4 171.5c-12 1.8-22 10.2-25.7 21.7s-.7 24.2 7.9 32.7L137.8 329l-24.6 145.7c-2 12 3 24.2 12.9 31.3s23 8 33.8 2.3l128.3-68.5 128.3 68.5c10.8 5.7 23.9 4.9 33.8-2.3s14.9-19.3 12.9-31.3L438.5 329l104.2-103.1c8.6-8.5 11.7-21.2 7.9-32.7s-13.7-19.9-25.7-21.7l-143.7-21.2z"/></svg>');--md-admonition-icon--pf:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M336 352c97.2 0 176-78.8 176-176S433.2 0 336 0 160 78.8 160 176c0 18.7 2.9 36.8 8.3 53.7L7 391c-4.5 4.5-7 10.6-7 17v80c0 13.3 10.7 24 24 24h80c13.3 0 24-10.7 24-24v-40h40c13.3 0 24-10.7 24-24v-40h40c6.4 0 12.5-2.5 17-7l33.3-33.3c16.9 5.4 35 8.3 53.7 8.3m40-256a40 40 0 1 1 0 80 40 40 0 1 1 0-80"/></svg>');--md-admonition-icon--st:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M96 48 82.7 61.3c-12 12-18.7 28.2-18.7 45.2v132.4c0 10.7 5.3 20.7 14.2 26.6l10.6 7c14.3 9.6 32.7 10.7 48.1 3l3.2-1.6c2.6-1.3 5-2.8 7.3-4.5l49.4-37c6.6-5 15.7-5 22.3 0 10.2 7.7 9.9 23.1-.7 30.3L90.4 350C73.9 361.3 64 380 64 400h320l28.9-159c2.1-11.3 3.1-22.8 3.1-34.3V192C416 86 330 0 224 0H83.8C72.9 0 64 8.9 64 19.8c0 7.5 4.2 14.3 10.9 17.7zm24 68a20 20 0 1 1 40 0 20 20 0 1 1-40 0M22.6 473.4c-4.2 4.2-6.6 10-6.6 16 0 12.5 10.1 22.6 22.6 22.6h370.7c12.5 0 22.6-10.1 22.6-22.6 0-6-2.4-11.8-6.6-16L384 432H64z"/></svg>');--md-admonition-icon--wr:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 32c14.2 0 27.3 7.5 34.5 19.8l216 368c7.3 12.4 7.3 27.7.2 40.1S486.3 480 472 480H40c-14.3 0-27.6-7.7-34.7-20.1s-7-27.8.2-40.1l216-368C228.7 39.5 241.8 32 256 32m0 128c-13.3 0-24 10.7-24 24v112c0 13.3 10.7 24 24 24s24-10.7 24-24V184c0-13.3-10.7-24-24-24m32 224a32 32 0 1 0-64 0 32 32 0 1 0 64 0"/></svg>');--md-admonition-icon--mt:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 96c0-53 43-96 96-96h38.4C439.9 0 480 40.1 480 89.6V376c0 75.1-60.9 136-136 136s-136-60.9-136-136v-80c0-22.1-17.9-40-40-40s-40 17.9-40 40v168c0 26.5-21.5 48-48 48s-48-21.5-48-48V296c0-75.1 60.9-136 136-136s136 60.9 136 136v80c0 22.1 17.9 40 40 40s40-17.9 40-40V192h-32c-53 0-96-43-96-96m144-8a24 24 0 1 0-48 0 24 24 0 1 0 48 0"/></svg>');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M96 0C43 0 0 43 0 96v320c0 53 43 96 96 96h320c17.7 0 32-14.3 32-32s-14.3-32-32-32v-64c17.7 0 32-14.3 32-32V32c0-17.7-14.3-32-32-32H96m0 384h256v64H96c-17.7 0-32-14.3-32-32s14.3-32 32-32m32-240c0-8.8 7.2-16 16-16h192c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16m16 48h192c8.8 0 16 7.2 16 16s-7.2 16-16 16H144c-8.8 0-16-7.2-16-16s7.2-16 16-16"/></svg>');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-40-176h24v-64h-24c-13.3 0-24-10.7-24-24s10.7-24 24-24h48c13.3 0 24 10.7 24 24v88h8c13.3 0 24 10.7 24 24s-10.7 24-24 24h-80c-13.3 0-24-10.7-24-24s10.7-24 24-24m40-208a32 32 0 1 1 0 64 32 32 0 1 1 0-64"/></svg>');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M480 32c0-12.9-7.8-24.6-19.8-29.6S434.5.2 425.3 9.3L381.7 53c-48 48-113.1 75-181 75H64c-35.3 0-64 28.7-64 64v96c0 35.3 28.7 64 64 64v128c0 17.7 14.3 32 32 32h64c17.7 0 32-14.3 32-32V352h8.7c67.9 0 133 27 181 75l43.6 43.6c9.2 9.2 22.9 11.9 34.9 6.9s19.8-16.6 19.8-29.6V300.3c18.6-8.8 32-32.5 32-60.4s-13.4-51.6-32-60.4zm-64 76.7v262.6C357.2 317.8 280.5 288 200.7 288H192v-96h8.7c79.8 0 156.5-29.8 215.3-83.3"/></svg>');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M438.6 105.4c12.5 12.5 12.5 32.8 0 45.3l-256 256c-12.5 12.5-32.8 12.5-45.3 0l-128-128c-12.5-12.5-12.5-32.8 0-45.3s32.8-12.5 45.3 0L160 338.7l233.4-233.3c12.5-12.5 32.8-12.5 45.3 0z"/></svg>');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-86.2-346.7c7.9-22.3 29.1-37.3 52.8-37.3h58.3c34.9 0 63.1 28.3 63.1 63.1 0 22.6-12.1 43.5-31.7 54.8L280 264.4c-.2 13-10.9 23.6-24 23.6-13.3 0-24-10.7-24-24v-13.5c0-8.6 4.6-16.5 12.1-20.8l44.3-25.4c4.7-2.7 7.6-7.7 7.6-13.1 0-8.4-6.8-15.1-15.1-15.1h-58.3c-3.4 0-6.4 2.1-7.5 5.3l-.4 1.2c-4.4 12.5-18.2 19-30.6 14.6s-19-18.2-14.6-30.6l.4-1.2zM224 352a32 32 0 1 1 64 0 32 32 0 1 1-64 0"/></svg>');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M256 32c14.2 0 27.3 7.5 34.5 19.8l216 368c7.3 12.4 7.3 27.7.2 40.1S486.3 480 472 480H40c-14.3 0-27.6-7.7-34.7-20.1s-7-27.8.2-40.1l216-368C228.7 39.5 241.8 32 256 32m0 128c-13.3 0-24 10.7-24 24v112c0 13.3 10.7 24 24 24s24-10.7 24-24V184c0-13.3-10.7-24-24-24m32 224a32 32 0 1 0-64 0 32 32 0 1 0 64 0"/></svg>');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.1 52.4 442.6 6.5c-1.9-3.9-6.1-6.5-10.5-6.5s-8.5 2.6-10.4 6.5l-16.5 45.9-46 16.8c-4.3 1.6-7.3 5.9-7.2 10.4 0 4.5 3 8.7 7.2 10.2l45.7 16.8 16.8 45.8c1.5 4.4 5.8 7.5 10.4 7.5s8.9-3.1 10.4-7.5l16.5-45.8 45.7-16.8c4.2-1.5 7.2-5.7 7.2-10.2 0-4.6-3-8.9-7.2-10.4zm-132.4 53c-12.5-12.5-32.8-12.5-45.3 0l-2.9 2.9c-22-8-45.8-12.3-70.5-12.3C93.1 96 0 189.1 0 304s93.1 208 208 208 208-93.1 208-208c0-24.7-4.3-48.5-12.2-70.5l2.9-2.9c12.5-12.5 12.5-32.8 0-45.3l-80-80zM200 192c-57.4 0-104 46.6-104 104v8c0 8.8-7.2 16-16 16s-16-7.2-16-16v-8c0-75.1 60.9-136 136-136h8c8.8 0 16 7.2 16 16s-7.2 16-16 16z"/></svg>');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 398.9c58.5-41.1 96-104.1 96-174.9C512 100.3 397.4 0 256 0S0 100.3 0 224c0 70.7 37.5 133.8 96 174.9V464c0 26.5 21.5 48 48 48h48v-48c0-8.8 7.2-16 16-16s16 7.2 16 16v48h64v-48c0-8.8 7.2-16 16-16s16 7.2 16 16v48h48c26.5 0 48-21.5 48-48v-65.1M96 256a64 64 0 1 1 128 0 64 64 0 1 1-128 0m256-64a64 64 0 1 1 0 128 64 64 0 1 1 0-128"/></svg>');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M320 0c17.7 0 32 14.3 32 32v64h120c39.8 0 72 32.2 72 72v272c0 39.8-32.2 72-72 72H168c-39.8 0-72-32.2-72-72V168c0-39.8 32.2-72 72-72h120V32c0-17.7 14.3-32 32-32M208 384c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zm96 0c-8.8 0-16 7.2-16 16s7.2 16 16 16h32c8.8 0 16-7.2 16-16s-7.2-16-16-16zM264 256a40 40 0 1 0-80 0 40 40 0 1 0 80 0m152 40a40 40 0 1 0 0-80 40 40 0 1 0 0 80M48 224h16v192H48c-26.5 0-48-21.5-48-48v-96c0-26.5 21.5-48 48-48m544 0c26.5 0 48 21.5 48 48v96c0 26.5-21.5 48-48 48h-16V224z"/></svg>');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M288 0H128c-17.7 0-32 14.3-32 32s14.3 32 32 32v132.8c0 11.8-3.3 23.5-9.5 33.5L10.3 406.2C3.6 417.2 0 429.7 0 442.6 0 480.9 31.1 512 69.4 512h309.2c38.3 0 69.4-31.1 69.4-69.4 0-12.8-3.6-25.4-10.3-36.4L329.5 230.4c-6.2-10.1-9.5-21.7-9.5-33.5V64c17.7 0 32-14.3 32-32S337.7 0 320 0zm-96 196.8V64h64v132.8c0 23.7 6.6 46.9 19 67.1l34.5 56.1h-171l34.5-56.1c12.4-20.2 19-43.4 19-67.1"/></svg>');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M0 216C0 149.7 53.7 96 120 96h8c17.7 0 32 14.3 32 32s-14.3 32-32 32h-8c-30.9 0-56 25.1-56 56v8h64c35.3 0 64 28.7 64 64v64c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64V216m256 0c0-66.3 53.7-120 120-120h8c17.7 0 32 14.3 32 32s-14.3 32-32 32h-8c-30.9 0-56 25.1-56 56v8h64c35.3 0 64 28.7 64 64v64c0 35.3-28.7 64-64 64h-64c-35.3 0-64-28.7-64-64V216"/></svg>');}</style>


  
  
  
  
  <style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c-4.41 0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8m0-18A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m1 5h-2v4H7v2h4v4h2v-4h4v-2h-4z"/></svg>');}</style>


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Times+New+Roman:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Times New Roman";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
      <link rel="stylesheet" href="../../stylesheets/admonitions.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="custom">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#convolutional-neural-networks-cnn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ML Notes" class="md-header__button md-logo" aria-label="ML Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="m48.7 125.8 53.2 31.9c7.8 4.7 17.8 2 22.2-5.9l77.5-139.7c3-5.4-.9-12.1-7.1-12.1-1.6 0-3.2.5-4.6 1.4l-142 97.4c-9.6 6.6-9.2 20.9.8 26.9zM16 171.7v123.5c0 8 10.4 11 14.7 4.4l60-92c5-7.6 2.6-17.8-5.2-22.5L40.2 158c-10.6-6.4-24.2 1.3-24.2 13.7M310.4 12.1 388 151.7c4.4 7.9 14.5 10.6 22.2 5.9l53.2-31.9c10-6 10.4-20.3.8-26.9L322.1 1.4c-1.4-.9-3-1.4-4.6-1.4-6.2 0-10.1 6.7-7.1 12.1M496 171.7c0-12.4-13.6-20.1-24.2-13.7l-45.3 27.2c-7.8 4.7-10.1 14.9-5.2 22.5l60 92c4.3 6.7 14.7 3.6 14.7-4.4V171.8zm-49.3 246-160.6 18.9c-8.1.9-14.1 7.8-14.1 15.9v52.8c0 3.7 3 6.8 6.8 6.8.8 0 1.6-.1 2.4-.4l172.7-64c6.1-2.2 10.1-8 10.1-14.5 0-9.3-8.1-16.5-17.3-15.4zM233.2 512c3.7 0 6.8-3 6.8-6.8v-52.6c0-8.1-6.1-14.9-14.1-15.9l-160.6-19c-9.2-1.1-17.3 6.1-17.3 15.4 0 6.5 4 12.3 10.1 14.5l172.7 64c.8.3 1.6.4 2.4.4M41.7 382.9l170.9 20.2c7.8.9 13.4-7.5 9.5-14.3l-85.7-150c-5.9-10.4-20.7-10.8-27.3-.8L30.2 358.2c-6.5 9.9-.3 23.3 11.5 24.7m439.6-24.8-78.4-120c-6.5-10-21.4-9.6-27.3.8l-85.4 149.6c-3.9 6.8 1.6 15.2 9.5 14.3l170.1-20c11.8-1.4 18-14.7 11.5-24.6zm-216.9 11 78.4-137.2c6.1-10.7-1.6-23.9-13.9-23.9H183.2c-12.3 0-20 13.3-13.9 23.9l78.4 137.2c3.7 6.4 13 6.4 16.7 0m-90-193.1h163.2c12.2 0 19.9-13.1 14-23.8l-80-144c-2.8-5.1-8.2-8.2-14-8.2h-3.2c-5.8 0-11.2 3.2-14 8.2l-80 144c-5.9 10.7 1.8 23.8 14 23.8"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ML Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Convolutional Neural Networks (CNN)
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="custom"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/tsljgj/ML-note" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    tsljgj/ML-note
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  About

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lnn/" class="md-tabs__link">
          
  
  Deep Learning

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../dp/basics/" class="md-tabs__link">
          
  
  Dynamic Programming

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../ds/basics/" class="md-tabs__link">
          
  
  Data Structure

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../math/prime/" class="md-tabs__link">
          
  
  OI Math

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../probability/intro/" class="md-tabs__link">
          
  
  Probability

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../math/numbers/" class="md-tabs__link">
          
  
  Real Analysis

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../math/linear_alg/intro/" class="md-tabs__link">
          
  
  Linear Algebra

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ML Notes" class="md-nav__button md-logo" aria-label="ML Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="m48.7 125.8 53.2 31.9c7.8 4.7 17.8 2 22.2-5.9l77.5-139.7c3-5.4-.9-12.1-7.1-12.1-1.6 0-3.2.5-4.6 1.4l-142 97.4c-9.6 6.6-9.2 20.9.8 26.9zM16 171.7v123.5c0 8 10.4 11 14.7 4.4l60-92c5-7.6 2.6-17.8-5.2-22.5L40.2 158c-10.6-6.4-24.2 1.3-24.2 13.7M310.4 12.1 388 151.7c4.4 7.9 14.5 10.6 22.2 5.9l53.2-31.9c10-6 10.4-20.3.8-26.9L322.1 1.4c-1.4-.9-3-1.4-4.6-1.4-6.2 0-10.1 6.7-7.1 12.1M496 171.7c0-12.4-13.6-20.1-24.2-13.7l-45.3 27.2c-7.8 4.7-10.1 14.9-5.2 22.5l60 92c4.3 6.7 14.7 3.6 14.7-4.4V171.8zm-49.3 246-160.6 18.9c-8.1.9-14.1 7.8-14.1 15.9v52.8c0 3.7 3 6.8 6.8 6.8.8 0 1.6-.1 2.4-.4l172.7-64c6.1-2.2 10.1-8 10.1-14.5 0-9.3-8.1-16.5-17.3-15.4zM233.2 512c3.7 0 6.8-3 6.8-6.8v-52.6c0-8.1-6.1-14.9-14.1-15.9l-160.6-19c-9.2-1.1-17.3 6.1-17.3 15.4 0 6.5 4 12.3 10.1 14.5l172.7 64c.8.3 1.6.4 2.4.4M41.7 382.9l170.9 20.2c7.8.9 13.4-7.5 9.5-14.3l-85.7-150c-5.9-10.4-20.7-10.8-27.3-.8L30.2 358.2c-6.5 9.9-.3 23.3 11.5 24.7m439.6-24.8-78.4-120c-6.5-10-21.4-9.6-27.3.8l-85.4 149.6c-3.9 6.8 1.6 15.2 9.5 14.3l170.1-20c11.8-1.4 18-14.7 11.5-24.6zm-216.9 11 78.4-137.2c6.1-10.7-1.6-23.9-13.9-23.9H183.2c-12.3 0-20 13.3-13.9 23.9l78.4 137.2c3.7 6.4 13 6.4 16.7 0m-90-193.1h163.2c12.2 0 19.9-13.1 14-23.8l-80-144c-2.8-5.1-8.2-8.2-14-8.2h-3.2c-5.8 0-11.2 3.2-14 8.2l-80 144c-5.9 10.7 1.8 23.8 14 23.8"/></svg>

    </a>
    ML Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/tsljgj/ML-note" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    tsljgj/ML-note
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    About
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Deep Learning
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Deep Learning
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lnn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linear Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../softmax/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Softmax
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../perceptron/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multilayer Perceptrons (MLP)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../logistic/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Logistic Regression
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../glms/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generalized Linear Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../generative/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Generative Learning Algorithms
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../kernel/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Kernel Methods
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gaussian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Gaussian Processes
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mdp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Markov Decision Processes (MDP)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../unsupervised/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Unsupervised Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../knn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    k - Nearest Neighbor (kNN)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linearClassifier/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linear Classifier
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Convolutional Neural Networks (CNN)
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Convolutional Neural Networks (CNN)
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutional Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relu-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Relu Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pooling-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Pooling Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fully-connected-fc-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Fully Connected (FC) Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softmax-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Softmax Loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../nn/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Neural Network Architectures
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Dynamic Programming
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Dynamic Programming
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dp/basics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basics
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dp/linear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Linear DP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dp/knapsack/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Knapsack
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dp/interval/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Interval DP
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../dp/tree/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tree DP
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Data Structure
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Data Structure
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ds/basics/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basic Data Structure
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    OI Math
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            OI Math
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/prime/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prime
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Probability
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Probability
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../probability/discrete%20random%20variables/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Discrete Random Variables
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Real Analysis
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Real Analysis
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/numbers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Basic Number Theory
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Linear Algebra
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Linear Algebra
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/linear_alg/intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/linear_alg/elimination/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Elimination Matrix
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/linear_alg/matmul/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Matrix Multiplication
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/linear_alg/inverses/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inverses
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../math/linear_alg/21-241/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    21-241
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#convolutions" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutions
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutional Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#relu-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Relu Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pooling-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Pooling Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#fully-connected-fc-layer" class="md-nav__link">
    <span class="md-ellipsis">
      Fully Connected (FC) Layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#softmax-loss" class="md-nav__link">
    <span class="md-ellipsis">
      Softmax Loss
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)<a class="headerlink" href="#convolutional-neural-networks-cnn" title="Permanent link">&para;</a></h1>
<h2 id="convolutions">Convolutions<a class="headerlink" href="#convolutions" title="Permanent link">&para;</a></h2>
<div class="admonition df">
<p class="admonition-title"><strong>Definition</strong> (Convolutions)</p>
<p>In mathematics, the <em>convolution</em> between two functions <span class="arithmatex">\(f,g: \mathbb{R}^d \rightarrow \mathbb{R}\)</span> is defined as </p>
<div class="arithmatex">\[\begin{align*}
(f * g)(x) = \int f(t)g(x-t)dt
\end{align*}\]</div>
<p>We measure the overlap between <span class="arithmatex">\(f\)</span> and <span class="arithmatex">\(g\)</span> when one function is "flipped" and shifted by <span class="arithmatex">\(x\)</span>.</p>
</div>
<div class="admonition eg">
<p class="admonition-title"><strong>Example</strong> (Dice - Not a Good Example)</p>
<p>Assume there are two 6-face dices. We want to know the probability of the sum of two dices equals to 4. <br>
Define <span class="arithmatex">\(f(x) = \text{probability of getting x on dice 1}\)</span>, <span class="arithmatex">\(g(x) = \text{probability of getting x on dice 2}\)</span>. The probability of getting a sum of 4 is:</p>
<div class="arithmatex">\[\begin{align*}
(f * g)(4) = \sum^3_{m=1}f(4-m)g(m) = f(1)g(3) + f(2)g(2) + f(3)g(1)
\end{align*}\]</div>
<p>We can view this as first flipped the function <span class="arithmatex">\(g\)</span>, and then shift <span class="arithmatex">\(1,2,3,4\)</span> position and calculate the overlapping area.</p>
<p><figure markdown="span">
<a class="glightbox" href="../../graphs/dl/cnn/cnn_dice.svg" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Image title" src="../../graphs/dl/cnn/cnn_dice.svg" width="400" /></a>
</figure></p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Convolutions are Impact Sum)</p>
</div>
<h2 id="architecture-overview">Architecture Overview<a class="headerlink" href="#architecture-overview" title="Permanent link">&para;</a></h2>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Layers in ConvNets)</p>
<p>We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer. We will stack these layers to form a full ConvNet architecture.</p>
</div>
<div class="admonition eg">
<p class="admonition-title"><strong>Example</strong> (ConvNet Architecture for CIFAR-10 Classification)</p>
<ul>
<li><strong>INPUT Layer</strong> [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B.</li>
<li><strong>CONV Layer</strong> will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This may result in volume such as [32x32x12] if we decided to use 12 filters.</li>
<li><strong>RELU Layer</strong> will apply an elementwise activation function, such as the <span class="arithmatex">\(\max(0,x)\)</span> thresholding at zero. This leaves the size of the volume unchanged ([32x32x12]).</li>
<li><strong>POOL Layer</strong> will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [16x16x12].</li>
<li><strong>FC Layer</strong> (i.e. fully-connected) will compute the class scores, resulting in volume of size [1x1x10], where each of the 10 numbers correspond to a class score, such as among the 10 categories of CIFAR-10.</li>
</ul>
<p><figure markdown="span">
<a class="glightbox" href="https://cs231n.github.io/assets/cnn/convnet.jpeg" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Image title" src="https://cs231n.github.io/assets/cnn/convnet.jpeg" width="600" /></a>
<figcaption>Activations of an ConvNet Architecture</figcaption>
</figure></p>
</div>
<h2 id="convolutional-layer">Convolutional Layer<a class="headerlink" href="#convolutional-layer" title="Permanent link">&para;</a></h2>
<div class="admonition df">
<p class="admonition-title"><strong>Definition</strong> (Filter - Receptive Fields)</p>
<p>The <em>Filter</em>, or the Receptive Field, in the context of CNN, is a <span class="arithmatex">\(F \times F \times 3\)</span> square with which we use to multiply local regions in the image. </p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Intuition about Filters)</p>
<p>Each filter is looking for a specific feature in the picture.</p>
</div>
<div class="admonition df">
<p class="admonition-title"><strong>Definition</strong> (Stride)</p>
<p><em>Stride</em> is the number of the pixel jumped when the filters slide. When the stride is 1 then we move the filters one pixel at a time. When the stride is 2 (or uncommonly 3 or more, though this is rare in practice) then the filters jump 2 pixels at a time as we slide them around. This will produce smaller output volumes spatially.</p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Why Stride 1)</p>
<p>Why use stride of 1 in CONV? Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.</p>
</div>
<div class="admonition df">
<p class="admonition-title"><strong>Definition</strong> (Zero-Padding)</p>
<p>The <em>zero-padding</em> is a boarder around the input volume that only has element 0. Sometimes it will be convenient to pad the input volume with zeros around the border. The size of this zero-padding is a hyperparameter. The nice feature of zero padding is that it will allow us to control the spatial size of the output volumes</p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Why Padding?)</p>
<p>Why use padding? In addition to keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be washed away too quickly.</p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Computing Output volume)</p>
<p>The Conv Layer:</p>
<ul>
<li>Accepts a volume of size <span class="arithmatex">\( W_1 \times H_1 \times D_1 \)</span></li>
<li>Requires four hyperparameters: <br>
   &nbsp;&nbsp;&nbsp;&nbsp;1. Number of filters <span class="arithmatex">\( K \)</span>, <br>
   &nbsp;&nbsp;&nbsp;&nbsp;2. their spatial extent <span class="arithmatex">\( F \)</span>, <br>
   &nbsp;&nbsp;&nbsp;&nbsp;3. the stride <span class="arithmatex">\( S \)</span>, <br>
   &nbsp;&nbsp;&nbsp;&nbsp;4. the amount of zero padding <span class="arithmatex">\( P \)</span>.</li>
<li>Produces a volume of size <span class="arithmatex">\( W_2 \times H_2 \times D_2 \)</span> where: <br>
    &nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\( W_2 = \left(\frac{W_1 - F + 2P}{S}\right) + 1 \)</span> <br>
    &nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\( H_2 = \left(\frac{H_1 - F + 2P}{S}\right) + 1 \)</span> &nbsp; (i.e. width and height are computed equally by symmetry) <br>
    &nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\( D_2 = K \)</span></li>
<li>With parameter sharing, it introduces <span class="arithmatex">\( F \cdot F \cdot D_1 \)</span> weights per filter, for a total of <span class="arithmatex">\( (F \cdot F \cdot D_1) \cdot K \)</span> weights and <span class="arithmatex">\( K \)</span> biases.</li>
<li>In the output volume, the <span class="arithmatex">\( d \)</span>-th depth slice (of size <span class="arithmatex">\( W_2 \times H_2 \)</span>) is the result of performing a valid convolution of the <span class="arithmatex">\( d \)</span>-th filter over the input volume with a stride of <span class="arithmatex">\( S \)</span>, and then offset by <span class="arithmatex">\( d \)</span>-th bias.</li>
</ul>
<p>A common setting of the hyperparameters is <span class="arithmatex">\( F = 3 \)</span>, <span class="arithmatex">\( S = 1 \)</span>, <span class="arithmatex">\( P = 1 \)</span>.</p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Convolution Demo)</p>
<p>Below is a running demo of a CONV layer. The input volume is of size <span class="arithmatex">\( W_1 = 5 \)</span>, <span class="arithmatex">\( H_1 = 5 \)</span>, <span class="arithmatex">\( D_1 = 3 \)</span>, and the CONV layer parameters are <span class="arithmatex">\( K = 2 \)</span>, <span class="arithmatex">\( F = 3 \)</span>, <span class="arithmatex">\( S = 2 \)</span>, <span class="arithmatex">\( P = 1 \)</span>. Therefore, the output volume size has spatial size <span class="arithmatex">\( (5 - 3 + 2)/2 + 1 = 3 \)</span>. The visualization below iterates over the output activations (green), and shows that each element is computed by <strong>elementwise multiplying the highlighted input (blue) with the filter (red), summing it up, and then offsetting the result by the bias</strong>.<br><br></p>
<p><div class="fig figcenter fighighlight">
<iframe src="https://cs231n.github.io/assets/conv-demo/index.html" width="100%" height="700px;" style="border:none;"></iframe>
<div class="figcaption"></div>
</div></p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementation as Matrix Multiplication)</p>
<p>A common implementation pattern of the CONV layer is to formulate the forward pass of a convolutional layer as one big matrix multiply as follows:<br><br></p>
<p>The local regions (blocks that have the same shape as the filter) in the input image are stretched out into columns in an operation commonly called <strong>im2col</strong>. For example, if the input is [227x227x3] and it is to be convolved with 11x11x3 filters at stride 4, then we would take blocks of shape [11x11x3] in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives <span class="arithmatex">\(((227-11)/4+1)^2\)</span> = 3025 blocks, leading to an output matrix <span class="arithmatex">\(X_{col}\)</span> of <em>im2col</em> of size [363 x 3025].<br>
<br>
Remember that we are to multiply each column of <span class="arithmatex">\(X_{col}\)</span> with the weights of the CONV Layer. The weights of the CONV layer are similarly stretched out into rows. For example, if there are 96 filters of size [11x11x3] this would give a matrix <span class="arithmatex">\(W_{row}\)</span> of size [96 x 363].<br>
<br>
The result of a convolution is now equivalent to performing one large matrix multiply <code>np.dot(W_row, X_col)</code>. In our example, the output of this operation would be [96 x 3025], giving the output of the dot product of each filter at each location.<br>
<br>
The result must finally be reshaped back to its proper output dimension [55x55x96].<br>
<br>
The downside is that it can use a lot of memory, since some values in the input volume are replicated multiple times in <span class="arithmatex">\(X_{col}\)</span>. The benefit is that there are many very efficient implementations of Matrix Multiplication that we can take advantage of (e.g. BLAS API). </p>
</div>
<div class="admonition nt">
<p class="admonition-title"><strong>Note</strong> (1x1 Convolution)</p>
<p>As an aside, several papers use 1x1 convolutions, as first investigated by <a href="http://arxiv.org/abs/1312.4400"><u>Network in Network</u></a>.</p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementing Convolution Layer)</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">conv_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">conv_param</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A naive implementation of the forward pass for a convolutional layer.</span>

<span class="sd">    The input consists of N data points, each with C channels, height H and</span>
<span class="sd">    width W. We convolve each input with F different filters, where each filter</span>
<span class="sd">    spans all C channels and has height HH and width WW.</span>

<span class="sd">    Input:</span>
<span class="sd">    - x: Input data of shape (N, C, H, W)</span>
<span class="sd">    - w: Filter weights of shape (F, C, HH, WW)</span>
<span class="sd">    - b: Biases, of shape (F,)</span>
<span class="sd">    - conv_param: A dictionary with the following keys:</span>
<span class="sd">    - &#39;stride&#39;: The number of pixels between adjacent receptive fields in the</span>
<span class="sd">        horizontal and vertical directions.</span>
<span class="sd">    - &#39;pad&#39;: The number of pixels that will be used to zero-pad the input.</span>

<span class="sd">    During padding, &#39;pad&#39; zeros should be placed symmetrically (i.e equally on both sides)</span>
<span class="sd">    along the height and width axes of the input. Be careful not to modfiy the original</span>
<span class="sd">    input x directly.</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - out: Output data, of shape (N, F, H&#39;, W&#39;) where H&#39; and W&#39; are given by</span>
<span class="sd">    H&#39; = 1 + (H + 2 * pad - HH) / stride</span>
<span class="sd">    W&#39; = 1 + (W + 2 * pad - WW) / stride</span>
<span class="sd">    - cache: (x, w, b, conv_param)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">F</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">HH</span><span class="p">,</span> <span class="n">WW</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">stride</span><span class="p">,</span> <span class="n">pad</span> <span class="o">=</span> <span class="n">conv_param</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">],</span> <span class="n">conv_param</span><span class="p">[</span><span class="s1">&#39;pad&#39;</span><span class="p">]</span>
    <span class="n">H_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">HH</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>
    <span class="n">W_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">WW</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H_out</span><span class="p">,</span> <span class="n">W_out</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">image_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">image_index</span><span class="p">]</span>

        <span class="c1"># Create a new matrix with the padded dimensions</span>
        <span class="n">padded_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">C</span><span class="p">,</span> <span class="n">H</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span><span class="p">,</span> <span class="n">W</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span><span class="p">))</span>

        <span class="c1"># Insert the original image into the padded matrix</span>
        <span class="n">padded_image</span><span class="p">[:,</span> <span class="n">pad</span><span class="p">:</span><span class="n">pad</span><span class="o">+</span><span class="n">H</span><span class="p">,</span> <span class="n">pad</span><span class="p">:</span><span class="n">pad</span><span class="o">+</span><span class="n">W</span><span class="p">]</span> <span class="o">=</span> <span class="n">image</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">padded_H</span><span class="p">,</span> <span class="n">padded_W</span> <span class="o">=</span> <span class="n">padded_image</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">for</span> <span class="n">filter_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">F</span><span class="p">):</span>
            <span class="n">_filter</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">filter_index</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_H</span><span class="o">-</span><span class="n">HH</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">padded_W</span><span class="o">-</span><span class="n">WW</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">stride</span><span class="p">):</span>
                    <span class="c1"># Extract the region of the padded image corresponding to the filter&#39;s location</span>
                    <span class="n">region</span> <span class="o">=</span> <span class="n">padded_image</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">HH</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="n">WW</span><span class="p">]</span>

                    <span class="c1"># Perform element-wise multiplication and sum the result</span>
                    <span class="n">out</span><span class="p">[</span><span class="n">image_index</span><span class="p">][</span><span class="n">filter_index</span><span class="p">][</span><span class="n">i</span><span class="o">//</span><span class="n">stride</span><span class="p">][</span><span class="n">j</span><span class="o">//</span><span class="n">stride</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">region</span> <span class="o">*</span> <span class="n">_filter</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">filter_index</span><span class="p">]</span>             

    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">conv_param</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>

<span class="k">def</span> <span class="nf">conv_backward_naive</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A naive implementation of the backward pass for a convolutional layer.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - dout: Upstream derivatives of shape (N, F, H_out, W_out).</span>
<span class="sd">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - dx: Gradient with respect to x, of shape (N, C, H, W)</span>
<span class="sd">    - dw: Gradient with respect to w, of shape (F, C, HH, WW)</span>
<span class="sd">    - db: Gradient with respect to b, of shape (F,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">conv_param</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">F</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">HH</span><span class="p">,</span> <span class="n">WW</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">stride</span><span class="p">,</span> <span class="n">pad</span> <span class="o">=</span> <span class="n">conv_param</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">],</span> <span class="n">conv_param</span><span class="p">[</span><span class="s1">&#39;pad&#39;</span><span class="p">]</span>
    <span class="n">H_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">HH</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>
    <span class="n">W_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">pad</span> <span class="o">-</span> <span class="n">WW</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>

    <span class="c1"># Initialize gradients</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># Pad the input x and dx</span>
    <span class="n">padded_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">),</span> <span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">)</span>
    <span class="n">padded_dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">padded_x</span><span class="p">)</span>

    <span class="c1"># Compute db</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="c1"># Compute dw and dx</span>
    <span class="k">for</span> <span class="n">image_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">padded_x</span><span class="p">[</span><span class="n">image_index</span><span class="p">]</span>
        <span class="n">dimage</span> <span class="o">=</span> <span class="n">padded_dx</span><span class="p">[</span><span class="n">image_index</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">filter_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">F</span><span class="p">):</span>
            <span class="n">_filter</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="n">filter_index</span><span class="p">]</span>
            <span class="n">dout_filter</span> <span class="o">=</span> <span class="n">dout</span><span class="p">[</span><span class="n">image_index</span><span class="p">,</span> <span class="n">filter_index</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">H_out</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W_out</span><span class="p">):</span>
                    <span class="c1"># Calculate the current region</span>
                    <span class="n">i_start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="n">j_start</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="n">i_end</span> <span class="o">=</span> <span class="n">i_start</span> <span class="o">+</span> <span class="n">HH</span>
                    <span class="n">j_end</span> <span class="o">=</span> <span class="n">j_start</span> <span class="o">+</span> <span class="n">WW</span>

                    <span class="n">region</span> <span class="o">=</span> <span class="n">image</span><span class="p">[:,</span> <span class="n">i_start</span><span class="p">:</span><span class="n">i_end</span><span class="p">,</span> <span class="n">j_start</span><span class="p">:</span><span class="n">j_end</span><span class="p">]</span>

                    <span class="c1"># Update the gradient for w (dw)</span>
                    <span class="n">dw</span><span class="p">[</span><span class="n">filter_index</span><span class="p">]</span> <span class="o">+=</span> <span class="n">region</span> <span class="o">*</span> <span class="n">dout_filter</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

                    <span class="c1"># Update the gradient for x (dx)</span>
                    <span class="n">dimage</span><span class="p">[:,</span> <span class="n">i_start</span><span class="p">:</span><span class="n">i_end</span><span class="p">,</span> <span class="n">j_start</span><span class="p">:</span><span class="n">j_end</span><span class="p">]</span> <span class="o">+=</span> <span class="n">_filter</span> <span class="o">*</span> <span class="n">dout_filter</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

        <span class="c1"># Remove padding from the gradient for x</span>
        <span class="n">dx</span><span class="p">[</span><span class="n">image_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">dimage</span><span class="p">[:,</span> <span class="n">pad</span><span class="p">:</span><span class="o">-</span><span class="n">pad</span><span class="p">,</span> <span class="n">pad</span><span class="p">:</span><span class="o">-</span><span class="n">pad</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></td></tr></table></div>
</div>
<h2 id="relu-layer">Relu Layer<a class="headerlink" href="#relu-layer" title="Permanent link">&para;</a></h2>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementing Relu Layer)</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">relu_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">:</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;Computes the forward pass for a layer of rectified linear units (ReLUs).</span>

<span class="w">    </span><span class="nl">Input</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="o">:</span><span class="w"> </span><span class="n">Inputs</span><span class="p">,</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">shape</span>

<span class="w">    </span><span class="n">Returns</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">tuple</span><span class="w"> </span><span class="n">of</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">out</span><span class="o">:</span><span class="w"> </span><span class="n">Output</span><span class="p">,</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">cache</span><span class="o">:</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span>

<span class="w">    </span><span class="n">out</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)</span>

<span class="w">    </span><span class="n">cache</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">out</span><span class="p">,</span><span class="w"> </span><span class="n">cache</span>


<span class="n">def</span><span class="w"> </span><span class="n">relu_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span><span class="w"> </span><span class="n">cache</span><span class="p">)</span><span class="o">:</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;Computes the backward pass for a layer of rectified linear units (ReLUs).</span>

<span class="w">    </span><span class="nl">Input</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">dout</span><span class="o">:</span><span class="w"> </span><span class="n">Upstream</span><span class="w"> </span><span class="n">derivatives</span><span class="p">,</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">any</span><span class="w"> </span><span class="n">shape</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">cache</span><span class="o">:</span><span class="w"> </span><span class="n">Input</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">dout</span>

<span class="w">    </span><span class="nl">Returns</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">dx</span><span class="o">:</span><span class="w"> </span><span class="n">Gradient</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">respect</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">dx</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">cache</span>

<span class="w">    </span><span class="n">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dout</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">dx</span>
</code></pre></div></td></tr></table></div>
</div>
<h2 id="pooling-layer">Pooling Layer<a class="headerlink" href="#pooling-layer" title="Permanent link">&para;</a></h2>
<div class="admonition df">
<p class="admonition-title"><strong>Definition</strong> (Pooling Layer)</p>
<p>It is common to periodically insert a Pooling layer in-between successive Conv layers in a ConvNet architecture. The goal is: <strong>To progressively reduce the spatial size of the representation, thus to reduce the amount of parameters and computation, and hence to control overfitting</strong>. We use <strong>MAX</strong> operation to achieve these goals.<br>
<br>
The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations. Every MAX operation would in this case be taking a max over 4 numbers (little 2x2 region in some depth slice). The depth dimension remains unchanged. More generally, the pooling layer: <br>
<br></p>
<ul>
<li>Accepts a volume of size <span class="arithmatex">\( W_1 \times H_1 \times D_1 \)</span>.</li>
<li>Requires two hyperparameters: <br>
&nbsp;&nbsp;&nbsp;&nbsp;1. their spatial extent <span class="arithmatex">\( F \)</span>, <br>
&nbsp;&nbsp;&nbsp;&nbsp;2. the stride <span class="arithmatex">\( S \)</span>. <br></li>
<li>Produces a volume of size <span class="arithmatex">\( W_2 \times H_2 \times D_2 \)</span> where: <br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\( W_2 = \left(\frac{W_1 - F}{S}\right) + 1 \)</span> <br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\( H_2 = \left(\frac{H_1 - F}{S}\right) + 1 \)</span> <br>
&nbsp;&nbsp;&nbsp;&nbsp;<span class="arithmatex">\( D_2 = D_1 \)</span></li>
<li>Introduces zero parameters since it computes a fixed function of the input.</li>
<li>For Pooling layers, it is not common to pad the input using zero-padding.</li>
</ul>
<p>In most cases, <span class="arithmatex">\( F = 3 \)</span>, <span class="arithmatex">\( S = 2 \)</span> (also called overlapping pooling), or more commonly <span class="arithmatex">\( F = 2 \)</span>, <span class="arithmatex">\( S = 2 \)</span>. Pooling sizes with larger receptive fields are too destructive.</p>
<p><figure markdown="span">
<a class="glightbox" href="https://cs231n.github.io/assets/cnn/maxpool.jpeg" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Image title" src="https://cs231n.github.io/assets/cnn/maxpool.jpeg" width="600" /></a>
<figcaption>Pooling Layer</figcaption>
</figure></p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementing Pooling Layer)</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">max_pool_forward_naive</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pool_param</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A naive implementation of the forward pass for a max-pooling layer.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - x: Input data, of shape (N, C, H, W)</span>
<span class="sd">    - pool_param: dictionary with the following keys:</span>
<span class="sd">    - &#39;pool_height&#39;: The height of each pooling region</span>
<span class="sd">    - &#39;pool_width&#39;: The width of each pooling region</span>
<span class="sd">    - &#39;stride&#39;: The distance between adjacent pooling regions</span>

<span class="sd">    No padding is necessary here, eg you can assume:</span>
<span class="sd">    - (H - pool_height) % stride == 0</span>
<span class="sd">    - (W - pool_width) % stride == 0</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - out: Output data, of shape (N, C, H&#39;, W&#39;) where H&#39; and W&#39; are given by</span>
<span class="sd">    H&#39; = 1 + (H - pool_height) // stride</span>
<span class="sd">    W&#39; = 1 + (W - pool_width) // stride</span>
<span class="sd">    - cache: (x, pool_param)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">pool_height</span><span class="p">,</span> <span class="n">pool_width</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s1">&#39;pool_height&#39;</span><span class="p">],</span> <span class="n">pool_param</span><span class="p">[</span><span class="s1">&#39;pool_width&#39;</span><span class="p">],</span> <span class="n">pool_param</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">]</span>
    <span class="n">H_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">pool_height</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>
    <span class="n">W_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">pool_width</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H_out</span><span class="p">,</span> <span class="n">W_out</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">image_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">H_out</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W_out</span><span class="p">):</span>
                    <span class="n">i_start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="n">j_start</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="n">i_end</span> <span class="o">=</span> <span class="n">i_start</span> <span class="o">+</span> <span class="n">pool_height</span>
                    <span class="n">j_end</span> <span class="o">=</span> <span class="n">j_start</span> <span class="o">+</span> <span class="n">pool_width</span>

                    <span class="c1"># Extract the region to pool</span>
                    <span class="n">region</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">image_index</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i_start</span><span class="p">:</span><span class="n">i_end</span><span class="p">,</span> <span class="n">j_start</span><span class="p">:</span><span class="n">j_end</span><span class="p">]</span>

                    <span class="c1"># Perform max pooling</span>
                    <span class="n">out</span><span class="p">[</span><span class="n">image_index</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">region</span><span class="p">)</span>

    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pool_param</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>

<span class="k">def</span> <span class="nf">max_pool_backward_naive</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A naive implementation of the backward pass for a max-pooling layer.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - dout: Upstream derivatives of shape (N, C, H_out, W_out)</span>
<span class="sd">    - cache: A tuple of (x, pool_param) as in the forward pass.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - dx: Gradient with respect to x, of shape (N, C, H, W)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">pool_param</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">pool_height</span><span class="p">,</span> <span class="n">pool_width</span><span class="p">,</span> <span class="n">stride</span> <span class="o">=</span> <span class="n">pool_param</span><span class="p">[</span><span class="s1">&#39;pool_height&#39;</span><span class="p">],</span> <span class="n">pool_param</span><span class="p">[</span><span class="s1">&#39;pool_width&#39;</span><span class="p">],</span> <span class="n">pool_param</span><span class="p">[</span><span class="s1">&#39;stride&#39;</span><span class="p">]</span>
    <span class="n">H_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">H</span> <span class="o">-</span> <span class="n">pool_height</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>
    <span class="n">W_out</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">pool_width</span><span class="p">)</span> <span class="o">//</span> <span class="n">stride</span>

    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">image_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">H_out</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W_out</span><span class="p">):</span>
                    <span class="n">i_start</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="n">j_start</span> <span class="o">=</span> <span class="n">j</span> <span class="o">*</span> <span class="n">stride</span>
                    <span class="n">i_end</span> <span class="o">=</span> <span class="n">i_start</span> <span class="o">+</span> <span class="n">pool_height</span>
                    <span class="n">j_end</span> <span class="o">=</span> <span class="n">j_start</span> <span class="o">+</span> <span class="n">pool_width</span>

                    <span class="c1"># Extract the region of the input that was pooled</span>
                    <span class="n">region</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">image_index</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i_start</span><span class="p">:</span><span class="n">i_end</span><span class="p">,</span> <span class="n">j_start</span><span class="p">:</span><span class="n">j_end</span><span class="p">]</span>

                    <span class="c1"># Find the mask of the maximum value in the region</span>
                    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">region</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">region</span><span class="p">))</span>

                    <span class="c1"># Distribute the gradient from dout to the corresponding max location in dx</span>
                    <span class="n">dx</span><span class="p">[</span><span class="n">image_index</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i_start</span><span class="p">:</span><span class="n">i_end</span><span class="p">,</span> <span class="n">j_start</span><span class="p">:</span><span class="n">j_end</span><span class="p">]</span> <span class="o">+=</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">dout</span><span class="p">[</span><span class="n">image_index</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">dx</span>
</code></pre></div></td></tr></table></div>
</div>
<h2 id="fully-connected-fc-layer">Fully Connected (FC) Layer<a class="headerlink" href="#fully-connected-fc-layer" title="Permanent link">&para;</a></h2>
<p>Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.</p>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Converting FC Layers to CONV Layers)</p>
<p>Note that the neurons in both FC layers and CONV layers compute dot products, so their functional form is identical. Therefore, it turns out that its possible to convert between FC and CONV layers. Of these two conversions, the ability to convert an FC layer to a CONV layer is particularly useful in practice. For example, an FC layer with <span class="arithmatex">\( K = 4096 \)</span> that is looking at some input volume of size <span class="arithmatex">\( 7 \times 7 \times 512 \)</span> can be equivalently expressed as a CONV layer with <span class="arithmatex">\( F = 7, P = 0, S = 1, K = 4096 \)</span>. In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be <span class="arithmatex">\( 1 \times 1 \times 4096 \)</span> since only a single depth column fits across the input volume, giving identical result as the initial FC layer.</p>
</div>
<div class="admonition eg">
<p class="admonition-title"><strong>Example</strong> (FC-CONV Conversion in AlexNet)</p>
<p>Consider a ConvNet architecture that takes a 224x224x3 image, and then uses a series of CONV layers and POOL layers to reduce the image to an activations volume of size 7x7x512 (in an AlexNet architecture that well see later, this is done by use of 5 pooling layers that downsample the input spatially by a factor of two each time, making the final spatial size 224/2/2/2/2/2 = 7). From there, an AlexNet uses two FC layers of size 4096 and finally the last FC layers with 1000 neurons that compute the class scores. We can convert each of these three FC layers to CONV layers as described above: <br></p>
<ul>
<li>Replace the first FC layer that looks at [7x7x512] volume with a CONV layer that uses filter size <span class="arithmatex">\( F = 7 \)</span>, giving output volume [1x1x4096].</li>
<li>Replace the second FC layer with a CONV layer that uses filter size <span class="arithmatex">\( F = 1 \)</span>, giving output volume [1x1x4096].</li>
<li>Replace the last FC layer similarly, with <span class="arithmatex">\( F = 1 \)</span>, giving final output [1x1x1000].</li>
</ul>
<p>Each of these conversions could in practice involve manipulating (e.g. reshaping) the weight matrix <span class="arithmatex">\( W \)</span> in each FC layer into CONV layer filters. It turns out that this conversion allows us to slide the original ConvNet very efficiently across many spatial positions in a larger image, in a single forward pass.<br><br></p>
<p>For example, if 224x224 image gives a volume of size [7x7x512] - i.e. a reduction by 32, then forwarding an image of size 384x384 through the converted architecture would give the equivalent volume in size [12x12x512], since 384/32 = 12. Following through with the next 3 CONV layers that we just converted from FC layers would now give the final volume of size [6x6x1000], since (12 - 7)/1 + 1 = 6. Note that instead of a single vector of class scores of size [1x1x1000], were now getting an entire 6x6 array of class scores across the 384x384 image.<br><br></p>
<p>Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.<br><br></p>
<p>Naturally, forwarding the converted ConvNet a single time is much more efficient than iterating the original ConvNet over all those 36 locations, since the 36 evaluations share computation. This trick is often used in practice to get better performance, where for example, it is common to resize an image to make it bigger, use a converted ConvNet to evaluate the class scores at many spatial positions and then average the class scores.</p>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementing Affine Layer)</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes the forward pass for an affine (fully connected) layer.</span>

<span class="sd">    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span>
<span class="sd">    examples, where each example x[i] has shape (d_1, ..., d_k). We will</span>
<span class="sd">    reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span>
<span class="sd">    then transform it to an output vector of dimension M.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span>
<span class="sd">    - w: A numpy array of weights, of shape (D, M)</span>
<span class="sd">    - b: A numpy array of biases, of shape (M,)</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - out: output, of shape (N, M)</span>
<span class="sd">    - cache: (x, w, b)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">@</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>

    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>

<span class="k">def</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes the backward pass for an affine (fully connected) layer.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - dout: Upstream derivative, of shape (N, M)</span>
<span class="sd">    - cache: Tuple of:</span>
<span class="sd">    - x: Input data, of shape (N, d_1, ... d_k)</span>
<span class="sd">    - w: Weights, of shape (D, M)</span>
<span class="sd">    - b: Biases, of shape (M,)</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span>
<span class="sd">    - dw: Gradient with respect to w, of shape (D, M)</span>
<span class="sd">    - db: Gradient with respect to b, of shape (M,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">dout</span> <span class="o">@</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dout</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">dout</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></td></tr></table></div>
</div>
<h2 id="softmax-loss">Softmax Loss<a class="headerlink" href="#softmax-loss" title="Permanent link">&para;</a></h2>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementing Softmax Loss)</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">softmax_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="o">:</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;Computes the loss and gradient for softmax classification.</span>

<span class="w">    </span><span class="nl">Inputs</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="o">:</span><span class="w"> </span><span class="n">Input</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">)</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">score</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">jth</span>
<span class="w">    </span><span class="k">class</span><span class="w"> </span><span class="nc">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">ith</span><span class="w"> </span><span class="n">input</span><span class="p">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="o">:</span><span class="w"> </span><span class="n">Vector</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">labels</span><span class="p">,</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="p">(</span><span class="n">N</span><span class="p">,)</span><span class="w"> </span><span class="n">where</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">label</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="k">and</span>
<span class="w">    </span><span class="mi">0</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">C</span>

<span class="w">    </span><span class="n">Returns</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">tuple</span><span class="w"> </span><span class="n">of</span><span class="o">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">loss</span><span class="o">:</span><span class="w"> </span><span class="n">Scalar</span><span class="w"> </span><span class="n">giving</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">loss</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">dx</span><span class="o">:</span><span class="w"> </span><span class="n">Gradient</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">respect</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">x</span>
<span class="w">    </span><span class="s">&quot;&quot;&quot;</span>
<span class="w">    </span><span class="n">loss</span><span class="p">,</span><span class="w"> </span><span class="n">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">None</span><span class="p">,</span><span class="w"> </span><span class="n">None</span>

<span class="w">    </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">samples</span>

<span class="w">    </span><span class="n">P</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">keepdims</span><span class="o">=</span><span class="n">True</span><span class="p">))</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="n">numerically</span><span class="w"> </span><span class="n">stable</span><span class="w"> </span><span class="n">exponents</span>
<span class="w">    </span><span class="n">P</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="n">P</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">keepdims</span><span class="o">=</span><span class="n">True</span><span class="p">)</span><span class="w">            </span><span class="err">#</span><span class="w"> </span><span class="n">row</span><span class="o">-</span><span class="n">wise</span><span class="w"> </span><span class="n">probabilities</span><span class="w"> </span><span class="p">(</span><span class="n">softmax</span><span class="p">)</span>

<span class="w">    </span><span class="n">loss</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">P</span><span class="p">[</span><span class="n">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="n">y</span><span class="p">]).</span><span class="n">sum</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">N</span><span class="w">     </span><span class="err">#</span><span class="w"> </span><span class="n">sum</span><span class="w"> </span><span class="n">cross</span><span class="w"> </span><span class="n">entropies</span><span class="w"> </span><span class="n">as</span><span class="w"> </span><span class="n">loss</span>

<span class="w">    </span><span class="n">P</span><span class="p">[</span><span class="n">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="w"> </span><span class="n">y</span><span class="p">]</span><span class="w"> </span><span class="o">-=</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">dx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">P</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">N</span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">loss</span><span class="p">,</span><span class="w"> </span><span class="n">dx</span>
</code></pre></div></td></tr></table></div>
</div>
<h2 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h2>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementing Batchnorm)</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">batchnorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass for batch normalization.</span>

<span class="sd">    During training the sample mean and (uncorrected) sample variance are</span>
<span class="sd">    computed from minibatch statistics and used to normalize the incoming data.</span>
<span class="sd">    During training we also keep an exponentially decaying running mean of the</span>
<span class="sd">    mean and variance of each feature, and these averages are used to normalize</span>
<span class="sd">    data at test-time.</span>

<span class="sd">    At each timestep we update the running averages for mean and variance using</span>
<span class="sd">    an exponential decay based on the momentum parameter:</span>

<span class="sd">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span>
<span class="sd">    running_var = momentum * running_var + (1 - momentum) * sample_var</span>

<span class="sd">    Note that the batch normalization paper suggests a different test-time</span>
<span class="sd">    behavior: they compute sample mean and variance for each feature using a</span>
<span class="sd">    large number of training images rather than using a running average. For</span>
<span class="sd">    this implementation we have chosen to use running averages instead since</span>
<span class="sd">    they do not require an additional estimation step; the torch7</span>
<span class="sd">    implementation of batch normalization also uses running averages.</span>

<span class="sd">    Input:</span>
<span class="sd">    - x: Data of shape (N, D)</span>
<span class="sd">    - gamma: Scale parameter of shape (D,)</span>
<span class="sd">    - beta: Shift paremeter of shape (D,)</span>
<span class="sd">    - bn_param: Dictionary with the following keys:</span>
<span class="sd">    - mode: &#39;train&#39; or &#39;test&#39;; required</span>
<span class="sd">    - eps: Constant for numeric stability</span>
<span class="sd">    - momentum: Constant for running mean / variance.</span>
<span class="sd">    - running_mean: Array of shape (D,) giving running mean of features</span>
<span class="sd">    - running_var Array of shape (D,) giving running variance of features</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - out: of shape (N, D)</span>
<span class="sd">    - cache: A tuple of values needed in the backward pass</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">bn_param</span><span class="p">[</span><span class="s2">&quot;mode&quot;</span><span class="p">]</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;momentum&quot;</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>

    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">running_mean</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;running_mean&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">running_var</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;running_var&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span><span class="p">:</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">x_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mu</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_new</span> <span class="o">+</span> <span class="n">beta</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;shape&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">D</span><span class="p">))</span>            <span class="c1"># reshape used in backprop</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="n">bn_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;axis&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>                <span class="c1"># axis to sum used in backprop</span>
        <span class="n">cache</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">x_new</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axis</span>   <span class="c1"># save for backprop</span>

        <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>                                                    <span class="c1"># if not batchnorm</span>
            <span class="n">running_mean</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">running_mean</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">mu</span> <span class="c1"># update overall mean</span>
            <span class="n">running_var</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">running_var</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">var</span>  <span class="c1"># update overall variance</span>

    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">:</span>
        <span class="n">x_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_new</span> <span class="o">+</span> <span class="n">beta</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid forward batchnorm mode &quot;</span><span class="si">%s</span><span class="s1">&quot;&#39;</span> <span class="o">%</span> <span class="n">mode</span><span class="p">)</span>

    <span class="c1"># Store the updated running means back into bn_param</span>
    <span class="n">bn_param</span><span class="p">[</span><span class="s2">&quot;running_mean&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_mean</span>
    <span class="n">bn_param</span><span class="p">[</span><span class="s2">&quot;running_var&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_var</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>

<span class="k">def</span> <span class="nf">batchnorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Backward pass for batch normalization.</span>

<span class="sd">    For this implementation, you should write out a computation graph for</span>
<span class="sd">    batch normalization on paper and propagate gradients backward through</span>
<span class="sd">    intermediate nodes.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - dout: Upstream derivatives, of shape (N, D)</span>
<span class="sd">    - cache: Variable of intermediates from batchnorm_forward.</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - dx: Gradient with respect to inputs x, of shape (N, D)</span>
<span class="sd">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span>
<span class="sd">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">cache</span>          <span class="c1"># expand cache</span>

    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">dout</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>            <span class="c1"># derivative w.r.t. beta</span>
    <span class="n">dgamma</span> <span class="o">=</span> <span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">x_hat</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="c1"># derivative w.r.t. gamma</span>

    <span class="n">dx_hat</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">gamma</span>                                       <span class="c1"># derivative w.t.r. x_hat</span>
    <span class="n">dstd</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx_hat</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">std</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>          <span class="c1"># derivative w.t.r. std</span>
    <span class="n">dvar</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">dstd</span> <span class="o">/</span> <span class="n">std</span>                                     <span class="c1"># derivative w.t.r. var</span>
    <span class="n">dx1</span> <span class="o">=</span> <span class="n">dx_hat</span> <span class="o">/</span> <span class="n">std</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span> <span class="o">*</span> <span class="n">dvar</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>          <span class="c1"># partial derivative w.t.r. dx</span>
    <span class="n">dmu</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dx1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                                  <span class="c1"># derivative w.t.r. mu</span>
    <span class="n">dx2</span> <span class="o">=</span> <span class="n">dmu</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span>                                       <span class="c1"># partial derivative w.t.r. dx</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx1</span> <span class="o">+</span> <span class="n">dx2</span>                                              <span class="c1"># full derivative w.t.r. x</span>

    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span>

<span class="k">def</span> <span class="nf">batchnorm_backward_alt</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Alternative backward pass for batch normalization.</span>

<span class="sd">    For this implementation you should work out the derivatives for the batch</span>
<span class="sd">    normalizaton backward pass on paper and simplify as much as possible. You</span>
<span class="sd">    should be able to derive a simple expression for the backward pass.</span>
<span class="sd">    See the jupyter notebook for more hints.</span>

<span class="sd">    Note: This implementation should expect to receive the same cache variable</span>
<span class="sd">    as batchnorm_backward, but might not use all of the values in the cache.</span>

<span class="sd">    Inputs / outputs: Same as batchnorm_backward</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">std</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">x_hat</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">cache</span> <span class="c1"># expand cache</span>
    <span class="n">S</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                     <span class="c1"># helper function</span>

    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">dout</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>            <span class="c1"># derivative w.r.t. beta</span>
    <span class="n">dgamma</span> <span class="o">=</span> <span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">x_hat</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;F&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span> <span class="c1"># derivative w.r.t. gamma</span>

    <span class="n">dx</span> <span class="o">=</span> <span class="n">dout</span> <span class="o">*</span> <span class="n">gamma</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span> <span class="o">*</span> <span class="n">std</span><span class="p">)</span>          <span class="c1"># temporarily initialize scale value</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dout</span><span class="p">)</span><span class="o">*</span><span class="n">dx</span>  <span class="o">-</span> <span class="n">S</span><span class="p">(</span><span class="n">dx</span><span class="o">*</span><span class="n">x_hat</span><span class="p">)</span><span class="o">*</span><span class="n">x_hat</span> <span class="o">-</span> <span class="n">S</span><span class="p">(</span><span class="n">dx</span><span class="p">)</span> <span class="c1"># derivative w.r.t. unnormalized x</span>

    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span>
</code></pre></div></td></tr></table></div>
</div>
<div class="admonition im">
<p class="admonition-title"><strong>Important Note</strong> (Implementing Layernorm)</p>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">layernorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">ln_param</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Forward pass for layer normalization.</span>

<span class="sd">    During both training and test-time, the incoming data is normalized per data-point,</span>
<span class="sd">    before being scaled by gamma and beta parameters identical to that of batch normalization.</span>

<span class="sd">    Note that in contrast to batch normalization, the behavior during train and test-time for</span>
<span class="sd">    layer normalization are identical, and we do not need to keep track of running averages</span>
<span class="sd">    of any sort.</span>

<span class="sd">    Input:</span>
<span class="sd">    - x: Data of shape (N, D)</span>
<span class="sd">    - gamma: Scale parameter of shape (D,)</span>
<span class="sd">    - beta: Shift paremeter of shape (D,)</span>
<span class="sd">    - ln_param: Dictionary with the following keys:</span>
<span class="sd">        - eps: Constant for numeric stability</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - out: of shape (N, D)</span>
<span class="sd">    - cache: A tuple of values needed in the backward pass</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">ln_param</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="n">bn_param</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;axis&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">ln_param</span><span class="p">}</span> <span class="c1"># same as batchnorm in train mode + over which axis to sum for grad</span>
    <span class="p">[</span><span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>          <span class="c1"># assure 2D to perform transpose</span>

    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">batchnorm_forward</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">gamma</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">)</span> <span class="c1"># same as batchnorm</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">T</span>                                                    <span class="c1"># transpose back</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>

<span class="k">def</span> <span class="nf">layernorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Backward pass for layer normalization.</span>

<span class="sd">    For this implementation, you can heavily rely on the work you&#39;ve done already</span>
<span class="sd">    for batch normalization.</span>

<span class="sd">    Inputs:</span>
<span class="sd">    - dout: Upstream derivatives, of shape (N, D)</span>
<span class="sd">    - cache: Variable of intermediates from layernorm_forward.</span>

<span class="sd">    Returns a tuple of:</span>
<span class="sd">    - dx: Gradient with respect to inputs x, of shape (N, D)</span>
<span class="sd">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span>
<span class="sd">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="n">batchnorm_backward_alt</span><span class="p">(</span><span class="n">dout</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span> <span class="c1"># same as batchnorm backprop</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">dx</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span>
</code></pre></div></td></tr></table></div>
</div>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 Tsljgj
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://tsljgj.github.io" target="_blank" rel="noopener" title="tsljgj.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1v16.2c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1L416 512h-24c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.7h-32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/tsljgj" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1M480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2m-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3m-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1"/></svg>
    </a>
  
    
    
    
    
    <a href="www.linkedin.com/in/zhihaoy" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5m282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:<zhihaoyu@andrew.cmu.edu>" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.8l167.6-182.8c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.instant", "navigation.top", "navigation.instant.prefetch", "navigation.instant.progress", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="../../javascripts/shortcuts.js"></script>
      
        <script src="../../javascripts/extra.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>